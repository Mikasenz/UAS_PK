{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyL81cOm0z4J",
        "outputId": "95a0ebc0-017d-4f13-d906-6b52193439b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Representasi Vektor**"
      ],
      "metadata": {
        "id": "nZ5TGMgx1N6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# i. REPRESENTASI VEKTOR (FIXED)\n",
        "# 1. TF-IDF: sklearn.feature_extraction.text.TfidfVectorizer\n",
        "# 2. BERT Embedding: transformers → model pre-trained (indobenchmark/indobert-base-p1)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# BERT and Transformers\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ Transformers not available. Install with: pip install transformers torch\")\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RepresentasiVektor:\n",
        "    \"\"\"\n",
        "    i. Representasi Vektor sesuai spesifikasi:\n",
        "    1. TF-IDF dengan sklearn.feature_extraction.text.TfidfVectorizer\n",
        "    2. BERT Embedding dengan indobenchmark/indobert-base-p1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.processed_dir = os.path.join(base_dir, \"data\", \"processed\")\n",
        "        self.raw_dir = os.path.join(base_dir, \"CLEANED\")\n",
        "        self.output_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"📊 i. REPRESENTASI VEKTOR\")\n",
        "        print(f\"Input processed: {self.processed_dir}\")\n",
        "        print(f\"Input raw: {self.raw_dir}\")\n",
        "        print(f\"Output: {self.output_dir}\")\n",
        "\n",
        "        # 1. TF-IDF Vectorizer sesuai spesifikasi (FIXED)\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=15000,          # ⬆️ Naik dari 5000\n",
        "            min_df=2,\n",
        "            max_df=0.85,                 # ⬇️ Turun dari 0.95\n",
        "            ngram_range=(1, 3),          # ⬆️ Tambah trigrams\n",
        "            lowercase=True,\n",
        "            stop_words=self.get_enhanced_legal_stopwords(),  # FIXED: call method correctly\n",
        "            sublinear_tf=True,\n",
        "            norm='l2',\n",
        "            smooth_idf=True\n",
        "        )\n",
        "\n",
        "        # 2. BERT model sesuai spesifikasi: indobenchmark/indobert-base-p1\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.bert_model_name = \"indobenchmark/indobert-base-p1\"\n",
        "            self.max_length = 512\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "            print(f\"🖥️ Device: {self.device}\")\n",
        "\n",
        "        # Data storage\n",
        "        self.cases_df = None\n",
        "        self.case_ids = []\n",
        "        self.case_texts = {}\n",
        "        self.tfidf_vectors = None\n",
        "        self.bert_vectors = None\n",
        "\n",
        "    def get_enhanced_legal_stopwords(self) -> List[str]:\n",
        "        \"\"\"FIXED: Enhanced legal stopwords - keep important legal terms\"\"\"\n",
        "        # Basic stopwords only - REMOVE legal domain terms\n",
        "        basic_only = [\n",
        "            'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'dengan', 'untuk',\n",
        "            'dalam', 'oleh', 'adalah', 'akan', 'telah', 'sudah', 'dapat',\n",
        "            'tidak', 'belum', 'juga', 'bahwa', 'sebagai', 'atau', 'jika',\n",
        "            'karena', 'sehingga', 'maka', 'agar', 'itu', 'ini', 'tersebut',\n",
        "            'hal', 'ada', 'sebuah', 'suatu', 'semua', 'setiap', 'beberapa'\n",
        "        ]\n",
        "\n",
        "        # EXPLICITLY KEEP these important legal terms (don't add to stopwords):\n",
        "        # terdakwa, jaksa, hakim, terorisme, suap, gratifikasi, pengadaan,\n",
        "        # tender, pasal, pengadilan, putusan, vonis, hukuman, denda, penjara\n",
        "\n",
        "        print(f\"📝 Using enhanced stopwords: {len(basic_only)} terms\")\n",
        "        print(f\"   Keeping legal terms: terdakwa, jaksa, hakim, terorisme, etc.\")\n",
        "\n",
        "        return basic_only\n",
        "\n",
        "    def get_indonesian_stopwords(self) -> List[str]:\n",
        "        \"\"\"Original stopwords method - keep for compatibility\"\"\"\n",
        "        return [\n",
        "            'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'dengan', 'untuk', 'dalam', 'oleh',\n",
        "            'adalah', 'akan', 'telah', 'sudah', 'dapat', 'harus', 'tidak', 'belum', 'juga',\n",
        "            'bahwa', 'sebagai', 'atau', 'jika', 'karena', 'sehingga', 'maka', 'agar', 'itu',\n",
        "            'ini', 'tersebut', 'hal', 'ada', 'sebuah', 'suatu', 'semua', 'setiap', 'beberapa'\n",
        "        ]\n",
        "\n",
        "    def enhanced_text_preprocessing(self, text: str) -> str:\n",
        "        \"\"\"ADDED: Enhanced preprocessing untuk dokumen hukum\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Handle legal abbreviations - EXPAND them\n",
        "        legal_abbrev = {\n",
        "            'ps': 'pasal', 'ps.': 'pasal',\n",
        "            'uu': 'undang_undang', 'u.u': 'undang_undang',\n",
        "            'pp': 'peraturan_pemerintah', 'p.p': 'peraturan_pemerintah',\n",
        "            'ma': 'mahkamah_agung', 'm.a': 'mahkamah_agung',\n",
        "            'kpk': 'komisi_pemberantasan_terorisme',\n",
        "            'tipikor': 'tindak_pidana_terorisme'\n",
        "        }\n",
        "\n",
        "        for abbrev, expansion in legal_abbrev.items():\n",
        "            text = re.sub(r'\\b' + re.escape(abbrev) + r'\\b', expansion, text)\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters but keep legal punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\-/\\.]', ' ', text)\n",
        "\n",
        "        # Remove numbers that are too long (case numbers, etc.)\n",
        "        text = re.sub(r'\\b\\d{4,}\\b', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_legal_entities(self, text: str) -> List[str]:\n",
        "        \"\"\"ADDED: Extract important legal entities\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        # Money amounts - important for corruption cases\n",
        "        money_pattern = r'(rp\\.?\\s*\\d+[\\d\\.,]*(?:\\s*(?:juta|miliar|ribu|triliun))?)'\n",
        "        money_matches = re.findall(money_pattern, text.lower())\n",
        "        entities.extend([f'nominal_{match.replace(\" \", \"_\")}' for match in money_matches[:3]])\n",
        "\n",
        "        # Institutions\n",
        "        institutions = [\n",
        "            'kejaksaan', 'pengadilan', 'kpk', 'mahkamah', 'dpr', 'dprd',\n",
        "            'kemenkeu', 'kementerian', 'dinas', 'bumn', 'bumd', 'pemerintah'\n",
        "        ]\n",
        "        for inst in institutions:\n",
        "            if inst in text.lower():\n",
        "                entities.append(f'institusi_{inst}')\n",
        "\n",
        "        # Pasal references\n",
        "        pasal_pattern = r'pasal\\s+(\\d+)'\n",
        "        pasal_matches = re.findall(pasal_pattern, text.lower())\n",
        "        entities.extend([f'pasal_{match}' for match in pasal_matches[:5]])\n",
        "\n",
        "        return entities[:10]  # Limit entities\n",
        "\n",
        "    def load_cases_data(self) -> bool:\n",
        "        \"\"\"Load data dari cases.csv yang sudah diproses\"\"\"\n",
        "        cases_file = os.path.join(self.processed_dir, \"cases.csv\")\n",
        "\n",
        "        if not os.path.exists(cases_file):\n",
        "            logger.error(f\"File tidak ditemukan: {cases_file}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            self.cases_df = pd.read_csv(cases_file, encoding='utf-8')\n",
        "            print(f\"📁 Loaded {len(self.cases_df)} cases from CSV\")\n",
        "\n",
        "            # Prepare case data\n",
        "            self.prepare_case_data()\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading cases.csv: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_raw_document_text(self, case_id: str) -> str:\n",
        "        \"\"\"Load raw document text dari file .txt\"\"\"\n",
        "        filepath = os.path.join(self.raw_dir, f\"{case_id}.txt\")\n",
        "\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error reading {filepath}: {e}\")\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def prepare_case_data(self):\n",
        "        \"\"\"ENHANCED: Siapkan data kasus untuk vectorization\"\"\"\n",
        "        print(\"📋 Preparing case data for vectorization...\")\n",
        "\n",
        "        for idx, row in self.cases_df.iterrows():\n",
        "            filename = row['nama_file']\n",
        "            case_id = filename.replace('.txt', '') if filename.endswith('.txt') else filename\n",
        "\n",
        "            # Gabungkan metadata dengan weighting\n",
        "            text_parts = []\n",
        "\n",
        "            # Jenis perkara - triple weight (very important)\n",
        "            if pd.notna(row.get('jenis_perkara')):\n",
        "                jenis = str(row['jenis_perkara'])\n",
        "                text_parts.extend([jenis] * 3)\n",
        "\n",
        "            # Pasal - double weight\n",
        "            if pd.notna(row.get('pasal_yang_dilanggar')):\n",
        "                pasal = str(row['pasal_yang_dilanggar'])\n",
        "                text_parts.extend([pasal] * 2)\n",
        "\n",
        "            # Other metadata - single weight\n",
        "            if pd.notna(row.get('terdakwa')):\n",
        "                text_parts.append(str(row['terdakwa']))\n",
        "\n",
        "            if pd.notna(row.get('jaksa_penuntut_umum')):\n",
        "                text_parts.append(str(row['jaksa_penuntut_umum']))\n",
        "\n",
        "            if pd.notna(row.get('hakim')):\n",
        "                text_parts.append(str(row['hakim']))\n",
        "\n",
        "            # Load and process raw text\n",
        "            raw_text = self.load_raw_document_text(case_id)\n",
        "\n",
        "            if raw_text.strip():\n",
        "                # Enhanced preprocessing\n",
        "                cleaned_raw = self.enhanced_text_preprocessing(raw_text)\n",
        "\n",
        "                # Extract legal entities\n",
        "                entities = self.extract_legal_entities(raw_text)\n",
        "\n",
        "                # Limit text but include important parts\n",
        "                if len(cleaned_raw) > 3000:  # Increased from 2000\n",
        "                    # Try to keep the judgement/decision part\n",
        "                    if 'putusan' in cleaned_raw or 'memutuskan' in cleaned_raw:\n",
        "                        decision_start = max(\n",
        "                            cleaned_raw.find('putusan'),\n",
        "                            cleaned_raw.find('memutuskan')\n",
        "                        )\n",
        "                        if decision_start > 0:\n",
        "                            # Keep decision part + beginning\n",
        "                            beginning = cleaned_raw[:1500]\n",
        "                            decision_part = cleaned_raw[decision_start:decision_start+1500]\n",
        "                            cleaned_raw = beginning + ' ' + decision_part\n",
        "                        else:\n",
        "                            cleaned_raw = cleaned_raw[:3000]\n",
        "                    else:\n",
        "                        cleaned_raw = cleaned_raw[:3000]\n",
        "\n",
        "                text_parts.append(cleaned_raw)\n",
        "                text_parts.extend(entities)\n",
        "\n",
        "            # Final combined text\n",
        "            final_text = ' '.join(text_parts) if text_parts else f\"dokumen hukum {case_id}\"\n",
        "\n",
        "            self.case_ids.append(case_id)\n",
        "            self.case_texts[case_id] = final_text\n",
        "\n",
        "        print(f\"✅ Prepared {len(self.case_ids)} cases for vectorization\")\n",
        "\n",
        "        # Sample text analysis\n",
        "        if self.case_texts:\n",
        "            sample_case = list(self.case_texts.keys())[0]\n",
        "            sample_text = self.case_texts[sample_case]\n",
        "            print(f\"📝 Sample case text length: {len(sample_text)} chars\")\n",
        "            print(f\"   First 200 chars: {sample_text[:200]}...\")\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean document text - KEPT for compatibility\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^\\w\\s\\-/\\.]', ' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def apply_legal_term_boosting(self, tfidf_matrix):\n",
        "        \"\"\"ADDED: Boost important legal terms\"\"\"\n",
        "        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "        legal_boost_terms = {\n",
        "            'terorisme': 2.5,\n",
        "        'tindak_pidana_terorisme': 2.5,\n",
        "        'radikalisme': 2.0,\n",
        "        'bom': 2.0,\n",
        "        'peledakan': 2.0,\n",
        "        'senjata': 1.8,\n",
        "        'kelompok_teroris': 1.8,\n",
        "        'isis': 1.8,\n",
        "        'jaringan_teroris': 1.7,\n",
        "        'densus_88': 1.6,\n",
        "        'penangkapan': 1.6,\n",
        "        'penggerebekan': 1.5,\n",
        "        'tersangka': 1.5,\n",
        "        'penahanan': 1.4,\n",
        "        'pengadilan': 1.3,\n",
        "        'jaksa': 1.3,\n",
        "        'hakim': 1.3,\n",
        "        'pasal': 1.3,\n",
        "        'undang_undang': 1.3,\n",
        "        'hukuman': 1.4,\n",
        "        'vonis': 1.4,\n",
        "        'penjara': 1.3,\n",
        "        'denda': 1.2\n",
        "        }\n",
        "\n",
        "        boosted_count = 0\n",
        "        for term, boost in legal_boost_terms.items():\n",
        "            term_indices = np.where(feature_names == term)[0]\n",
        "            if len(term_indices) > 0:\n",
        "                tfidf_matrix[:, term_indices[0]] *= boost\n",
        "                boosted_count += 1\n",
        "\n",
        "        print(f\"📈 Boosted {boosted_count} legal terms in TF-IDF matrix\")\n",
        "        return tfidf_matrix\n",
        "\n",
        "    def create_tfidf_vectors(self) -> bool:\n",
        "        \"\"\"ENHANCED: TF-IDF with legal term boosting\"\"\"\n",
        "        print(\"\\n📊 1. Creating Enhanced TF-IDF vectors\")\n",
        "        print(\"   Features: 15K vocab, trigrams, legal stopwords, term boosting\")\n",
        "\n",
        "        if len(self.case_texts) == 0:\n",
        "            logger.error(\"No case texts available\")\n",
        "            return False\n",
        "\n",
        "        # Prepare texts for TF-IDF\n",
        "        texts = [self.case_texts[case_id] for case_id in self.case_ids]\n",
        "\n",
        "        try:\n",
        "            # Fit TF-IDF vectorizer\n",
        "            print(\"   Fitting TF-IDF vectorizer...\")\n",
        "            self.tfidf_vectors = self.tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "            # Apply legal term boosting\n",
        "            print(\"   Applying legal term boosting...\")\n",
        "            self.tfidf_vectors = self.apply_legal_term_boosting(self.tfidf_vectors)\n",
        "\n",
        "            # Get vocabulary info\n",
        "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "            vocab_size = len(feature_names)\n",
        "\n",
        "            print(f\"✅ Enhanced TF-IDF vectors created: {self.tfidf_vectors.shape}\")\n",
        "            print(f\"📈 Vocabulary size: {vocab_size:,}\")\n",
        "\n",
        "            # Check for important legal terms in vocabulary\n",
        "            important_legal_terms = [\n",
        "                'terorisme', 'radikalisme', 'bom', 'peledakan', 'senjata',\n",
        "    'tersangka', 'densus_88', 'kelompok_teroris', 'pengadilan', 'pasal'\n",
        "            ]\n",
        "\n",
        "            found_terms = [term for term in important_legal_terms if term in feature_names]\n",
        "            missing_terms = [term for term in important_legal_terms if term not in feature_names]\n",
        "\n",
        "            print(f\"📋 Legal terms in vocabulary: {found_terms}\")\n",
        "            if missing_terms:\n",
        "                print(f\"⚠️ Missing legal terms: {missing_terms}\")\n",
        "\n",
        "            # Test with enhanced queries\n",
        "            test_queries = [\n",
        "                \"aksi terorisme di jakarta\",\n",
        "    \"peledakan bom di gereja\",\n",
        "    \"penangkapan anggota kelompok teroris\",\n",
        "    \"radikalisme di lingkungan kampus\",\n",
        "    \"densus 88 gerebek tempat persembunyian\"\n",
        "            ]\n",
        "\n",
        "            for query in test_queries:\n",
        "                test_vector = self.tfidf_vectorizer.transform([query])\n",
        "                print(f\"🧪 Test query '{query}': {test_vector.nnz} non-zero elements\")\n",
        "\n",
        "                if test_vector.nnz == 0:\n",
        "                    print(f\"   ⚠️ Empty vector for '{query}'\")\n",
        "                    # Debug vocabulary overlap\n",
        "                    query_words = query.lower().split()\n",
        "                    overlap = [word for word in query_words if word in feature_names]\n",
        "                    print(f\"   Words found: {overlap}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating TF-IDF vectors: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_bert_model(self) -> bool:\n",
        "        \"\"\"Load BERT model dan tokenizer\"\"\"\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            print(\"⚠️ Transformers not available, skipping BERT\")\n",
        "            return False\n",
        "\n",
        "        print(f\"\\n🤖 2. Loading BERT model: {self.bert_model_name}\")\n",
        "\n",
        "        try:\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model.to(self.device)\n",
        "            self.bert_model.eval()\n",
        "\n",
        "            print(f\"✅ BERT model loaded successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading BERT model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_bert_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Dapatkan BERT embedding untuk satu teks\"\"\"\n",
        "        if not self.bert_model or not self.bert_tokenizer:\n",
        "            return None\n",
        "\n",
        "        # Preprocess text\n",
        "        if len(text) > self.max_length * 4:\n",
        "            text = text[:self.max_length * 4]\n",
        "\n",
        "        try:\n",
        "            inputs = self.bert_tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "            return embedding.flatten()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting BERT embedding: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_bert_vectors(self) -> bool:\n",
        "        \"\"\"2. BERT Embedding: transformers → model pre-trained (indobenchmark/indobert-base-p1)\"\"\"\n",
        "        if not TRANSFORMERS_AVAILABLE or not self.load_bert_model():\n",
        "            print(\"⚠️ Skipping BERT vectors\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n🤖 2. Creating BERT embeddings with indobenchmark/indobert-base-p1\")\n",
        "\n",
        "        bert_embeddings = []\n",
        "        total_docs = len(self.case_ids)\n",
        "\n",
        "        for i, case_id in enumerate(self.case_ids):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processing {i+1}/{total_docs}: {case_id[:30]}...\")\n",
        "\n",
        "            text = self.case_texts.get(case_id, f\"dokumen_hukum_{case_id}\")\n",
        "            embedding = self.get_bert_embedding(text)\n",
        "\n",
        "            if embedding is not None:\n",
        "                bert_embeddings.append(embedding)\n",
        "            else:\n",
        "                bert_embeddings.append(np.zeros(768))  # BERT base dimension\n",
        "\n",
        "        self.bert_vectors = np.array(bert_embeddings)\n",
        "\n",
        "        print(f\"\\n✅ BERT vectors created: {self.bert_vectors.shape}\")\n",
        "        return True\n",
        "\n",
        "    def save_vectors(self) -> Dict[str, str]:\n",
        "        \"\"\"Simpan vectors ke file dengan enhanced marker\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        saved_files = {}\n",
        "\n",
        "        print(\"\\n💾 Saving enhanced vectors...\")\n",
        "\n",
        "        # Save TF-IDF vectors with 'enhanced' prefix\n",
        "        if self.tfidf_vectors is not None:\n",
        "            tfidf_filename = f\"enhanced_tfidf_vectors_{timestamp}.pkl\"\n",
        "            tfidf_path = os.path.join(self.output_dir, tfidf_filename)\n",
        "\n",
        "            tfidf_data = {\n",
        "                'vectors': self.tfidf_vectors,\n",
        "                'vectorizer': self.tfidf_vectorizer,\n",
        "                'case_ids': self.case_ids,\n",
        "                'feature_names': self.tfidf_vectorizer.get_feature_names_out(),\n",
        "                'case_texts': self.case_texts,\n",
        "                'cases_metadata': self.cases_df,\n",
        "                'enhanced': True,  # Mark as enhanced\n",
        "                'vocab_size': len(self.tfidf_vectorizer.get_feature_names_out()),\n",
        "                'config': {\n",
        "                    'max_features': 15000,\n",
        "                    'ngram_range': (1, 3),\n",
        "                    'legal_term_boosting': True,\n",
        "                    'enhanced_preprocessing': True\n",
        "                }\n",
        "            }\n",
        "\n",
        "            with open(tfidf_path, 'wb') as f:\n",
        "                pickle.dump(tfidf_data, f)\n",
        "\n",
        "            saved_files['tfidf'] = tfidf_path\n",
        "            print(f\"📄 Enhanced TF-IDF vectors saved: {tfidf_filename}\")\n",
        "            print(f\"   Vocabulary: {tfidf_data['vocab_size']:,} terms\")\n",
        "\n",
        "        # Save BERT vectors with 'enhanced' prefix\n",
        "        if self.bert_vectors is not None:\n",
        "            bert_filename = f\"enhanced_bert_vectors_{timestamp}.pkl\"\n",
        "            bert_path = os.path.join(self.output_dir, bert_filename)\n",
        "\n",
        "            bert_data = {\n",
        "                'vectors': self.bert_vectors,\n",
        "                'case_ids': self.case_ids,\n",
        "                'model_name': self.bert_model_name,\n",
        "                'case_texts': self.case_texts,\n",
        "                'cases_metadata': self.cases_df,\n",
        "                'enhanced': True  # Mark as enhanced\n",
        "            }\n",
        "\n",
        "            with open(bert_path, 'wb') as f:\n",
        "                pickle.dump(bert_data, f)\n",
        "\n",
        "            saved_files['bert'] = bert_path\n",
        "            print(f\"🤖 Enhanced BERT vectors saved: {bert_filename}\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "    def process_representasi_vektor(self) -> bool:\n",
        "        \"\"\"Proses lengkap representasi vektor sesuai spesifikasi\"\"\"\n",
        "        print(\"📊 i. REPRESENTASI VEKTOR (ENHANCED)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Enhanced TF-IDF: 15K vocab, trigrams, legal boosting\")\n",
        "        print(\"2. BERT Embedding: indobenchmark/indobert-base-p1\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Load cases data\n",
        "        if not self.load_cases_data():\n",
        "            print(\"❌ Failed to load cases data\")\n",
        "            return False\n",
        "\n",
        "        # 1. Create Enhanced TF-IDF vectors\n",
        "        tfidf_success = self.create_tfidf_vectors()\n",
        "\n",
        "        # 2. Create BERT vectors\n",
        "        bert_success = self.create_bert_vectors()\n",
        "\n",
        "        # Save vectors\n",
        "        if tfidf_success or bert_success:\n",
        "            saved_files = self.save_vectors()\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"✅ i. ENHANCED REPRESENTASI VEKTOR COMPLETED!\")\n",
        "            print(f\"📊 Enhanced TF-IDF: {'✅' if tfidf_success else '❌'}\")\n",
        "            print(f\"🤖 BERT: {'✅' if bert_success else '❌'}\")\n",
        "            print(f\"📁 Total cases: {len(self.case_ids)}\")\n",
        "            print(f\"💾 Files saved to: {self.output_dir}\")\n",
        "            if tfidf_success:\n",
        "                vocab_size = len(self.tfidf_vectorizer.get_feature_names_out())\n",
        "                print(f\"📈 Enhanced vocabulary: {vocab_size:,} terms\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ No vectors were created successfully\")\n",
        "            return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk representasi vektor\"\"\"\n",
        "    print(\"🚀 MULAI i. ENHANCED REPRESENTASI VEKTOR\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        vectorizer = RepresentasiVektor()\n",
        "        success = vectorizer.process_representasi_vektor()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\n🎉 ENHANCED REPRESENTASI VEKTOR BERHASIL!\")\n",
        "            print(\"✨ Peningkatan yang diterapkan:\")\n",
        "            print(\"  ✅ Vocabulary 15K (naik dari 5K)\")\n",
        "            print(\"  ✅ Trigrams (unigram + bigram + trigram)\")\n",
        "            print(\"  ✅ Enhanced legal stopwords\")\n",
        "            print(\"  ✅ Legal term boosting\")\n",
        "            print(\"  ✅ Enhanced text preprocessing\")\n",
        "            print(\"  ✅ Legal entity extraction\")\n",
        "            print(\"Langkah selanjutnya: ii. Splitting Data\")\n",
        "        else:\n",
        "            print(\"\\n❌ Representasi vektor gagal.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hotw-Oj71Nsl",
        "outputId": "4255c7ef-555c-457e-cc11-ff4fc2fe36e4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI i. ENHANCED REPRESENTASI VEKTOR\n",
            "======================================================================\n",
            "📊 i. REPRESENTASI VEKTOR\n",
            "Input processed: /content/drive/MyDrive/terorisme/data/processed\n",
            "Input raw: /content/drive/MyDrive/terorisme/CLEANED\n",
            "Output: /content/drive/MyDrive/terorisme/data/vectors\n",
            "📝 Using enhanced stopwords: 36 terms\n",
            "   Keeping legal terms: terdakwa, jaksa, hakim, terorisme, etc.\n",
            "🖥️ Device: cpu\n",
            "📊 i. REPRESENTASI VEKTOR (ENHANCED)\n",
            "============================================================\n",
            "1. Enhanced TF-IDF: 15K vocab, trigrams, legal boosting\n",
            "2. BERT Embedding: indobenchmark/indobert-base-p1\n",
            "============================================================\n",
            "📁 Loaded 46 cases from CSV\n",
            "📋 Preparing case data for vectorization...\n",
            "✅ Prepared 46 cases for vectorization\n",
            "📝 Sample case text length: 3670 chars\n",
            "   First 200 chars: tindak pidana terorisme tindak pidana terorisme tindak pidana terorisme melanggar pasal 15; undang-undang nomor 15 tahun 2003; undang-undang nomor 1 tahun 2002; undang-undang nomor 5 tahun 2018; undan...\n",
            "\n",
            "📊 1. Creating Enhanced TF-IDF vectors\n",
            "   Features: 15K vocab, trigrams, legal stopwords, term boosting\n",
            "   Fitting TF-IDF vectorizer...\n",
            "   Applying legal term boosting...\n",
            "📈 Boosted 7 legal terms in TF-IDF matrix\n",
            "✅ Enhanced TF-IDF vectors created: (46, 3433)\n",
            "📈 Vocabulary size: 3,433\n",
            "📋 Legal terms in vocabulary: ['terorisme', 'senjata']\n",
            "⚠️ Missing legal terms: ['radikalisme', 'bom', 'peledakan', 'tersangka', 'densus_88', 'kelompok_teroris', 'pengadilan', 'pasal']\n",
            "🧪 Test query 'aksi terorisme di jakarta': 2 non-zero elements\n",
            "🧪 Test query 'peledakan bom di gereja': 0 non-zero elements\n",
            "   ⚠️ Empty vector for 'peledakan bom di gereja'\n",
            "   Words found: []\n",
            "🧪 Test query 'penangkapan anggota kelompok teroris': 2 non-zero elements\n",
            "🧪 Test query 'radikalisme di lingkungan kampus': 1 non-zero elements\n",
            "🧪 Test query 'densus 88 gerebek tempat persembunyian': 0 non-zero elements\n",
            "   ⚠️ Empty vector for 'densus 88 gerebek tempat persembunyian'\n",
            "   Words found: []\n",
            "\n",
            "🤖 2. Loading BERT model: indobenchmark/indobert-base-p1\n",
            "✅ BERT model loaded successfully\n",
            "\n",
            "🤖 2. Creating BERT embeddings with indobenchmark/indobert-base-p1\n",
            "Processing 1/46: case_2023_TK1_Putusan_PN_JAKAR...\n",
            "Processing 11/46: case_2023_TK1_Putusan_PN_JAKAR...\n",
            "Processing 21/46: case_2023_TK1_Putusan_PN_JAKAR...\n",
            "Processing 31/46: case_2023_TK1_Putusan_PN_JAKAR...\n",
            "Processing 41/46: case_2023_TK1_Putusan_PN_JAKAR...\n",
            "\n",
            "✅ BERT vectors created: (46, 768)\n",
            "\n",
            "💾 Saving enhanced vectors...\n",
            "📄 Enhanced TF-IDF vectors saved: enhanced_tfidf_vectors_20250625_122036.pkl\n",
            "   Vocabulary: 3,433 terms\n",
            "🤖 Enhanced BERT vectors saved: enhanced_bert_vectors_20250625_122036.pkl\n",
            "\n",
            "============================================================\n",
            "✅ i. ENHANCED REPRESENTASI VEKTOR COMPLETED!\n",
            "📊 Enhanced TF-IDF: ✅\n",
            "🤖 BERT: ✅\n",
            "📁 Total cases: 46\n",
            "💾 Files saved to: /content/drive/MyDrive/terorisme/data/vectors\n",
            "📈 Enhanced vocabulary: 3,433 terms\n",
            "============================================================\n",
            "\n",
            "🎉 ENHANCED REPRESENTASI VEKTOR BERHASIL!\n",
            "✨ Peningkatan yang diterapkan:\n",
            "  ✅ Vocabulary 15K (naik dari 5K)\n",
            "  ✅ Trigrams (unigram + bigram + trigram)\n",
            "  ✅ Enhanced legal stopwords\n",
            "  ✅ Legal term boosting\n",
            "  ✅ Enhanced text preprocessing\n",
            "  ✅ Legal entity extraction\n",
            "Langkah selanjutnya: ii. Splitting Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting Data**"
      ],
      "metadata": {
        "id": "Dwnbpf3gBPjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n"
      ],
      "metadata": {
        "id": "5lOBqj1eBPMU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ii. SPLITTING DATA\n",
        "# 1. Lakukan splitting data untuk membagi data menjadi data train dan data test\n",
        "# 2. Rasio perbandingan data dapat berdasarkan kebutuhan atau merujuk pada artikel penelitian,\n",
        "#    missal 70:30 atau 80:20.\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "import glob\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SplittingData:\n",
        "    \"\"\"\n",
        "    ii. Splitting Data sesuai spesifikasi:\n",
        "    1. Split data menjadi train dan test\n",
        "    2. Rasio 70:30 atau 80:20 berdasarkan artikel penelitian\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.splits_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"✂️ ii. SPLITTING DATA\")\n",
        "        print(f\"Input vectors: {self.vectors_dir}\")\n",
        "        print(f\"Output splits: {self.splits_dir}\")\n",
        "\n",
        "        # Data storage\n",
        "        self.tfidf_data = None\n",
        "        self.bert_data = None\n",
        "        self.case_ids = []\n",
        "\n",
        "        # Split configurations berdasarkan artikel penelitian\n",
        "        self.split_ratios = {\n",
        "            \"70_30\": 0.3,  # 70:30\n",
        "            \"80_20\": 0.2,  # 80:20 (lebih umum)\n",
        "        }\n",
        "        self.random_state = 42\n",
        "\n",
        "    def diagnose_vectors_directory(self) -> None:\n",
        "        \"\"\"Diagnose vectors directory to understand what files are available\"\"\"\n",
        "        print(\"\\n🔍 Diagnosing vectors directory...\")\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            print(f\"❌ Vectors directory doesn't exist: {self.vectors_dir}\")\n",
        "            print(\"💡 Please ensure the previous step (i. Representasi Vektor) has been completed\")\n",
        "            return\n",
        "\n",
        "        print(f\"📁 Directory exists: {self.vectors_dir}\")\n",
        "\n",
        "        # List all files in vectors directory\n",
        "        all_files = os.listdir(self.vectors_dir)\n",
        "        print(f\"📄 Total files in directory: {len(all_files)}\")\n",
        "\n",
        "        if all_files:\n",
        "            print(\"📋 Files found:\")\n",
        "            for file in sorted(all_files):\n",
        "                file_path = os.path.join(self.vectors_dir, file)\n",
        "                file_size = os.path.getsize(file_path)\n",
        "                print(f\"   - {file} ({file_size:,} bytes)\")\n",
        "        else:\n",
        "            print(\"📭 No files found in vectors directory\")\n",
        "\n",
        "        # Look for pickle files specifically\n",
        "        pickle_files = [f for f in all_files if f.endswith('.pkl')]\n",
        "        print(f\"🥒 Pickle files found: {len(pickle_files)}\")\n",
        "\n",
        "        # Look for specific vector files\n",
        "        tfidf_files = [f for f in all_files if 'tfidf' in f.lower()]\n",
        "        bert_files = [f for f in all_files if 'bert' in f.lower()]\n",
        "\n",
        "        print(f\"📊 TF-IDF related files: {len(tfidf_files)}\")\n",
        "        print(f\"🤖 BERT related files: {len(bert_files)}\")\n",
        "\n",
        "    def load_vectors(self) -> bool:\n",
        "        \"\"\"Load vectors yang sudah dibuat dari tahap sebelumnya\"\"\"\n",
        "        print(\"\\n📥 Loading vectors from previous step...\")\n",
        "\n",
        "        # First diagnose the directory\n",
        "        self.diagnose_vectors_directory()\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            logger.error(f\"Vectors directory not found: {self.vectors_dir}\")\n",
        "            return False\n",
        "\n",
        "        # Use glob to find vector files more flexibly\n",
        "        tfidf_pattern = os.path.join(self.vectors_dir, \"*tfidf*.pkl\")\n",
        "        bert_pattern = os.path.join(self.vectors_dir, \"*bert*.pkl\")\n",
        "\n",
        "        tfidf_files = glob.glob(tfidf_pattern)\n",
        "        bert_files = glob.glob(bert_pattern)\n",
        "\n",
        "        print(f\"🔍 Found {len(tfidf_files)} TF-IDF files\")\n",
        "        print(f\"🔍 Found {len(bert_files)} BERT files\")\n",
        "\n",
        "        # Load TF-IDF vectors\n",
        "        if tfidf_files:\n",
        "            # Use the most recent file\n",
        "            latest_tfidf = max(tfidf_files, key=os.path.getmtime)\n",
        "            print(f\"📊 Loading TF-IDF: {os.path.basename(latest_tfidf)}\")\n",
        "\n",
        "            try:\n",
        "                with open(latest_tfidf, 'rb') as f:\n",
        "                    self.tfidf_data = pickle.load(f)\n",
        "\n",
        "                # Check data structure\n",
        "                if isinstance(self.tfidf_data, dict):\n",
        "                    if 'case_ids' in self.tfidf_data:\n",
        "                        self.case_ids = self.tfidf_data['case_ids']\n",
        "                    elif 'vectors' in self.tfidf_data:\n",
        "                        # If no case_ids, generate them\n",
        "                        n_samples = self.tfidf_data['vectors'].shape[0]\n",
        "                        self.case_ids = [f\"case_{i:04d}\" for i in range(n_samples)]\n",
        "                        print(f\"⚠️ No case_ids found, generated {len(self.case_ids)} case IDs\")\n",
        "\n",
        "                    if 'vectors' in self.tfidf_data:\n",
        "                        print(f\"✅ TF-IDF vectors loaded: {self.tfidf_data['vectors'].shape}\")\n",
        "                    else:\n",
        "                        print(f\"❌ No 'vectors' key found in TF-IDF data\")\n",
        "                        return False\n",
        "                else:\n",
        "                    print(f\"❌ TF-IDF data is not a dictionary\")\n",
        "                    return False\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading TF-IDF vectors: {e}\")\n",
        "                print(f\"💥 TF-IDF loading error: {e}\")\n",
        "\n",
        "        # Load BERT vectors\n",
        "        if bert_files:\n",
        "            # Use the most recent file\n",
        "            latest_bert = max(bert_files, key=os.path.getmtime)\n",
        "            print(f\"🤖 Loading BERT: {os.path.basename(latest_bert)}\")\n",
        "\n",
        "            try:\n",
        "                with open(latest_bert, 'rb') as f:\n",
        "                    self.bert_data = pickle.load(f)\n",
        "\n",
        "                # Check data structure\n",
        "                if isinstance(self.bert_data, dict):\n",
        "                    if not self.case_ids and 'case_ids' in self.bert_data:\n",
        "                        self.case_ids = self.bert_data['case_ids']\n",
        "                    elif not self.case_ids and 'vectors' in self.bert_data:\n",
        "                        # If no case_ids, generate them\n",
        "                        n_samples = self.bert_data['vectors'].shape[0]\n",
        "                        self.case_ids = [f\"case_{i:04d}\" for i in range(n_samples)]\n",
        "                        print(f\"⚠️ No case_ids found, generated {len(self.case_ids)} case IDs\")\n",
        "\n",
        "                    if 'vectors' in self.bert_data:\n",
        "                        print(f\"✅ BERT vectors loaded: {self.bert_data['vectors'].shape}\")\n",
        "                    else:\n",
        "                        print(f\"❌ No 'vectors' key found in BERT data\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BERT vectors: {e}\")\n",
        "                print(f\"💥 BERT loading error: {e}\")\n",
        "\n",
        "        # Final check\n",
        "        if not self.case_ids:\n",
        "            print(\"❌ No case IDs found in any vector files\")\n",
        "            return False\n",
        "\n",
        "        print(f\"📊 Total cases loaded: {len(self.case_ids)}\")\n",
        "        return len(self.case_ids) > 0\n",
        "\n",
        "    def create_dummy_data_for_testing(self, n_samples: int = 100) -> bool:\n",
        "        \"\"\"Create dummy data for testing purposes when no vectors are available\"\"\"\n",
        "        print(f\"\\n🎭 Creating dummy data for testing ({n_samples} samples)...\")\n",
        "\n",
        "        # Create dummy TF-IDF data\n",
        "        n_features_tfidf = 1000\n",
        "        dummy_tfidf_vectors = np.random.rand(n_samples, n_features_tfidf)\n",
        "\n",
        "        self.tfidf_data = {\n",
        "            'vectors': dummy_tfidf_vectors,\n",
        "            'case_ids': [f\"dummy_case_{i:04d}\" for i in range(n_samples)],\n",
        "            'vectorizer': None,  # Would be the actual vectorizer\n",
        "            'feature_names': [f\"feature_{i}\" for i in range(n_features_tfidf)]\n",
        "        }\n",
        "\n",
        "        # Create dummy BERT data\n",
        "        n_features_bert = 768  # Standard BERT embedding size\n",
        "        dummy_bert_vectors = np.random.rand(n_samples, n_features_bert)\n",
        "\n",
        "        self.bert_data = {\n",
        "            'vectors': dummy_bert_vectors,\n",
        "            'case_ids': [f\"dummy_case_{i:04d}\" for i in range(n_samples)],\n",
        "            'model_name': 'dummy-bert-model'\n",
        "        }\n",
        "\n",
        "        self.case_ids = self.tfidf_data['case_ids']\n",
        "\n",
        "        print(f\"✅ Dummy data created:\")\n",
        "        print(f\"   📊 TF-IDF: {dummy_tfidf_vectors.shape}\")\n",
        "        print(f\"   🤖 BERT: {dummy_bert_vectors.shape}\")\n",
        "        print(f\"   📋 Cases: {len(self.case_ids)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def create_labels_for_stratification(self) -> Tuple[Optional[np.ndarray], Optional[LabelEncoder]]:\n",
        "        \"\"\"Buat labels untuk stratified splitting jika diperlukan\"\"\"\n",
        "        print(\"🏷️ Creating labels for stratified splitting...\")\n",
        "\n",
        "        # Strategy 1: Use case metadata if available\n",
        "        if self.tfidf_data and 'cases_metadata' in self.tfidf_data:\n",
        "            print(\"📋 Using case metadata for stratification\")\n",
        "            cases_df = self.tfidf_data['cases_metadata']\n",
        "\n",
        "            labels = []\n",
        "            for case_id in self.case_ids:\n",
        "                case_row = cases_df[cases_df['nama_file'].str.replace('.txt', '') == case_id]\n",
        "\n",
        "                if len(case_row) > 0:\n",
        "                    row = case_row.iloc[0]\n",
        "                    if pd.notna(row.get('jenis_perkara')):\n",
        "                        jenis = str(row['jenis_perkara']).lower()\n",
        "                        if 'pidana' in jenis:\n",
        "                            if 'terorisme' in jenis:\n",
        "                                labels.append('pidana_terorisme')\n",
        "                            else:\n",
        "                                labels.append('pidana_umum')\n",
        "                        elif 'perdata' in jenis:\n",
        "                            labels.append('perdata')\n",
        "                        else:\n",
        "                            labels.append('lainnya')\n",
        "                    else:\n",
        "                        labels.append('unknown')\n",
        "                else:\n",
        "                    labels.append('unknown')\n",
        "\n",
        "        # Strategy 2: Create synthetic labels for testing\n",
        "        else:\n",
        "            print(\"🎭 Creating synthetic labels for testing\")\n",
        "            # Create balanced synthetic labels\n",
        "            n_samples = len(self.case_ids)\n",
        "            n_classes = 4\n",
        "            labels = []\n",
        "\n",
        "            for i, case_id in enumerate(self.case_ids):\n",
        "                if 'dummy' in case_id:\n",
        "                    # For dummy data, create balanced classes\n",
        "                    class_idx = i % n_classes\n",
        "                    class_names = ['pidana_umum', 'pidana_terorisme', 'perdata', 'lainnya']\n",
        "                    labels.append(class_names[class_idx])\n",
        "                else:\n",
        "                    # For real data without metadata, use simple heuristic\n",
        "                    labels.append('unknown')\n",
        "\n",
        "        if not labels:\n",
        "            return None, None\n",
        "\n",
        "        # Convert to numeric labels\n",
        "        label_encoder = LabelEncoder()\n",
        "        numeric_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "        # Check if we have enough samples per class for stratification\n",
        "        unique_labels, counts = np.unique(numeric_labels, return_counts=True)\n",
        "        min_samples = min(counts)\n",
        "\n",
        "        print(f\"📊 Label distribution:\")\n",
        "        for label, count in zip(label_encoder.classes_, counts):\n",
        "            print(f\"   {label}: {count} samples\")\n",
        "\n",
        "        if min_samples >= 2:  # Minimum for train/test split\n",
        "            print(f\"✅ Stratification possible. Classes: {len(unique_labels)}, Min samples: {min_samples}\")\n",
        "            return numeric_labels, label_encoder\n",
        "        else:\n",
        "            print(f\"⚠️ Not enough samples per class for stratification. Min: {min_samples}\")\n",
        "            return None, None\n",
        "\n",
        "    def create_split(self, test_size: float, split_name: str) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Buat train-test split dengan rasio tertentu\n",
        "        Args:\n",
        "            test_size: float - Ukuran test set (0.2 untuk 80:20, 0.3 untuk 70:30)\n",
        "            split_name: str - Nama split untuk identifikasi\n",
        "        \"\"\"\n",
        "        print(f\"\\n✂️ Creating {split_name} split (test_size={test_size})...\")\n",
        "\n",
        "        n_samples = len(self.case_ids)\n",
        "        if n_samples == 0:\n",
        "            print(\"❌ No samples available for splitting\")\n",
        "            return None\n",
        "\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "        # Try stratified split\n",
        "        labels, label_encoder = self.create_labels_for_stratification()\n",
        "\n",
        "        try:\n",
        "            if labels is not None:\n",
        "                # Stratified split\n",
        "                train_indices, test_indices = train_test_split(\n",
        "                    indices,\n",
        "                    test_size=test_size,\n",
        "                    random_state=self.random_state,\n",
        "                    stratify=labels,\n",
        "                    shuffle=True\n",
        "                )\n",
        "                print(f\"📊 Using stratified split\")\n",
        "            else:\n",
        "                # Random split\n",
        "                train_indices, test_indices = train_test_split(\n",
        "                    indices,\n",
        "                    test_size=test_size,\n",
        "                    random_state=self.random_state,\n",
        "                    shuffle=True\n",
        "                )\n",
        "                print(f\"🎲 Using random split\")\n",
        "\n",
        "            # Create split data\n",
        "            split_data = {\n",
        "                'split_name': split_name,\n",
        "                'test_size': test_size,\n",
        "                'train_size': 1 - test_size,\n",
        "                'total_samples': n_samples,\n",
        "                'train_indices': train_indices,\n",
        "                'test_indices': test_indices,\n",
        "                'train_case_ids': [self.case_ids[i] for i in train_indices],\n",
        "                'test_case_ids': [self.case_ids[i] for i in test_indices],\n",
        "                'stratified': labels is not None,\n",
        "                'random_state': self.random_state,\n",
        "                'label_encoder': label_encoder\n",
        "            }\n",
        "\n",
        "            # Add vector splits\n",
        "            if self.tfidf_data and 'vectors' in self.tfidf_data:\n",
        "                tfidf_vectors = self.tfidf_data['vectors']\n",
        "                split_data['train_tfidf'] = tfidf_vectors[train_indices]\n",
        "                split_data['test_tfidf'] = tfidf_vectors[test_indices]\n",
        "                print(f\"📊 TF-IDF splits added\")\n",
        "\n",
        "            if self.bert_data and 'vectors' in self.bert_data:\n",
        "                bert_vectors = self.bert_data['vectors']\n",
        "                split_data['train_bert'] = bert_vectors[train_indices]\n",
        "                split_data['test_bert'] = bert_vectors[test_indices]\n",
        "                print(f\"🤖 BERT splits added\")\n",
        "\n",
        "            # Add label splits if available\n",
        "            if labels is not None:\n",
        "                split_data['train_labels'] = labels[train_indices]\n",
        "                split_data['test_labels'] = labels[test_indices]\n",
        "                print(f\"🏷️ Label splits added\")\n",
        "\n",
        "            print(f\"✅ {split_name} split created:\")\n",
        "            print(f\"   📚 Training: {len(train_indices)} cases ({len(train_indices)/n_samples:.1%})\")\n",
        "            print(f\"   🧪 Testing: {len(test_indices)} cases ({len(test_indices)/n_samples:.1%})\")\n",
        "\n",
        "            return split_data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating {split_name} split: {e}\")\n",
        "            print(f\"💥 Error creating {split_name} split: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_multiple_splits(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Buat multiple splits dengan rasio berbeda sesuai spesifikasi:\n",
        "        - 70:30 berdasarkan artikel penelitian\n",
        "        - 80:20 berdasarkan artikel penelitian\n",
        "        \"\"\"\n",
        "        print(\"\\n🔄 Creating multiple splits based on research articles...\")\n",
        "\n",
        "        all_splits = {}\n",
        "\n",
        "        for split_name, test_size in self.split_ratios.items():\n",
        "            print(f\"\\n📊 Creating {split_name} split...\")\n",
        "\n",
        "            split_data = self.create_split(test_size, split_name)\n",
        "            if split_data:\n",
        "                all_splits[split_name] = split_data\n",
        "\n",
        "        return all_splits\n",
        "\n",
        "    def save_splits(self, splits_data: Dict) -> Dict[str, str]:\n",
        "        \"\"\"Simpan splits data ke file\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        saved_files = {}\n",
        "\n",
        "        print(\"\\n💾 Saving splits data...\")\n",
        "\n",
        "        # Save main splits\n",
        "        splits_filename = f\"data_splits_{timestamp}.pkl\"\n",
        "        splits_path = os.path.join(self.splits_dir, splits_filename)\n",
        "\n",
        "        # Include original vectors data for reference\n",
        "        complete_splits_data = {\n",
        "            'splits': splits_data,\n",
        "            'tfidf_vectorizer': self.tfidf_data.get('vectorizer') if self.tfidf_data else None,\n",
        "            'bert_model_name': self.bert_data.get('model_name') if self.bert_data else None,\n",
        "            'all_case_ids': self.case_ids,\n",
        "            'split_info': {\n",
        "                'total_cases': len(self.case_ids),\n",
        "                'splits_created': list(splits_data.keys()),\n",
        "                'created_at': datetime.now().isoformat(),\n",
        "                'random_state': self.random_state\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(splits_path, 'wb') as f:\n",
        "                pickle.dump(complete_splits_data, f)\n",
        "\n",
        "            saved_files['splits'] = splits_path\n",
        "            print(f\"📄 Data splits saved: {splits_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"💥 Error saving splits: {e}\")\n",
        "\n",
        "        # Save split summary\n",
        "        summary_filename = f\"split_summary_{timestamp}.json\"\n",
        "        summary_path = os.path.join(self.splits_dir, summary_filename)\n",
        "\n",
        "        summary_data = {\n",
        "            'total_cases': len(self.case_ids),\n",
        "            'splits_created': list(splits_data.keys()),\n",
        "            'random_state': self.random_state,\n",
        "            'created_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Add split details\n",
        "        for split_name, split_data in splits_data.items():\n",
        "            summary_data[f'{split_name}_train'] = len(split_data['train_case_ids'])\n",
        "            summary_data[f'{split_name}_test'] = len(split_data['test_case_ids'])\n",
        "            summary_data[f'{split_name}_stratified'] = split_data['stratified']\n",
        "\n",
        "        try:\n",
        "            import json\n",
        "            with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            saved_files['summary'] = summary_path\n",
        "            print(f\"📋 Split summary saved: {summary_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"💥 Error saving summary: {e}\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "    def validate_splits(self, splits_data: Dict) -> bool:\n",
        "        \"\"\"Validasi splits data\"\"\"\n",
        "        print(\"\\n🔍 Validating splits...\")\n",
        "\n",
        "        all_valid = True\n",
        "\n",
        "        for split_name, split_data in splits_data.items():\n",
        "            print(f\"\\n📊 Validating {split_name}:\")\n",
        "\n",
        "            train_ids = set(split_data['train_case_ids'])\n",
        "            test_ids = set(split_data['test_case_ids'])\n",
        "\n",
        "            # Check no overlap\n",
        "            overlap = train_ids.intersection(test_ids)\n",
        "            if overlap:\n",
        "                print(f\"❌ Overlap found: {len(overlap)} cases\")\n",
        "                all_valid = False\n",
        "            else:\n",
        "                print(f\"✅ No overlap between train and test\")\n",
        "\n",
        "            # Check completeness\n",
        "            total_split = len(train_ids) + len(test_ids)\n",
        "            total_original = len(self.case_ids)\n",
        "            if total_split != total_original:\n",
        "                print(f\"❌ Size mismatch: {total_split} vs {total_original}\")\n",
        "                all_valid = False\n",
        "            else:\n",
        "                print(f\"✅ Complete split: {total_split} cases\")\n",
        "\n",
        "            # Check vector dimensions if available\n",
        "            if 'train_tfidf' in split_data and 'test_tfidf' in split_data:\n",
        "                train_shape = split_data['train_tfidf'].shape\n",
        "                test_shape = split_data['test_tfidf'].shape\n",
        "                if train_shape[1] != test_shape[1]:\n",
        "                    print(f\"❌ TF-IDF dimension mismatch: {train_shape[1]} vs {test_shape[1]}\")\n",
        "                    all_valid = False\n",
        "                else:\n",
        "                    print(f\"✅ TF-IDF dimensions match: {train_shape[1]} features\")\n",
        "\n",
        "            if 'train_bert' in split_data and 'test_bert' in split_data:\n",
        "                train_shape = split_data['train_bert'].shape\n",
        "                test_shape = split_data['test_bert'].shape\n",
        "                if train_shape[1] != test_shape[1]:\n",
        "                    print(f\"❌ BERT dimension mismatch: {train_shape[1]} vs {test_shape[1]}\")\n",
        "                    all_valid = False\n",
        "                else:\n",
        "                    print(f\"✅ BERT dimensions match: {train_shape[1]} features\")\n",
        "\n",
        "        if all_valid:\n",
        "            print(f\"\\n✅ All splits are valid!\")\n",
        "        else:\n",
        "            print(f\"\\n❌ Some splits have validation issues!\")\n",
        "\n",
        "        return all_valid\n",
        "\n",
        "    def process_splitting_data(self, use_dummy_data: bool = False) -> bool:\n",
        "        \"\"\"\n",
        "        Proses lengkap splitting data sesuai spesifikasi:\n",
        "        1. Load vectors dari tahap sebelumnya\n",
        "        2. Buat splits dengan rasio 70:30 dan 80:20\n",
        "        3. Validasi dan simpan splits\n",
        "        \"\"\"\n",
        "        print(\"✂️ ii. SPLITTING DATA\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Split data untuk train dan test\")\n",
        "        print(\"2. Rasio 70:30 atau 80:20 berdasarkan artikel penelitian\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Load vectors or create dummy data\n",
        "        if use_dummy_data:\n",
        "            print(\"🎭 Using dummy data for testing...\")\n",
        "            if not self.create_dummy_data_for_testing():\n",
        "                print(\"❌ Failed to create dummy data\")\n",
        "                return False\n",
        "        else:\n",
        "            if not self.load_vectors():\n",
        "                print(\"❌ Failed to load vectors\")\n",
        "                print(\"💡 Suggestion: Run with use_dummy_data=True for testing\")\n",
        "                return False\n",
        "\n",
        "        # 2. Create multiple splits berdasarkan artikel penelitian\n",
        "        splits_data = self.create_multiple_splits()\n",
        "\n",
        "        if not splits_data:\n",
        "            print(\"❌ Failed to create splits\")\n",
        "            return False\n",
        "\n",
        "        # 3. Validate splits\n",
        "        if not self.validate_splits(splits_data):\n",
        "            print(\"⚠️ Some validation issues found, but continuing...\")\n",
        "\n",
        "        # 4. Save splits\n",
        "        saved_files = self.save_splits(splits_data)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"✅ ii. SPLITTING DATA COMPLETED!\")\n",
        "        print(f\"📊 Splits created: {list(splits_data.keys())}\")\n",
        "        print(f\"📁 Total cases: {len(self.case_ids)}\")\n",
        "\n",
        "        # Show split details\n",
        "        for split_name, split_data in splits_data.items():\n",
        "            train_size = len(split_data['train_case_ids'])\n",
        "            test_size = len(split_data['test_case_ids'])\n",
        "            train_pct = train_size / (train_size + test_size) * 100\n",
        "            test_pct = test_size / (train_size + test_size) * 100\n",
        "            print(f\"   {split_name}: {train_size} train ({train_pct:.1f}%), {test_size} test ({test_pct:.1f}%)\")\n",
        "\n",
        "        print(f\"💾 Files saved to: {self.splits_dir}\")\n",
        "        for file_type, file_path in saved_files.items():\n",
        "            print(f\"   {file_type}: {os.path.basename(file_path)}\")\n",
        "\n",
        "        print(\"Langkah selanjutnya: iii. Model Retrieval\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk splitting data\"\"\"\n",
        "    print(\"🚀 MULAI ii. SPLITTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        splitter = SplittingData()\n",
        "\n",
        "        # Try to load real data first, fallback to dummy data if needed\n",
        "        success = splitter.process_splitting_data(use_dummy_data=False)\n",
        "\n",
        "        if not success:\n",
        "            print(\"\\n🎭 Trying with dummy data for testing...\")\n",
        "            success = splitter.process_splitting_data(use_dummy_data=True)\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\n🎉 SPLITTING DATA BERHASIL!\")\n",
        "            print(\"✨ Yang telah dilakukan:\")\n",
        "            print(\"  ✅ Load vectors dari tahap i. Representasi Vektor\")\n",
        "            print(\"  ✅ Split data dengan rasio 70:30 dan 80:20\")\n",
        "            print(\"  ✅ Stratified splitting jika memungkinkan\")\n",
        "            print(\"  ✅ Validasi splits untuk memastikan tidak ada overlap\")\n",
        "            print(\"  ✅ Simpan splits untuk tahap selanjutnya\")\n",
        "            print(\"\\n📋 Rekomendasi:\")\n",
        "            print(\"  💡 Jika menggunakan dummy data, pastikan menjalankan tahap i. Representasi Vektor dulu\")\n",
        "            print(\"  💡 Periksa file splits yang dihasilkan sebelum melanjutkan ke tahap selanjutnya\")\n",
        "            print(\"Langkah selanjutnya: iii. Model Retrieval\")\n",
        "        else:\n",
        "            print(\"\\n❌ Splitting data gagal.\")\n",
        "            print(\"💡 Periksa apakah tahap i. Representasi Vektor sudah berhasil dijalankan\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZpiUYy82chE",
        "outputId": "8b0c9439-6bb8-4037-b301-3e4ff88c5685"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI ii. SPLITTING DATA\n",
            "======================================================================\n",
            "✂️ ii. SPLITTING DATA\n",
            "Input vectors: /content/drive/MyDrive/terorisme/data/vectors\n",
            "Output splits: /content/drive/MyDrive/terorisme/data/splits\n",
            "✂️ ii. SPLITTING DATA\n",
            "============================================================\n",
            "1. Split data untuk train dan test\n",
            "2. Rasio 70:30 atau 80:20 berdasarkan artikel penelitian\n",
            "============================================================\n",
            "\n",
            "📥 Loading vectors from previous step...\n",
            "\n",
            "🔍 Diagnosing vectors directory...\n",
            "📁 Directory exists: /content/drive/MyDrive/terorisme/data/vectors\n",
            "📄 Total files in directory: 12\n",
            "📋 Files found:\n",
            "   - enhanced_bert_vectors_20250625_112837.pkl (351,159 bytes)\n",
            "   - enhanced_bert_vectors_20250625_113626.pkl (351,159 bytes)\n",
            "   - enhanced_bert_vectors_20250625_115637.pkl (351,159 bytes)\n",
            "   - enhanced_bert_vectors_20250625_122036.pkl (351,778 bytes)\n",
            "   - enhanced_bert_vectors_20250625_122735.pkl (351,778 bytes)\n",
            "   - enhanced_bert_vectors_20250625_123029.pkl (351,778 bytes)\n",
            "   - enhanced_tfidf_vectors_20250625_112837.pkl (618,603 bytes)\n",
            "   - enhanced_tfidf_vectors_20250625_113626.pkl (618,603 bytes)\n",
            "   - enhanced_tfidf_vectors_20250625_115637.pkl (618,603 bytes)\n",
            "   - enhanced_tfidf_vectors_20250625_122036.pkl (619,327 bytes)\n",
            "   - enhanced_tfidf_vectors_20250625_122735.pkl (619,327 bytes)\n",
            "   - enhanced_tfidf_vectors_20250625_123029.pkl (619,327 bytes)\n",
            "🥒 Pickle files found: 12\n",
            "📊 TF-IDF related files: 6\n",
            "🤖 BERT related files: 6\n",
            "🔍 Found 6 TF-IDF files\n",
            "🔍 Found 6 BERT files\n",
            "📊 Loading TF-IDF: enhanced_tfidf_vectors_20250625_123029.pkl\n",
            "✅ TF-IDF vectors loaded: (46, 3433)\n",
            "🤖 Loading BERT: enhanced_bert_vectors_20250625_123029.pkl\n",
            "✅ BERT vectors loaded: (46, 768)\n",
            "📊 Total cases loaded: 46\n",
            "\n",
            "🔄 Creating multiple splits based on research articles...\n",
            "\n",
            "📊 Creating 70_30 split...\n",
            "\n",
            "✂️ Creating 70_30 split (test_size=0.3)...\n",
            "🏷️ Creating labels for stratified splitting...\n",
            "📋 Using case metadata for stratification\n",
            "📊 Label distribution:\n",
            "   pidana_terorisme: 39 samples\n",
            "   unknown: 7 samples\n",
            "✅ Stratification possible. Classes: 2, Min samples: 7\n",
            "📊 Using stratified split\n",
            "📊 TF-IDF splits added\n",
            "🤖 BERT splits added\n",
            "🏷️ Label splits added\n",
            "✅ 70_30 split created:\n",
            "   📚 Training: 32 cases (69.6%)\n",
            "   🧪 Testing: 14 cases (30.4%)\n",
            "\n",
            "📊 Creating 80_20 split...\n",
            "\n",
            "✂️ Creating 80_20 split (test_size=0.2)...\n",
            "🏷️ Creating labels for stratified splitting...\n",
            "📋 Using case metadata for stratification\n",
            "📊 Label distribution:\n",
            "   pidana_terorisme: 39 samples\n",
            "   unknown: 7 samples\n",
            "✅ Stratification possible. Classes: 2, Min samples: 7\n",
            "📊 Using stratified split\n",
            "📊 TF-IDF splits added\n",
            "🤖 BERT splits added\n",
            "🏷️ Label splits added\n",
            "✅ 80_20 split created:\n",
            "   📚 Training: 36 cases (78.3%)\n",
            "   🧪 Testing: 10 cases (21.7%)\n",
            "\n",
            "🔍 Validating splits...\n",
            "\n",
            "📊 Validating 70_30:\n",
            "✅ No overlap between train and test\n",
            "✅ Complete split: 46 cases\n",
            "✅ TF-IDF dimensions match: 3433 features\n",
            "✅ BERT dimensions match: 768 features\n",
            "\n",
            "📊 Validating 80_20:\n",
            "✅ No overlap between train and test\n",
            "✅ Complete split: 46 cases\n",
            "✅ TF-IDF dimensions match: 3433 features\n",
            "✅ BERT dimensions match: 768 features\n",
            "\n",
            "✅ All splits are valid!\n",
            "\n",
            "💾 Saving splits data...\n",
            "📄 Data splits saved: data_splits_20250625_123843.pkl\n",
            "📋 Split summary saved: split_summary_20250625_123843.json\n",
            "\n",
            "============================================================\n",
            "✅ ii. SPLITTING DATA COMPLETED!\n",
            "📊 Splits created: ['70_30', '80_20']\n",
            "📁 Total cases: 46\n",
            "   70_30: 32 train (69.6%), 14 test (30.4%)\n",
            "   80_20: 36 train (78.3%), 10 test (21.7%)\n",
            "💾 Files saved to: /content/drive/MyDrive/terorisme/data/splits\n",
            "   splits: data_splits_20250625_123843.pkl\n",
            "   summary: split_summary_20250625_123843.json\n",
            "Langkah selanjutnya: iii. Model Retrieval\n",
            "============================================================\n",
            "\n",
            "🎉 SPLITTING DATA BERHASIL!\n",
            "✨ Yang telah dilakukan:\n",
            "  ✅ Load vectors dari tahap i. Representasi Vektor\n",
            "  ✅ Split data dengan rasio 70:30 dan 80:20\n",
            "  ✅ Stratified splitting jika memungkinkan\n",
            "  ✅ Validasi splits untuk memastikan tidak ada overlap\n",
            "  ✅ Simpan splits untuk tahap selanjutnya\n",
            "\n",
            "📋 Rekomendasi:\n",
            "  💡 Jika menggunakan dummy data, pastikan menjalankan tahap i. Representasi Vektor dulu\n",
            "  💡 Periksa file splits yang dihasilkan sebelum melanjutkan ke tahap selanjutnya\n",
            "Langkah selanjutnya: iii. Model Retrieval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL RETRIEVAL**"
      ],
      "metadata": {
        "id": "qT-n2f8wG3q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# iii. MODEL RETRIEVAL\n",
        "# 1. Gunakan model machine learning seperti Support Vector Machine (SVM) atau Naive Bayes\n",
        "#    pada representasi TF-IDF untuk classification/retrieval.\n",
        "# 2. Gunakan model transformer (BERT/RoBERTa/IndoBERT/dll) untuk retrieval pada hasil embedding.\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# BERT and Transformers\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ Transformers not available. Install with: pip install transformers torch\")\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ModelRetrieval:\n",
        "    \"\"\"\n",
        "    iii. Model Retrieval sesuai spesifikasi:\n",
        "    1. SVM atau Naive Bayes pada TF-IDF untuk classification/retrieval\n",
        "    2. BERT/RoBERTa/IndoBERT untuk retrieval pada hasil embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "        self.models_dir = os.path.join(base_dir, \"data\", \"models\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.models_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"🤖 iii. MODEL RETRIEVAL\")\n",
        "        print(f\"Input splits: {self.splits_dir}\")\n",
        "        print(f\"Output models: {self.models_dir}\")\n",
        "\n",
        "        # Model storage\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.evaluation_results = {}\n",
        "\n",
        "        # Data storage\n",
        "        self.splits_data = None\n",
        "        self.train_data = {}\n",
        "        self.test_data = {}\n",
        "\n",
        "        # BERT components\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.bert_model_name = \"indobenchmark/indobert-base-p1\"\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            print(f\"🖥️ Device: {self.device}\")\n",
        "\n",
        "    def load_splits_data(self) -> bool:\n",
        "        \"\"\"Load splits data dari tahap sebelumnya\"\"\"\n",
        "        print(\"\\n📥 Loading splits data...\")\n",
        "\n",
        "        # Find latest split file\n",
        "        if not os.path.exists(self.splits_dir):\n",
        "            logger.error(f\"Splits directory not found: {self.splits_dir}\")\n",
        "            return False\n",
        "\n",
        "        split_files = [f for f in os.listdir(self.splits_dir)\n",
        "                      if f.startswith('data_splits_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not split_files:\n",
        "            logger.error(\"No split files found\")\n",
        "            return False\n",
        "\n",
        "        latest_split = max(split_files)\n",
        "        split_path = os.path.join(self.splits_dir, latest_split)\n",
        "\n",
        "        try:\n",
        "            with open(split_path, 'rb') as f:\n",
        "                complete_data = pickle.load(f)\n",
        "\n",
        "            self.splits_data = complete_data['splits']\n",
        "            self.tfidf_vectorizer = complete_data.get('tfidf_vectorizer')\n",
        "            self.bert_model_name = complete_data.get('bert_model_name', self.bert_model_name)\n",
        "\n",
        "            print(f\"✅ Splits loaded from: {latest_split}\")\n",
        "            print(f\"📊 Available splits: {list(self.splits_data.keys())}\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading splits: {e}\")\n",
        "            return False\n",
        "\n",
        "    def prepare_training_data(self, split_name: str = \"80_20\") -> bool:\n",
        "        \"\"\"Siapkan data untuk training dari split tertentu\"\"\"\n",
        "        print(f\"\\n📋 Preparing training data for {split_name} split...\")\n",
        "\n",
        "        if split_name not in self.splits_data:\n",
        "            logger.error(f\"Split {split_name} not found\")\n",
        "            return False\n",
        "\n",
        "        split_info = self.splits_data[split_name]\n",
        "\n",
        "        # Extract training and testing data\n",
        "        self.train_data = {\n",
        "            'case_ids': split_info['train_case_ids'],\n",
        "            'indices': split_info['train_indices']\n",
        "        }\n",
        "\n",
        "        self.test_data = {\n",
        "            'case_ids': split_info['test_case_ids'],\n",
        "            'indices': split_info['test_indices']\n",
        "        }\n",
        "\n",
        "        # Add TF-IDF vectors if available\n",
        "        if 'train_tfidf' in split_info:\n",
        "            self.train_data['tfidf'] = split_info['train_tfidf']\n",
        "            self.test_data['tfidf'] = split_info['test_tfidf']\n",
        "            print(f\"📊 TF-IDF vectors: train {self.train_data['tfidf'].shape}, test {self.test_data['tfidf'].shape}\")\n",
        "\n",
        "        # Add BERT vectors if available\n",
        "        if 'train_bert' in split_info:\n",
        "            self.train_data['bert'] = split_info['train_bert']\n",
        "            self.test_data['bert'] = split_info['test_bert']\n",
        "            print(f\"🤖 BERT vectors: train {self.train_data['bert'].shape}, test {self.test_data['bert'].shape}\")\n",
        "\n",
        "        # Add labels if available\n",
        "        if 'train_labels' in split_info:\n",
        "            self.train_data['labels'] = split_info['train_labels']\n",
        "            self.test_data['labels'] = split_info['test_labels']\n",
        "            self.label_encoder = split_info['label_encoder']\n",
        "            print(f\"🏷️ Labels: {len(np.unique(self.train_data['labels']))} classes\")\n",
        "\n",
        "        print(f\"✅ Training data prepared:\")\n",
        "        print(f\"   📚 Training: {len(self.train_data['case_ids'])} cases\")\n",
        "        print(f\"   🧪 Testing: {len(self.test_data['case_ids'])} cases\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def train_svm_model(self) -> bool:\n",
        "        \"\"\"\n",
        "        1. Support Vector Machine (SVM) pada representasi TF-IDF untuk classification/retrieval\n",
        "        \"\"\"\n",
        "        print(\"\\n🔧 1. Training SVM model on TF-IDF...\")\n",
        "\n",
        "        if 'tfidf' not in self.train_data:\n",
        "            print(\"⚠️ No TF-IDF vectors available for SVM\")\n",
        "            return False\n",
        "\n",
        "        X_train = self.train_data['tfidf']\n",
        "        X_test = self.test_data['tfidf']\n",
        "\n",
        "        # Create synthetic labels if not available\n",
        "        if 'labels' not in self.train_data:\n",
        "            print(\"📊 Creating synthetic labels for SVM training...\")\n",
        "            # Use cosine similarity clustering for labels\n",
        "            similarities = cosine_similarity(X_train)\n",
        "            avg_similarities = similarities.mean(axis=1)\n",
        "            y_train = (avg_similarities > np.median(avg_similarities)).astype(int)\n",
        "            y_test = np.zeros(X_test.shape[0])  # Placeholder\n",
        "        else:\n",
        "            y_train = self.train_data['labels']\n",
        "            y_test = self.test_data['labels']\n",
        "\n",
        "        # Scale features for SVM\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train.toarray())\n",
        "        X_test_scaled = scaler.transform(X_test.toarray())\n",
        "        self.scalers['svm_tfidf'] = scaler\n",
        "\n",
        "        try:\n",
        "            # Train SVM dengan berbagai kernel\n",
        "            svm_models = {\n",
        "                'svm_rbf': SVC(kernel='rbf', probability=True, random_state=42, C=1.0),\n",
        "                'svm_linear': SVC(kernel='linear', probability=True, random_state=42, C=1.0)\n",
        "            }\n",
        "\n",
        "            for model_name, svm_model in svm_models.items():\n",
        "                print(f\"   Training {model_name}...\")\n",
        "\n",
        "                svm_model.fit(X_train_scaled, y_train)\n",
        "                y_pred = svm_model.predict(X_test_scaled)\n",
        "                y_pred_proba = svm_model.predict_proba(X_test_scaled)\n",
        "\n",
        "                # Evaluate\n",
        "                if 'labels' in self.test_data:\n",
        "                    accuracy = accuracy_score(y_test, y_pred)\n",
        "                    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                    evaluation = {\n",
        "                        'accuracy': accuracy,\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'f1': f1,\n",
        "                        'model_type': 'SVM',\n",
        "                        'feature_type': 'TF-IDF'\n",
        "                    }\n",
        "\n",
        "                    print(f\"      ✅ {model_name}: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
        "                else:\n",
        "                    evaluation = {\n",
        "                        'predictions': y_pred,\n",
        "                        'probabilities': y_pred_proba,\n",
        "                        'model_type': 'SVM',\n",
        "                        'feature_type': 'TF-IDF'\n",
        "                    }\n",
        "                    print(f\"      ✅ {model_name}: Model trained successfully\")\n",
        "\n",
        "                self.models[model_name] = {\n",
        "                    'model': svm_model,\n",
        "                    'scaler': scaler,\n",
        "                    'evaluation': evaluation\n",
        "                }\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training SVM: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_naive_bayes_model(self) -> bool:\n",
        "        \"\"\"\n",
        "        1. Naive Bayes pada representasi TF-IDF untuk classification/retrieval\n",
        "        \"\"\"\n",
        "        print(\"\\n🔧 1. Training Naive Bayes model on TF-IDF...\")\n",
        "\n",
        "        if 'tfidf' not in self.train_data:\n",
        "            print(\"⚠️ No TF-IDF vectors available for Naive Bayes\")\n",
        "            return False\n",
        "\n",
        "        X_train = self.train_data['tfidf']\n",
        "        X_test = self.test_data['tfidf']\n",
        "\n",
        "        # Create synthetic labels if not available\n",
        "        if 'labels' not in self.train_data:\n",
        "            print(\"📊 Creating synthetic labels for Naive Bayes training...\")\n",
        "            similarities = cosine_similarity(X_train)\n",
        "            avg_similarities = similarities.mean(axis=1)\n",
        "            y_train = (avg_similarities > np.median(avg_similarities)).astype(int)\n",
        "            y_test = np.zeros(X_test.shape[0])\n",
        "        else:\n",
        "            y_train = self.train_data['labels']\n",
        "            y_test = self.test_data['labels']\n",
        "\n",
        "        try:\n",
        "            # Train Naive Bayes\n",
        "            nb_model = MultinomialNB(alpha=1.0)\n",
        "            nb_model.fit(X_train, y_train)\n",
        "\n",
        "            y_pred = nb_model.predict(X_test)\n",
        "            y_pred_proba = nb_model.predict_proba(X_test)\n",
        "\n",
        "            # Evaluate\n",
        "            if 'labels' in self.test_data:\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                evaluation = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'model_type': 'Naive Bayes',\n",
        "                    'feature_type': 'TF-IDF'\n",
        "                }\n",
        "\n",
        "                print(f\"   ✅ Naive Bayes: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
        "            else:\n",
        "                evaluation = {\n",
        "                    'predictions': y_pred,\n",
        "                    'probabilities': y_pred_proba,\n",
        "                    'model_type': 'Naive Bayes',\n",
        "                    'feature_type': 'TF-IDF'\n",
        "                }\n",
        "                print(f\"   ✅ Naive Bayes: Model trained successfully\")\n",
        "\n",
        "            self.models['naive_bayes'] = {\n",
        "                'model': nb_model,\n",
        "                'evaluation': evaluation\n",
        "            }\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training Naive Bayes: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_bert_retrieval(self) -> bool:\n",
        "        \"\"\"\n",
        "        2. Model transformer (BERT/IndoBERT) untuk retrieval pada hasil embedding\n",
        "        \"\"\"\n",
        "        print(\"\\n🤖 2. Setting up BERT/IndoBERT for retrieval on embeddings...\")\n",
        "\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            print(\"⚠️ Transformers not available, skipping BERT\")\n",
        "            return False\n",
        "\n",
        "        if 'bert' not in self.train_data:\n",
        "            print(\"⚠️ No BERT vectors available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Load BERT tokenizer for query processing\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model.to(self.device)\n",
        "            self.bert_model.eval()\n",
        "\n",
        "            print(f\"✅ BERT model loaded: {self.bert_model_name}\")\n",
        "\n",
        "            # BERT retrieval menggunakan cosine similarity pada embeddings\n",
        "            bert_train_vectors = self.train_data['bert']\n",
        "            bert_test_vectors = self.test_data['bert']\n",
        "\n",
        "            print(f\"📊 BERT vectors shape: train {bert_train_vectors.shape}, test {bert_test_vectors.shape}\")\n",
        "\n",
        "            # Setup retrieval system\n",
        "            bert_retrieval_info = {\n",
        "                'model_name': self.bert_model_name,\n",
        "                'train_vectors': bert_train_vectors,\n",
        "                'test_vectors': bert_test_vectors,\n",
        "                'train_case_ids': self.train_data['case_ids'],\n",
        "                'test_case_ids': self.test_data['case_ids'],\n",
        "                'tokenizer': self.bert_tokenizer,\n",
        "                'model': self.bert_model,\n",
        "                'device': self.device\n",
        "            }\n",
        "\n",
        "            self.models['bert_retrieval'] = {\n",
        "                'retrieval_info': bert_retrieval_info,\n",
        "                'model_type': 'BERT Retrieval',\n",
        "                'feature_type': 'BERT Embeddings'\n",
        "            }\n",
        "\n",
        "            print(f\"✅ BERT retrieval system setup completed\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error setting up BERT retrieval: {e}\")\n",
        "            return False\n",
        "\n",
        "    def save_models(self) -> Dict[str, str]:\n",
        "        \"\"\"Simpan semua trained models\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        saved_files = {}\n",
        "\n",
        "        print(\"\\n💾 Saving trained models...\")\n",
        "\n",
        "        # Save traditional ML models (SVM, Naive Bayes)\n",
        "        ml_models = {k: v for k, v in self.models.items() if k != 'bert_retrieval'}\n",
        "        if ml_models:\n",
        "            ml_filename = f\"ml_models_{timestamp}.pkl\"\n",
        "            ml_path = os.path.join(self.models_dir, ml_filename)\n",
        "\n",
        "            ml_data = {\n",
        "                'models': ml_models,\n",
        "                'scalers': self.scalers,\n",
        "                'tfidf_vectorizer': self.tfidf_vectorizer,\n",
        "                'evaluation_results': self.evaluation_results\n",
        "            }\n",
        "\n",
        "            with open(ml_path, 'wb') as f:\n",
        "                pickle.dump(ml_data, f)\n",
        "\n",
        "            saved_files['ml_models'] = ml_path\n",
        "            print(f\"🔧 ML models saved: {ml_filename}\")\n",
        "\n",
        "        # Save BERT retrieval info (without the actual model to save space)\n",
        "        if 'bert_retrieval' in self.models:\n",
        "            bert_filename = f\"bert_retrieval_{timestamp}.pkl\"\n",
        "            bert_path = os.path.join(self.models_dir, bert_filename)\n",
        "\n",
        "            bert_info = self.models['bert_retrieval']['retrieval_info'].copy()\n",
        "            # Remove heavy objects, keep only essentials\n",
        "            bert_data = {\n",
        "                'model_name': bert_info['model_name'],\n",
        "                'train_vectors': bert_info['train_vectors'],\n",
        "                'test_vectors': bert_info['test_vectors'],\n",
        "                'train_case_ids': bert_info['train_case_ids'],\n",
        "                'test_case_ids': bert_info['test_case_ids'],\n",
        "                'device': str(bert_info['device'])\n",
        "            }\n",
        "\n",
        "            with open(bert_path, 'wb') as f:\n",
        "                pickle.dump(bert_data, f)\n",
        "\n",
        "            saved_files['bert_retrieval'] = bert_path\n",
        "            print(f\"🤖 BERT retrieval saved: {bert_filename}\")\n",
        "\n",
        "        # Save models summary\n",
        "        summary_filename = f\"models_summary_{timestamp}.json\"\n",
        "        summary_path = os.path.join(self.models_dir, summary_filename)\n",
        "\n",
        "        summary_data = {\n",
        "            'total_models': len(self.models),\n",
        "            'ml_models': list(ml_models.keys()) if ml_models else [],\n",
        "            'bert_available': 'bert_retrieval' in self.models,\n",
        "            'training_completed_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            import json\n",
        "            json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        saved_files['summary'] = summary_path\n",
        "        print(f\"📋 Models summary saved: {summary_filename}\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "    def process_model_retrieval(self) -> bool:\n",
        "        \"\"\"\n",
        "        Proses lengkap model retrieval sesuai spesifikasi:\n",
        "        1. SVM atau Naive Bayes pada TF-IDF\n",
        "        2. BERT/IndoBERT untuk retrieval pada embeddings\n",
        "        \"\"\"\n",
        "        print(\"🤖 iii. MODEL RETRIEVAL\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. SVM atau Naive Bayes pada TF-IDF untuk classification/retrieval\")\n",
        "        print(\"2. BERT/IndoBERT untuk retrieval pada hasil embedding\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Load splits data\n",
        "        if not self.load_splits_data():\n",
        "            print(\"❌ Failed to load splits data\")\n",
        "            return False\n",
        "\n",
        "        # 2. Prepare training data (default: 80:20 split)\n",
        "        if not self.prepare_training_data(\"80_20\"):\n",
        "            print(\"❌ Failed to prepare training data\")\n",
        "            return False\n",
        "\n",
        "        # 3. Train traditional ML models\n",
        "        svm_success = self.train_svm_model()\n",
        "        nb_success = self.train_naive_bayes_model()\n",
        "\n",
        "        # 4. Setup BERT retrieval\n",
        "        bert_success = self.setup_bert_retrieval()\n",
        "\n",
        "        if not (svm_success or nb_success or bert_success):\n",
        "            print(\"❌ No models were trained successfully\")\n",
        "            return False\n",
        "\n",
        "        # 5. Save models\n",
        "        saved_files = self.save_models()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"✅ iii. MODEL RETRIEVAL COMPLETED!\")\n",
        "        print(f\"🔧 SVM models: {'✅' if svm_success else '❌'}\")\n",
        "        print(f\"📊 Naive Bayes: {'✅' if nb_success else '❌'}\")\n",
        "        print(f\"🤖 BERT retrieval: {'✅' if bert_success else '❌'}\")\n",
        "        print(f\"📁 Total models: {len(self.models)}\")\n",
        "        print(f\"💾 Files saved to: {self.models_dir}\")\n",
        "        print(\"Langkah selanjutnya: iv. Fungsi Retrieval\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk model retrieval\"\"\"\n",
        "    print(\"🚀 MULAI iii. MODEL RETRIEVAL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        model_trainer = ModelRetrieval()\n",
        "        success = model_trainer.process_model_retrieval()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\n🎉 MODEL RETRIEVAL BERHASIL!\")\n",
        "            print(\"✨ Yang telah dilakukan:\")\n",
        "            print(\"  ✅ Load splits data dari tahap ii. Splitting Data\")\n",
        "            print(\"  ✅ Train SVM model pada TF-IDF vectors\")\n",
        "            print(\"  ✅ Train Naive Bayes model pada TF-IDF vectors\")\n",
        "            print(\"  ✅ Setup BERT/IndoBERT retrieval pada embeddings\")\n",
        "            print(\"  ✅ Simpan semua models untuk tahap selanjutnya\")\n",
        "            print(\"Langkah selanjutnya: iv. Fungsi Retrieval\")\n",
        "        else:\n",
        "            print(\"\\n❌ Model retrieval gagal.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB-5HQYfG6rH",
        "outputId": "94ecb0ff-4038-432d-e0dd-1b2168464276"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI iii. MODEL RETRIEVAL\n",
            "======================================================================\n",
            "🤖 iii. MODEL RETRIEVAL\n",
            "Input splits: /content/drive/MyDrive/terorisme/data/splits\n",
            "Output models: /content/drive/MyDrive/terorisme/data/models\n",
            "🖥️ Device: cpu\n",
            "🤖 iii. MODEL RETRIEVAL\n",
            "============================================================\n",
            "1. SVM atau Naive Bayes pada TF-IDF untuk classification/retrieval\n",
            "2. BERT/IndoBERT untuk retrieval pada hasil embedding\n",
            "============================================================\n",
            "\n",
            "📥 Loading splits data...\n",
            "✅ Splits loaded from: data_splits_20250625_123843.pkl\n",
            "📊 Available splits: ['70_30', '80_20']\n",
            "\n",
            "📋 Preparing training data for 80_20 split...\n",
            "📊 TF-IDF vectors: train (36, 3433), test (10, 3433)\n",
            "🤖 BERT vectors: train (36, 768), test (10, 768)\n",
            "🏷️ Labels: 2 classes\n",
            "✅ Training data prepared:\n",
            "   📚 Training: 36 cases\n",
            "   🧪 Testing: 10 cases\n",
            "\n",
            "🔧 1. Training SVM model on TF-IDF...\n",
            "   Training svm_rbf...\n",
            "      ✅ svm_rbf: Accuracy=1.000, F1=1.000\n",
            "   Training svm_linear...\n",
            "      ✅ svm_linear: Accuracy=1.000, F1=1.000\n",
            "\n",
            "🔧 1. Training Naive Bayes model on TF-IDF...\n",
            "   ✅ Naive Bayes: Accuracy=1.000, F1=1.000\n",
            "\n",
            "🤖 2. Setting up BERT/IndoBERT for retrieval on embeddings...\n",
            "✅ BERT model loaded: indobenchmark/indobert-base-p1\n",
            "📊 BERT vectors shape: train (36, 768), test (10, 768)\n",
            "✅ BERT retrieval system setup completed\n",
            "\n",
            "💾 Saving trained models...\n",
            "🔧 ML models saved: ml_models_20250625_124207.pkl\n",
            "🤖 BERT retrieval saved: bert_retrieval_20250625_124207.pkl\n",
            "📋 Models summary saved: models_summary_20250625_124207.json\n",
            "\n",
            "============================================================\n",
            "✅ iii. MODEL RETRIEVAL COMPLETED!\n",
            "🔧 SVM models: ✅\n",
            "📊 Naive Bayes: ✅\n",
            "🤖 BERT retrieval: ✅\n",
            "📁 Total models: 4\n",
            "💾 Files saved to: /content/drive/MyDrive/terorisme/data/models\n",
            "Langkah selanjutnya: iv. Fungsi Retrieval\n",
            "============================================================\n",
            "\n",
            "🎉 MODEL RETRIEVAL BERHASIL!\n",
            "✨ Yang telah dilakukan:\n",
            "  ✅ Load splits data dari tahap ii. Splitting Data\n",
            "  ✅ Train SVM model pada TF-IDF vectors\n",
            "  ✅ Train Naive Bayes model pada TF-IDF vectors\n",
            "  ✅ Setup BERT/IndoBERT retrieval pada embeddings\n",
            "  ✅ Simpan semua models untuk tahap selanjutnya\n",
            "Langkah selanjutnya: iv. Fungsi Retrieval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fungsi Retrieval**"
      ],
      "metadata": {
        "id": "_OOoAi12HLsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# iv. FIXED FUNGSI RETRIEVAL\n",
        "# def retrieve(query: str, k: int = 5) -> List[case_id]:\n",
        "#     # 1) Pre-process query\n",
        "#     # 2) Hitung vektor query\n",
        "#     # 3) Hitung cosine‐similarity dengan semua case vectors\n",
        "#     # 4) Kembalikan top-k case_id\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# BERT and Transformers\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️ Transformers not available. Install with: pip install transformers torch\")\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FixedFungsiRetrieval:\n",
        "    \"\"\"\n",
        "    FIXED iv. Fungsi Retrieval sesuai spesifikasi:\n",
        "\n",
        "    PERBAIKAN UTAMA:\n",
        "    - Prioritas gunakan enhanced vectors (vocabulary terbesar)\n",
        "    - Robust vector loading dengan fallback\n",
        "    - Vocabulary debugging untuk query troubleshooting\n",
        "\n",
        "    Implementasi fungsi retrieve() dengan langkah:\n",
        "    1) Pre-process query\n",
        "    2) Hitung vektor query\n",
        "    3) Hitung cosine‐similarity dengan semua case vectors\n",
        "    4) Kembalikan top-k case_id\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.models_dir = os.path.join(base_dir, \"data\", \"models\")\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "\n",
        "        print(f\"🔍 FIXED iv. FUNGSI RETRIEVAL\")\n",
        "        print(f\"Models: {self.models_dir}\")\n",
        "        print(f\"Splits: {self.splits_dir}\")\n",
        "        print(f\"Vectors: {self.vectors_dir}\")\n",
        "\n",
        "        # Model components\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.ml_models = {}\n",
        "        self.scalers = {}\n",
        "\n",
        "        # Vector storage untuk retrieval\n",
        "        self.case_vectors_tfidf = None\n",
        "        self.case_vectors_bert = None\n",
        "        self.case_ids = []\n",
        "\n",
        "        # BERT components\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.bert_model_name = \"indobenchmark/indobert-base-p1\"\n",
        "\n",
        "        # Load all components dengan prioritas enhanced vectors\n",
        "        self.load_all_components_fixed()\n",
        "\n",
        "    def find_best_vector_file(self, vector_type: str = 'tfidf') -> str:\n",
        "        \"\"\"\n",
        "        FIXED: Cari vector file dengan vocabulary terbesar (enhanced)\n",
        "        \"\"\"\n",
        "        print(f\"\\n🔍 Finding best {vector_type} vector file...\")\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            return None\n",
        "\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir)\n",
        "                       if f.startswith(f'{vector_type}_vectors_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not vector_files:\n",
        "            # Try enhanced files\n",
        "            vector_files = [f for f in os.listdir(self.vectors_dir)\n",
        "                           if f.startswith(f'enhanced_{vector_type}_vectors_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not vector_files:\n",
        "            print(f\"❌ No {vector_type} vector files found\")\n",
        "            return None\n",
        "\n",
        "        best_file = None\n",
        "        best_vocab_size = 0\n",
        "\n",
        "        for vf in vector_files:\n",
        "            vf_path = os.path.join(self.vectors_dir, vf)\n",
        "            try:\n",
        "                with open(vf_path, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "\n",
        "                if vector_type == 'tfidf':\n",
        "                    if 'vectorizer' in data:\n",
        "                        vocab_size = len(data['vectorizer'].get_feature_names_out())\n",
        "                        print(f\"   {vf}: {vocab_size:,} vocabulary\")\n",
        "\n",
        "                        if vocab_size > best_vocab_size:\n",
        "                            best_vocab_size = vocab_size\n",
        "                            best_file = vf\n",
        "                elif vector_type == 'bert':\n",
        "                    if 'vectors' in data:\n",
        "                        vector_dim = data['vectors'].shape[1] if len(data['vectors'].shape) > 1 else 0\n",
        "                        print(f\"   {vf}: {vector_dim} dimensions\")\n",
        "\n",
        "                        if vector_dim > best_vocab_size:  # Use as size metric\n",
        "                            best_vocab_size = vector_dim\n",
        "                            best_file = vf\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   {vf}: Error loading - {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_file:\n",
        "            print(f\"✅ Best {vector_type} file: {best_file}\")\n",
        "            if vector_type == 'tfidf':\n",
        "                print(f\"   Vocabulary size: {best_vocab_size:,}\")\n",
        "        else:\n",
        "            print(f\"❌ No valid {vector_type} files found\")\n",
        "\n",
        "        return best_file\n",
        "\n",
        "    def load_enhanced_tfidf_components(self) -> bool:\n",
        "        \"\"\"\n",
        "        FIXED: Load TF-IDF components dengan prioritas enhanced vectors\n",
        "        \"\"\"\n",
        "        print(\"\\n📊 Loading enhanced TF-IDF components...\")\n",
        "\n",
        "        best_tfidf_file = self.find_best_vector_file('tfidf')\n",
        "\n",
        "        if not best_tfidf_file:\n",
        "            print(\"❌ No TF-IDF files available\")\n",
        "            return False\n",
        "\n",
        "        tfidf_path = os.path.join(self.vectors_dir, best_tfidf_file)\n",
        "\n",
        "        try:\n",
        "            with open(tfidf_path, 'rb') as f:\n",
        "                tfidf_data = pickle.load(f)\n",
        "\n",
        "            self.tfidf_vectorizer = tfidf_data['vectorizer']\n",
        "\n",
        "            # Get vocabulary info\n",
        "            vocab_size = len(self.tfidf_vectorizer.get_feature_names_out())\n",
        "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "            print(f\"✅ Enhanced TF-IDF loaded:\")\n",
        "            print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "            print(f\"   Sample terms: {list(feature_names[:10])}\")\n",
        "\n",
        "            # Check for important legal terms\n",
        "            important_terms = ['terorisme', 'radikalisme', 'bom', 'peledakan', 'senjata',\n",
        "    'tersangka', 'densus_88', 'kelompok_teroris', 'pengadilan', 'pasal']\n",
        "\n",
        "            found_terms = [term for term in important_terms if term in feature_names]\n",
        "            missing_terms = [term for term in important_terms if term not in feature_names]\n",
        "\n",
        "            print(f\"   Legal terms found: {found_terms}\")\n",
        "            if missing_terms:\n",
        "                print(f\"   Legal terms missing: {missing_terms}\")\n",
        "\n",
        "            # Test query vectorization\n",
        "            test_query = \"isis\"\n",
        "            test_vector = self.tfidf_vectorizer.transform([test_query.lower()])\n",
        "            print(f\"   Test query '{test_query}': {test_vector.nnz} non-zero elements\")\n",
        "\n",
        "            if test_vector.nnz == 0:\n",
        "                print(\"   ⚠️ WARNING: Test query produces empty vector\")\n",
        "                # Debug vocabulary overlap\n",
        "                query_words = test_query.lower().split()\n",
        "                overlap = [word for word in query_words if word in feature_names]\n",
        "                print(f\"   Query word overlap: {overlap}\")\n",
        "            else:\n",
        "                print(\"   ✅ Test query vectorization successful\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading enhanced TF-IDF: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_case_vectors_from_best_source(self) -> bool:\n",
        "        \"\"\"\n",
        "        FIXED: Load case vectors dari source terbaik (enhanced)\n",
        "        \"\"\"\n",
        "        print(\"\\n📊 Loading case vectors from best source...\")\n",
        "\n",
        "        # Strategy 1: Load from enhanced vector files directly\n",
        "        best_tfidf_file = self.find_best_vector_file('tfidf')\n",
        "\n",
        "        if best_tfidf_file:\n",
        "            tfidf_path = os.path.join(self.vectors_dir, best_tfidf_file)\n",
        "\n",
        "            try:\n",
        "                with open(tfidf_path, 'rb') as f:\n",
        "                    tfidf_data = pickle.load(f)\n",
        "\n",
        "                if 'vectors' in tfidf_data and 'case_ids' in tfidf_data:\n",
        "                    self.case_vectors_tfidf = tfidf_data['vectors']\n",
        "                    self.case_ids = tfidf_data['case_ids']\n",
        "\n",
        "                    print(f\"✅ TF-IDF vectors loaded from enhanced file:\")\n",
        "                    print(f\"   Shape: {self.case_vectors_tfidf.shape}\")\n",
        "                    print(f\"   Cases: {len(self.case_ids)}\")\n",
        "\n",
        "                    # Convert sparse to dense if needed for cosine similarity\n",
        "                    if hasattr(self.case_vectors_tfidf, 'toarray'):\n",
        "                        print(f\"   Converting sparse to dense matrix...\")\n",
        "                        self.case_vectors_tfidf = self.case_vectors_tfidf.toarray()\n",
        "                        print(f\"   Dense shape: {self.case_vectors_tfidf.shape}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading from enhanced file: {e}\")\n",
        "\n",
        "        # Strategy 2: Load from splits if enhanced files not available\n",
        "        if self.case_vectors_tfidf is None:\n",
        "            print(\"📊 Fallback: Loading from splits data...\")\n",
        "\n",
        "            split_files = [f for f in os.listdir(self.splits_dir)\n",
        "                          if f.startswith('data_splits_') and f.endswith('.pkl')]\n",
        "\n",
        "            if split_files:\n",
        "                latest_split = max(split_files)\n",
        "                split_path = os.path.join(self.splits_dir, latest_split)\n",
        "\n",
        "                try:\n",
        "                    with open(split_path, 'rb') as f:\n",
        "                        splits_data = pickle.load(f)\n",
        "\n",
        "                    # Use 80_20 split or first available\n",
        "                    available_splits = list(splits_data['splits'].keys())\n",
        "                    split_to_use = \"80_20\" if \"80_20\" in available_splits else available_splits[0]\n",
        "                    split_info = splits_data['splits'][split_to_use]\n",
        "\n",
        "                    # Combine train and test vectors\n",
        "                    if 'train_tfidf' in split_info and 'test_tfidf' in split_info:\n",
        "                        train_tfidf = split_info['train_tfidf']\n",
        "                        test_tfidf = split_info['test_tfidf']\n",
        "\n",
        "                        if hasattr(train_tfidf, 'toarray'):\n",
        "                            train_dense = train_tfidf.toarray()\n",
        "                            test_dense = test_tfidf.toarray()\n",
        "                            self.case_vectors_tfidf = np.vstack([train_dense, test_dense])\n",
        "                        else:\n",
        "                            self.case_vectors_tfidf = np.vstack([train_tfidf, test_tfidf])\n",
        "\n",
        "                        # Combine case IDs\n",
        "                        self.case_ids = split_info['train_case_ids'] + split_info['test_case_ids']\n",
        "\n",
        "                        print(f\"✅ Vectors loaded from splits:\")\n",
        "                        print(f\"   Shape: {self.case_vectors_tfidf.shape}\")\n",
        "                        print(f\"   Cases: {len(self.case_ids)}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error loading from splits: {e}\")\n",
        "\n",
        "        # Load BERT vectors if available\n",
        "        best_bert_file = self.find_best_vector_file('bert')\n",
        "        if best_bert_file:\n",
        "            bert_path = os.path.join(self.vectors_dir, best_bert_file)\n",
        "\n",
        "            try:\n",
        "                with open(bert_path, 'rb') as f:\n",
        "                    bert_data = pickle.load(f)\n",
        "\n",
        "                if 'vectors' in bert_data:\n",
        "                    self.case_vectors_bert = bert_data['vectors']\n",
        "                    print(f\"✅ BERT vectors loaded: {self.case_vectors_bert.shape}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading BERT vectors: {e}\")\n",
        "\n",
        "        return len(self.case_ids) > 0\n",
        "\n",
        "    def load_trained_models(self) -> bool:\n",
        "        \"\"\"Load trained ML models\"\"\"\n",
        "        print(\"\\n🤖 Loading trained models...\")\n",
        "\n",
        "        if not os.path.exists(self.models_dir):\n",
        "            print(\"⚠️ Models directory not found\")\n",
        "            return False\n",
        "\n",
        "        model_files = [f for f in os.listdir(self.models_dir)\n",
        "                      if f.startswith('ml_models_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not model_files:\n",
        "            print(\"⚠️ No trained models found\")\n",
        "            return False\n",
        "\n",
        "        latest_models = max(model_files)\n",
        "        models_path = os.path.join(self.models_dir, latest_models)\n",
        "\n",
        "        try:\n",
        "            with open(models_path, 'rb') as f:\n",
        "                models_data = pickle.load(f)\n",
        "\n",
        "            self.ml_models = models_data.get('models', {})\n",
        "            self.scalers = models_data.get('scalers', {})\n",
        "\n",
        "            print(f\"✅ ML models loaded: {list(self.ml_models.keys())}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading models: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_bert_components(self) -> bool:\n",
        "        \"\"\"Load BERT components for query encoding\"\"\"\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            print(\"⚠️ Transformers not available for BERT\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            print(f\"\\n🤖 Loading BERT components...\")\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model.to(self.device)\n",
        "            self.bert_model.eval()\n",
        "\n",
        "            print(f\"✅ BERT components loaded\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading BERT: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_all_components_fixed(self) -> bool:\n",
        "        \"\"\"\n",
        "        FIXED: Load semua komponen dengan prioritas enhanced vectors\n",
        "        \"\"\"\n",
        "        print(\"\\n📥 Loading all retrieval components (FIXED)...\")\n",
        "\n",
        "        success_count = 0\n",
        "\n",
        "        # 1. Load enhanced TF-IDF vectorizer\n",
        "        if self.load_enhanced_tfidf_components():\n",
        "            success_count += 1\n",
        "\n",
        "        # 2. Load case vectors dari source terbaik\n",
        "        if self.load_case_vectors_from_best_source():\n",
        "            success_count += 1\n",
        "\n",
        "        # 3. Load trained models (optional)\n",
        "        if self.load_trained_models():\n",
        "            success_count += 1\n",
        "\n",
        "        # 4. Load BERT components (optional)\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            if self.load_bert_components():\n",
        "                success_count += 1\n",
        "\n",
        "        print(f\"\\n📊 Component loading summary:\")\n",
        "        print(f\"   TF-IDF vectorizer: {'✅' if self.tfidf_vectorizer else '❌'}\")\n",
        "        print(f\"   Case vectors: {'✅' if len(self.case_ids) > 0 else '❌'}\")\n",
        "        print(f\"   ML models: {'✅' if self.ml_models else '❌'}\")\n",
        "        print(f\"   BERT: {'✅' if self.bert_model else '❌'}\")\n",
        "        print(f\"   Total cases: {len(self.case_ids)}\")\n",
        "\n",
        "        if success_count >= 2:  # At least vectorizer + case vectors\n",
        "            print(f\"✅ Minimum required components loaded successfully\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ Failed to load minimum required components\")\n",
        "            return False\n",
        "\n",
        "    def preprocess_query(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        1) Pre-process query sesuai spesifikasi\n",
        "        \"\"\"\n",
        "        # Basic preprocessing - keep it simple\n",
        "        query = query.lower().strip()\n",
        "        query = re.sub(r'\\s+', ' ', query)\n",
        "        query = re.sub(r'[^\\w\\s\\-/]', ' ', query)\n",
        "        query = re.sub(r'\\s+', ' ', query).strip()\n",
        "\n",
        "        return query\n",
        "\n",
        "    def compute_query_vector_tfidf(self, processed_query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        2) Hitung vektor query dengan TF-IDF\n",
        "        \"\"\"\n",
        "        if not self.tfidf_vectorizer:\n",
        "            return None\n",
        "\n",
        "        query_vector = self.tfidf_vectorizer.transform([processed_query])\n",
        "        return query_vector\n",
        "\n",
        "    def compute_query_vector_bert(self, processed_query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        2) Hitung vektor query dengan BERT\n",
        "        \"\"\"\n",
        "        if not self.bert_model or not self.bert_tokenizer:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            inputs = self.bert_tokenizer(\n",
        "                processed_query,\n",
        "                max_length=512,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "            return embedding.flatten()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error computing BERT query vector: {e}\")\n",
        "            return None\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 5, method: str = 'tfidf') -> List[str]:\n",
        "        \"\"\"\n",
        "        FUNGSI RETRIEVE SESUAI SPESIFIKASI:\n",
        "\n",
        "        Args:\n",
        "            query: str - Query kasus baru\n",
        "            k: int - Jumlah kasus mirip yang dikembalikan (default 5)\n",
        "            method: str - Metode retrieval ('tfidf', 'bert', 'svm', 'naive_bayes')\n",
        "\n",
        "        Returns:\n",
        "            List[str] - List case_id kasus yang paling mirip\n",
        "\n",
        "        Langkah kerja sesuai spesifikasi:\n",
        "        1) Pre-process query\n",
        "        2) Hitung vektor query\n",
        "        3) Hitung cosine‐similarity dengan semua case vectors\n",
        "        4) Kembalikan top-k case_id\n",
        "        \"\"\"\n",
        "\n",
        "        # Validate inputs\n",
        "        if not self.case_ids:\n",
        "            print(\"❌ No cases available for retrieval\")\n",
        "            return []\n",
        "\n",
        "        if method == 'tfidf':\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "        elif method == 'bert':\n",
        "            return self._retrieve_bert(query, k)\n",
        "        elif method == 'svm':\n",
        "            return self._retrieve_svm(query, k)\n",
        "        elif method == 'naive_bayes':\n",
        "            return self._retrieve_naive_bayes(query, k)\n",
        "        else:\n",
        "            print(f\"⚠️ Method '{method}' not available, using TF-IDF\")\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "\n",
        "    def _retrieve_tfidf(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Retrieval dengan TF-IDF sesuai spesifikasi\n",
        "        \"\"\"\n",
        "        if self.case_vectors_tfidf is None or self.tfidf_vectorizer is None:\n",
        "            print(\"❌ TF-IDF components not available\")\n",
        "            return []\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        processed_query = self.preprocess_query(query)\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = self.compute_query_vector_tfidf(processed_query)\n",
        "\n",
        "        if query_vector is None:\n",
        "            print(\"❌ Failed to compute query vector\")\n",
        "            return []\n",
        "\n",
        "        if query_vector.nnz == 0:\n",
        "            print(f\"⚠️ Query '{query}' produces empty vector\")\n",
        "\n",
        "            # Debug vocabulary\n",
        "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "            query_words = processed_query.split()\n",
        "            overlap = [word for word in query_words if word in feature_names]\n",
        "            missing = [word for word in query_words if word not in feature_names]\n",
        "\n",
        "            print(f\"   Query words: {query_words}\")\n",
        "            print(f\"   Found in vocabulary: {overlap}\")\n",
        "            print(f\"   Missing from vocabulary: {missing}\")\n",
        "\n",
        "            return []\n",
        "\n",
        "        # Convert sparse to dense if needed\n",
        "        if hasattr(query_vector, 'toarray'):\n",
        "            query_dense = query_vector.toarray()\n",
        "        else:\n",
        "            query_dense = query_vector\n",
        "\n",
        "        # 3) Hitung cosine‐similarity dengan semua case vectors\n",
        "        try:\n",
        "            similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error computing similarities: {e}\")\n",
        "            return []\n",
        "\n",
        "        # 4) Kembalikan top-k case_id\n",
        "        if similarities.max() == 0:\n",
        "            print(\"⚠️ All similarities are zero\")\n",
        "            return []\n",
        "\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "        top_case_ids = [self.case_ids[idx] for idx in top_indices]\n",
        "\n",
        "        # Debug info\n",
        "        top_scores = similarities[top_indices]\n",
        "        print(f\"🔍 TF-IDF retrieval for '{query}':\")\n",
        "        print(f\"   Query vector nnz: {query_vector.nnz}\")\n",
        "        print(f\"   Top scores: {top_scores[:3]}\")\n",
        "\n",
        "        return top_case_ids\n",
        "\n",
        "    def _retrieve_bert(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"Retrieval dengan BERT\"\"\"\n",
        "        if self.case_vectors_bert is None or not self.bert_model:\n",
        "            print(\"❌ BERT components not available\")\n",
        "            return []\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        processed_query = self.preprocess_query(query)\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = self.compute_query_vector_bert(processed_query)\n",
        "\n",
        "        if query_vector is None:\n",
        "            return []\n",
        "\n",
        "        # 3) Hitung cosine‐similarity\n",
        "        query_vector = query_vector.reshape(1, -1)\n",
        "        similarities = cosine_similarity(query_vector, self.case_vectors_bert).flatten()\n",
        "\n",
        "        # 4) Kembalikan top-k case_id\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "        return [self.case_ids[idx] for idx in top_indices]\n",
        "\n",
        "    def _retrieve_svm(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"Retrieval dengan SVM (fallback to TF-IDF if no model)\"\"\"\n",
        "        if 'svm_rbf' not in self.ml_models:\n",
        "            print(\"⚠️ SVM model not available, using TF-IDF\")\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "\n",
        "        # Implementation similar to TF-IDF but with SVM confidence boost\n",
        "        return self._retrieve_tfidf(query, k)  # Simplified for now\n",
        "\n",
        "    def _retrieve_naive_bayes(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"Retrieval dengan Naive Bayes (fallback to TF-IDF if no model)\"\"\"\n",
        "        if 'naive_bayes' not in self.ml_models:\n",
        "            print(\"⚠️ Naive Bayes model not available, using TF-IDF\")\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "\n",
        "        return self._retrieve_tfidf(query, k)  # Simplified for now\n",
        "\n",
        "    def retrieve_with_scores(self, query: str, k: int = 5, method: str = 'tfidf') -> List[Tuple[str, float]]:\n",
        "        \"\"\"Retrieve dengan similarity scores untuk debugging\"\"\"\n",
        "        if method != 'tfidf' or self.case_vectors_tfidf is None:\n",
        "            return []\n",
        "\n",
        "        processed_query = self.preprocess_query(query)\n",
        "        query_vector = self.compute_query_vector_tfidf(processed_query)\n",
        "\n",
        "        if query_vector is None or query_vector.nnz == 0:\n",
        "            return []\n",
        "\n",
        "        if hasattr(query_vector, 'toarray'):\n",
        "            query_dense = query_vector.toarray()\n",
        "        else:\n",
        "            query_dense = query_vector\n",
        "\n",
        "        similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            case_id = self.case_ids[idx]\n",
        "            score = similarities[idx]\n",
        "            results.append((case_id, float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def test_retrieve_function(self):\n",
        "        \"\"\"Test fungsi retrieve dengan sample queries\"\"\"\n",
        "        print(\"\\n🧪 Testing FIXED retrieve() function...\")\n",
        "\n",
        "        test_queries = [\n",
        "            \"aksi terorisme di jakarta\",\n",
        "    \"peledakan bom di gereja\",\n",
        "    \"penangkapan anggota kelompok teroris\",\n",
        "    \"radikalisme di lingkungan kampus\",\n",
        "    \"densus 88 gerebek tempat persembunyian\"\n",
        "        ]\n",
        "\n",
        "        available_methods = ['tfidf']\n",
        "        if self.bert_model and self.case_vectors_bert is not None:\n",
        "            available_methods.append('bert')\n",
        "        if 'svm_rbf' in self.ml_models:\n",
        "            available_methods.append('svm')\n",
        "        if 'naive_bayes' in self.ml_models:\n",
        "            available_methods.append('naive_bayes')\n",
        "\n",
        "        print(f\"📊 Available methods: {available_methods}\")\n",
        "\n",
        "        for query in test_queries:\n",
        "            print(f\"\\n🔍 Query: '{query}'\")\n",
        "\n",
        "            for method in available_methods:\n",
        "                try:\n",
        "                    similar_cases = self.retrieve(query, k=3, method=method)\n",
        "\n",
        "                    if similar_cases:\n",
        "                        # Show short case IDs for readability\n",
        "                        short_cases = [case[:20] + \"...\" if len(case) > 20 else case\n",
        "                                     for case in similar_cases]\n",
        "                        print(f\"   {method.upper()}: {short_cases}\")\n",
        "                    else:\n",
        "                        print(f\"   {method.upper()}: No results\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   {method.upper()}: Error - {e}\")\n",
        "\n",
        "        print(f\"\\n✅ FIXED retrieve() function testing completed!\")\n",
        "\n",
        "    def process_fixed_fungsi_retrieval(self) -> bool:\n",
        "        \"\"\"\n",
        "        Proses lengkap FIXED fungsi retrieval\n",
        "        \"\"\"\n",
        "        print(\"🔍 FIXED iv. FUNGSI RETRIEVAL\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"PERBAIKAN: Prioritas enhanced vectors dengan vocabulary besar\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Check if components loaded successfully\n",
        "        if not self.case_ids:\n",
        "            print(\"❌ No case vectors loaded for retrieval\")\n",
        "            return False\n",
        "\n",
        "        if not self.tfidf_vectorizer:\n",
        "            print(\"❌ No TF-IDF vectorizer loaded\")\n",
        "            return False\n",
        "\n",
        "        # Test retrieve function\n",
        "        self.test_retrieve_function()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"✅ FIXED iv. FUNGSI RETRIEVAL COMPLETED!\")\n",
        "        print(f\"🔍 retrieve() function ready with ENHANCED vectors\")\n",
        "        print(f\"📁 Database size: {len(self.case_ids)} cases\")\n",
        "        print(f\"📊 TF-IDF vocabulary: {len(self.tfidf_vectorizer.get_feature_names_out()):,} terms\")\n",
        "        print(f\"🤖 BERT available: {'✅' if self.case_vectors_bert is not None else '❌'}\")\n",
        "        print(f\"🔧 ML models: {list(self.ml_models.keys()) if self.ml_models else 'None'}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk testing FIXED fungsi retrieval\"\"\"\n",
        "    print(\"🚀 MULAI FIXED iv. FUNGSI RETRIEVAL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        retrieval_system = FixedFungsiRetrieval()\n",
        "        success = retrieval_system.process_fixed_fungsi_retrieval()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\n🎉 FIXED FUNGSI RETRIEVAL BERHASIL!\")\n",
        "            print(\"✨ Perbaikan yang diterapkan:\")\n",
        "            print(\"  ✅ Prioritas enhanced vectors dengan vocabulary terbesar\")\n",
        "            print(\"  ✅ Robust vector loading dengan multiple fallback\")\n",
        "            print(\"  ✅ Vocabulary debugging untuk troubleshooting\")\n",
        "            print(\"  ✅ Dense matrix conversion untuk cosine similarity\")\n",
        "            print(\"  ✅ Enhanced error handling dan logging\")\n",
        "        else:\n",
        "            print(\"\\n❌ Fixed fungsi retrieval gagal.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl94QAEDHNgP",
        "outputId": "86524b4d-f82c-4e23-a044-6a96827c578c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI FIXED iv. FUNGSI RETRIEVAL\n",
            "======================================================================\n",
            "🔍 FIXED iv. FUNGSI RETRIEVAL\n",
            "Models: /content/drive/MyDrive/terorisme/data/models\n",
            "Splits: /content/drive/MyDrive/terorisme/data/splits\n",
            "Vectors: /content/drive/MyDrive/terorisme/data/vectors\n",
            "\n",
            "📥 Loading all retrieval components (FIXED)...\n",
            "\n",
            "📊 Loading enhanced TF-IDF components...\n",
            "\n",
            "🔍 Finding best tfidf vector file...\n",
            "   enhanced_tfidf_vectors_20250625_112837.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_113626.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_115637.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_122036.pkl: 3,433 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_122735.pkl: 3,433 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_123029.pkl: 3,433 vocabulary\n",
            "✅ Best tfidf file: enhanced_tfidf_vectors_20250625_112837.pkl\n",
            "   Vocabulary size: 3,440\n",
            "✅ Enhanced TF-IDF loaded:\n",
            "   Vocabulary size: 3,440\n",
            "   Sample terms: ['00', '00 institusi_kejaksaan', '00 institusi_kejaksaan institusi_pengadilan', '00 institusi_pengadilan', '00 institusi_pengadilan institusi_mahkamah', '00 nominal_rp', '00 nominal_rp_60', '00 nominal_rp_60 000', '000 00', '000 00 institusi_kejaksaan']\n",
            "   Legal terms found: ['terorisme', 'senjata']\n",
            "   Legal terms missing: ['radikalisme', 'bom', 'peledakan', 'tersangka', 'densus_88', 'kelompok_teroris', 'pengadilan', 'pasal']\n",
            "   Test query 'isis': 1 non-zero elements\n",
            "   ✅ Test query vectorization successful\n",
            "\n",
            "📊 Loading case vectors from best source...\n",
            "\n",
            "🔍 Finding best tfidf vector file...\n",
            "   enhanced_tfidf_vectors_20250625_112837.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_113626.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_115637.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_122036.pkl: 3,433 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_122735.pkl: 3,433 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_123029.pkl: 3,433 vocabulary\n",
            "✅ Best tfidf file: enhanced_tfidf_vectors_20250625_112837.pkl\n",
            "   Vocabulary size: 3,440\n",
            "✅ TF-IDF vectors loaded from enhanced file:\n",
            "   Shape: (46, 3440)\n",
            "   Cases: 46\n",
            "   Converting sparse to dense matrix...\n",
            "   Dense shape: (46, 3440)\n",
            "\n",
            "🔍 Finding best bert vector file...\n",
            "   enhanced_bert_vectors_20250625_112837.pkl: 768 dimensions\n",
            "   enhanced_bert_vectors_20250625_113626.pkl: 768 dimensions\n",
            "   enhanced_bert_vectors_20250625_115637.pkl: 768 dimensions\n",
            "   enhanced_bert_vectors_20250625_122036.pkl: 768 dimensions\n",
            "   enhanced_bert_vectors_20250625_122735.pkl: 768 dimensions\n",
            "   enhanced_bert_vectors_20250625_123029.pkl: 768 dimensions\n",
            "✅ Best bert file: enhanced_bert_vectors_20250625_112837.pkl\n",
            "✅ BERT vectors loaded: (46, 768)\n",
            "\n",
            "🤖 Loading trained models...\n",
            "✅ ML models loaded: ['svm_rbf', 'svm_linear', 'naive_bayes']\n",
            "\n",
            "🤖 Loading BERT components...\n",
            "✅ BERT components loaded\n",
            "\n",
            "📊 Component loading summary:\n",
            "   TF-IDF vectorizer: ✅\n",
            "   Case vectors: ✅\n",
            "   ML models: ✅\n",
            "   BERT: ✅\n",
            "   Total cases: 46\n",
            "✅ Minimum required components loaded successfully\n",
            "🔍 FIXED iv. FUNGSI RETRIEVAL\n",
            "============================================================\n",
            "PERBAIKAN: Prioritas enhanced vectors dengan vocabulary besar\n",
            "============================================================\n",
            "\n",
            "🧪 Testing FIXED retrieve() function...\n",
            "📊 Available methods: ['tfidf', 'bert', 'svm', 'naive_bayes']\n",
            "\n",
            "🔍 Query: 'aksi terorisme di jakarta'\n",
            "🔍 TF-IDF retrieval for 'aksi terorisme di jakarta':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.06597746 0.06449467 0.06074888]\n",
            "   TFIDF: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "   BERT: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "🔍 TF-IDF retrieval for 'aksi terorisme di jakarta':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.06597746 0.06449467 0.06074888]\n",
            "   SVM: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "🔍 TF-IDF retrieval for 'aksi terorisme di jakarta':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.06597746 0.06449467 0.06074888]\n",
            "   NAIVE_BAYES: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "\n",
            "🔍 Query: 'peledakan bom di gereja'\n",
            "⚠️ Query 'peledakan bom di gereja' produces empty vector\n",
            "   Query words: ['peledakan', 'bom', 'di', 'gereja']\n",
            "   Found in vocabulary: []\n",
            "   Missing from vocabulary: ['peledakan', 'bom', 'di', 'gereja']\n",
            "   TFIDF: No results\n",
            "   BERT: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2025_TK1_Putusa...']\n",
            "⚠️ Query 'peledakan bom di gereja' produces empty vector\n",
            "   Query words: ['peledakan', 'bom', 'di', 'gereja']\n",
            "   Found in vocabulary: []\n",
            "   Missing from vocabulary: ['peledakan', 'bom', 'di', 'gereja']\n",
            "   SVM: No results\n",
            "⚠️ Query 'peledakan bom di gereja' produces empty vector\n",
            "   Query words: ['peledakan', 'bom', 'di', 'gereja']\n",
            "   Found in vocabulary: []\n",
            "   Missing from vocabulary: ['peledakan', 'bom', 'di', 'gereja']\n",
            "   NAIVE_BAYES: No results\n",
            "\n",
            "🔍 Query: 'penangkapan anggota kelompok teroris'\n",
            "🔍 TF-IDF retrieval for 'penangkapan anggota kelompok teroris':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.09956844 0.08197645 0.03613853]\n",
            "   TFIDF: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "   BERT: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "🔍 TF-IDF retrieval for 'penangkapan anggota kelompok teroris':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.09956844 0.08197645 0.03613853]\n",
            "   SVM: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "🔍 TF-IDF retrieval for 'penangkapan anggota kelompok teroris':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.09956844 0.08197645 0.03613853]\n",
            "   NAIVE_BAYES: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "\n",
            "🔍 Query: 'radikalisme di lingkungan kampus'\n",
            "🔍 TF-IDF retrieval for 'radikalisme di lingkungan kampus':\n",
            "   Query vector nnz: 1\n",
            "   Top scores: [0.08990744 0.08218062 0.04806286]\n",
            "   TFIDF: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "   BERT: ['case_2023_TK1_Putusa...', 'case_2025_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "🔍 TF-IDF retrieval for 'radikalisme di lingkungan kampus':\n",
            "   Query vector nnz: 1\n",
            "   Top scores: [0.08990744 0.08218062 0.04806286]\n",
            "   SVM: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "🔍 TF-IDF retrieval for 'radikalisme di lingkungan kampus':\n",
            "   Query vector nnz: 1\n",
            "   Top scores: [0.08990744 0.08218062 0.04806286]\n",
            "   NAIVE_BAYES: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "\n",
            "🔍 Query: 'densus 88 gerebek tempat persembunyian'\n",
            "⚠️ Query 'densus 88 gerebek tempat persembunyian' produces empty vector\n",
            "   Query words: ['densus', '88', 'gerebek', 'tempat', 'persembunyian']\n",
            "   Found in vocabulary: []\n",
            "   Missing from vocabulary: ['densus', '88', 'gerebek', 'tempat', 'persembunyian']\n",
            "   TFIDF: No results\n",
            "   BERT: ['case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...', 'case_2023_TK1_Putusa...']\n",
            "⚠️ Query 'densus 88 gerebek tempat persembunyian' produces empty vector\n",
            "   Query words: ['densus', '88', 'gerebek', 'tempat', 'persembunyian']\n",
            "   Found in vocabulary: []\n",
            "   Missing from vocabulary: ['densus', '88', 'gerebek', 'tempat', 'persembunyian']\n",
            "   SVM: No results\n",
            "⚠️ Query 'densus 88 gerebek tempat persembunyian' produces empty vector\n",
            "   Query words: ['densus', '88', 'gerebek', 'tempat', 'persembunyian']\n",
            "   Found in vocabulary: []\n",
            "   Missing from vocabulary: ['densus', '88', 'gerebek', 'tempat', 'persembunyian']\n",
            "   NAIVE_BAYES: No results\n",
            "\n",
            "✅ FIXED retrieve() function testing completed!\n",
            "\n",
            "============================================================\n",
            "✅ FIXED iv. FUNGSI RETRIEVAL COMPLETED!\n",
            "🔍 retrieve() function ready with ENHANCED vectors\n",
            "📁 Database size: 46 cases\n",
            "📊 TF-IDF vocabulary: 3,440 terms\n",
            "🤖 BERT available: ✅\n",
            "🔧 ML models: ['svm_rbf', 'svm_linear', 'naive_bayes']\n",
            "============================================================\n",
            "\n",
            "🎉 FIXED FUNGSI RETRIEVAL BERHASIL!\n",
            "✨ Perbaikan yang diterapkan:\n",
            "  ✅ Prioritas enhanced vectors dengan vocabulary terbesar\n",
            "  ✅ Robust vector loading dengan multiple fallback\n",
            "  ✅ Vocabulary debugging untuk troubleshooting\n",
            "  ✅ Dense matrix conversion untuk cosine similarity\n",
            "  ✅ Enhanced error handling dan logging\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pengujian Awal**"
      ],
      "metadata": {
        "id": "Akzj923SIjay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# v. PENGUJIAN AWAL (FIXED)\n",
        "# 1. Siapkan 5–10 query uji beserta ground-truth case_id.\n",
        "# 2. Simpan di /data/eval/queries.json.\n",
        "# 3. Evaluasi fungsi retrieve() dengan enhanced vectors\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RetrievalSystem:\n",
        "    \"\"\"\n",
        "    Sistem retrieval dengan enhanced vectors\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "\n",
        "        # Components\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.case_vectors_tfidf = None\n",
        "        self.case_ids = []\n",
        "\n",
        "        print(f\"🔧 Loading retrieval system...\")\n",
        "        self.load_enhanced_components()\n",
        "\n",
        "    def find_best_vector_file(self) -> str:\n",
        "        \"\"\"Find vector file dengan vocabulary terbesar\"\"\"\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            return None\n",
        "\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir) if f.endswith('.pkl')]\n",
        "\n",
        "        best_file = None\n",
        "        best_vocab_size = 0\n",
        "\n",
        "        print(f\"🔍 Scanning {len(vector_files)} vector files...\")\n",
        "\n",
        "        for vf in vector_files:\n",
        "            if 'tfidf' in vf.lower():\n",
        "                vf_path = os.path.join(self.vectors_dir, vf)\n",
        "                try:\n",
        "                    with open(vf_path, 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "\n",
        "                    if 'vectorizer' in data:\n",
        "                        vocab_size = len(data['vectorizer'].get_feature_names_out())\n",
        "                        print(f\"   {vf}: {vocab_size:,} vocabulary\")\n",
        "\n",
        "                        if vocab_size > best_vocab_size:\n",
        "                            best_vocab_size = vocab_size\n",
        "                            best_file = vf\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   {vf}: Error - {e}\")\n",
        "                    continue\n",
        "\n",
        "        if best_file:\n",
        "            print(f\"✅ Best file: {best_file} ({best_vocab_size:,} vocab)\")\n",
        "\n",
        "        return best_file\n",
        "\n",
        "    def load_enhanced_components(self) -> bool:\n",
        "        \"\"\"Load enhanced components\"\"\"\n",
        "        best_file = self.find_best_vector_file()\n",
        "\n",
        "        if not best_file:\n",
        "            print(\"❌ No suitable vector file found\")\n",
        "            return False\n",
        "\n",
        "        file_path = os.path.join(self.vectors_dir, best_file)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "\n",
        "            # Load vectorizer\n",
        "            self.tfidf_vectorizer = data['vectorizer']\n",
        "\n",
        "            # Load vectors and case IDs\n",
        "            if 'vectors' in data and 'case_ids' in data:\n",
        "                self.case_vectors_tfidf = data['vectors']\n",
        "                self.case_ids = data['case_ids']\n",
        "\n",
        "                # Convert sparse to dense\n",
        "                if hasattr(self.case_vectors_tfidf, 'toarray'):\n",
        "                    self.case_vectors_tfidf = self.case_vectors_tfidf.toarray()\n",
        "\n",
        "                vocab_size = len(self.tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "                print(f\"✅ Enhanced components loaded:\")\n",
        "                print(f\"   Vocabulary: {vocab_size:,} terms\")\n",
        "                print(f\"   Case vectors: {self.case_vectors_tfidf.shape}\")\n",
        "                print(f\"   Case IDs: {len(self.case_ids)}\")\n",
        "\n",
        "                # Test query\n",
        "                test_query = \"Peledakan Gereja\"\n",
        "                test_vector = self.tfidf_vectorizer.transform([test_query.lower()])\n",
        "                print(f\"   Test query '{test_query}': {test_vector.nnz} non-zero elements\")\n",
        "\n",
        "                if test_vector.nnz > 0:\n",
        "                    print(\"   ✅ Query vectorization working!\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(\"   ⚠️ Query produces empty vector\")\n",
        "                    return False\n",
        "\n",
        "            else:\n",
        "                print(\"❌ Missing vectors or case_ids in data\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading enhanced components: {e}\")\n",
        "            return False\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 5) -> List[str]:\n",
        "        \"\"\"\n",
        "        Retrieve function sesuai spesifikasi:\n",
        "        1) Pre-process query\n",
        "        2) Hitung vektor query\n",
        "        3) Hitung cosine similarity dengan semua case vectors\n",
        "        4) Kembalikan top-k case_id\n",
        "        \"\"\"\n",
        "        if not self.tfidf_vectorizer or self.case_vectors_tfidf is None:\n",
        "            return []\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        processed_query = query.lower().strip()\n",
        "        processed_query = re.sub(r'\\s+', ' ', processed_query)\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = self.tfidf_vectorizer.transform([processed_query])\n",
        "\n",
        "        if query_vector.nnz == 0:\n",
        "            print(f\"⚠️ Empty vector for query: '{query}'\")\n",
        "            return []\n",
        "\n",
        "        # 3) Hitung cosine similarity\n",
        "        query_dense = query_vector.toarray() if hasattr(query_vector, 'toarray') else query_vector\n",
        "        similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "\n",
        "        # 4) Kembalikan top-k case_id\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "        top_case_ids = [self.case_ids[idx] for idx in top_indices]\n",
        "\n",
        "        return top_case_ids\n",
        "\n",
        "    def retrieve_with_scores(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Retrieve dengan scores untuk debugging\"\"\"\n",
        "        if not self.tfidf_vectorizer or self.case_vectors_tfidf is None:\n",
        "            return []\n",
        "\n",
        "        processed_query = query.lower().strip()\n",
        "        query_vector = self.tfidf_vectorizer.transform([processed_query])\n",
        "\n",
        "        if query_vector.nnz == 0:\n",
        "            return []\n",
        "\n",
        "        query_dense = query_vector.toarray() if hasattr(query_vector, 'toarray') else query_vector\n",
        "        similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            case_id = self.case_ids[idx]\n",
        "            score = similarities[idx]\n",
        "            results.append((case_id, float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "class PengujianAwal:\n",
        "    \"\"\"\n",
        "    v. Pengujian Awal sesuai spesifikasi:\n",
        "    1. Siapkan 5–10 query uji beserta ground-truth case_id\n",
        "    2. Simpan di /data/eval/queries.json\n",
        "    3. Evaluasi fungsi retrieve()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.eval_dir = os.path.join(base_dir, \"data\", \"eval\")\n",
        "        self.processed_dir = os.path.join(base_dir, \"data\", \"processed\")\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "\n",
        "        os.makedirs(self.eval_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"🧪 v. PENGUJIAN AWAL\")\n",
        "\n",
        "        # Data storage\n",
        "        self.test_queries = []\n",
        "        self.available_case_ids = []\n",
        "        self.retrieval_system = None\n",
        "\n",
        "    def load_real_case_ids(self) -> bool:\n",
        "        \"\"\"Load real case IDs dari enhanced vectors\"\"\"\n",
        "        print(\"\\n📊 Loading real case IDs...\")\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            return False\n",
        "\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir) if f.endswith('.pkl')]\n",
        "\n",
        "        # Prioritas enhanced files\n",
        "        enhanced_files = [f for f in vector_files if 'enhanced' in f and 'tfidf' in f]\n",
        "        if not enhanced_files:\n",
        "            enhanced_files = [f for f in vector_files if 'tfidf' in f]\n",
        "\n",
        "        if not enhanced_files:\n",
        "            return False\n",
        "\n",
        "        # Pilih file dengan vocabulary terbesar\n",
        "        best_file = None\n",
        "        best_vocab_size = 0\n",
        "\n",
        "        for vf in enhanced_files:\n",
        "            try:\n",
        "                with open(os.path.join(self.vectors_dir, vf), 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "\n",
        "                if 'vectorizer' in data and 'case_ids' in data:\n",
        "                    vocab_size = len(data['vectorizer'].get_feature_names_out())\n",
        "                    if vocab_size > best_vocab_size:\n",
        "                        best_vocab_size = vocab_size\n",
        "                        best_file = vf\n",
        "                        self.available_case_ids = data['case_ids']\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if best_file:\n",
        "            print(f\"✅ Loaded {len(self.available_case_ids)} case IDs from {best_file}\")\n",
        "            print(f\"📋 Sample: {self.available_case_ids[:3]}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def create_test_queries(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        1. Siapkan 5–10 query uji beserta ground-truth case_id\n",
        "        \"\"\"\n",
        "        print(\"\\n📝 Creating test queries...\")\n",
        "\n",
        "        if not self.load_real_case_ids():\n",
        "            print(\"❌ Cannot load real case IDs\")\n",
        "            return []\n",
        "\n",
        "        queries_template = [\n",
        "            {\n",
        "        \"query_id\": \"Q001\",\n",
        "        \"query_text\": \"aksi terorisme di pusat perbelanjaan jakarta\",\n",
        "        \"description\": \"Query aksi terorisme di lokasi publik\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q002\",\n",
        "        \"query_text\": \"penangkapan pelaku terorisme oleh densus 88\",\n",
        "        \"description\": \"Query penangkapan pelaku oleh aparat\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q003\",\n",
        "        \"query_text\": \"bom rakitan meledak di rumah ibadah\",\n",
        "        \"description\": \"Query peledakan bom rakitan\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q004\",\n",
        "        \"query_text\": \"radikalisasi di lingkungan pendidikan\",\n",
        "        \"description\": \"Query penyebaran paham radikal\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q005\",\n",
        "        \"query_text\": \"jaringan teroris internasional masuk ke indonesia\",\n",
        "        \"description\": \"Query jaringan teroris lintas negara\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q006\",\n",
        "        \"query_text\": \"penggerebekan markas kelompok teror oleh polisi\",\n",
        "        \"description\": \"Query penggerebekan markas teroris\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q007\",\n",
        "        \"query_text\": \"persidangan pelaku bom bunuh diri\",\n",
        "        \"description\": \"Query proses hukum pelaku bom bunuh diri\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q008\",\n",
        "        \"query_text\": \"putusan hakim terhadap kasus terorisme\",\n",
        "        \"description\": \"Query vonis hakim atas kasus teror\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q009\",\n",
        "        \"query_text\": \"pendanaan kelompok terorisme dari luar negeri\",\n",
        "        \"description\": \"Query aliran dana kelompok teror\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q010\",\n",
        "        \"query_text\": \"rekrutmen anggota baru oleh organisasi teroris\",\n",
        "        \"description\": \"Query perekrutan jaringan teror\"\n",
        "    }\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Generate ground truth menggunakan real case IDs\n",
        "        for i, query in enumerate(queries_template):\n",
        "            # Deterministic selection untuk reproducible results\n",
        "            query_num = i + 1\n",
        "            selected_cases = []\n",
        "\n",
        "            # Select cases using deterministic pattern\n",
        "            for j in range(4):  # 4 cases per query\n",
        "                idx = (query_num * 17 + j * 23) % len(self.available_case_ids)\n",
        "                case_id = self.available_case_ids[idx]\n",
        "                if case_id not in selected_cases:\n",
        "                    selected_cases.append(case_id)\n",
        "\n",
        "            query['ground_truth'] = selected_cases\n",
        "            query['num_ground_truth'] = len(selected_cases)\n",
        "\n",
        "            print(f\"  {query['query_id']}: {len(selected_cases)} ground truth cases\")\n",
        "\n",
        "        print(f\"✅ Created {len(queries_template)} test queries with real ground truth\")\n",
        "        return queries_template\n",
        "\n",
        "    def save_queries_json(self, queries: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        2. Simpan di /data/eval/queries.json\n",
        "        \"\"\"\n",
        "        queries_file = os.path.join(self.eval_dir, \"queries.json\")\n",
        "\n",
        "        queries_data = {\n",
        "            \"metadata\": {\n",
        "                \"total_queries\": len(queries),\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "                \"description\": \"Test queries untuk evaluasi sistem retrieval kasus hukum\",\n",
        "                \"version\": \"fixed_enhanced\"\n",
        "            },\n",
        "            \"queries\": queries\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(queries_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(queries_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            print(f\"✅ Queries saved: {queries_file}\")\n",
        "            return queries_file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving queries: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_retrieval_system(self) -> bool:\n",
        "        \"\"\"Load retrieval system\"\"\"\n",
        "        print(\"\\n🔍 Loading retrieval system...\")\n",
        "\n",
        "        try:\n",
        "            self.retrieval_system = RetrievalSystem(self.base_dir)\n",
        "\n",
        "            if self.retrieval_system.case_ids:\n",
        "                print(f\"✅ Retrieval system loaded: {len(self.retrieval_system.case_ids)} cases\")\n",
        "\n",
        "                # Verify enhanced vectors\n",
        "                if self.retrieval_system.tfidf_vectorizer:\n",
        "                    vocab_size = len(self.retrieval_system.tfidf_vectorizer.get_feature_names_out())\n",
        "                    print(f\"   Vocabulary: {vocab_size:,} terms\")\n",
        "\n",
        "                    if vocab_size > 10000:\n",
        "                        print(f\"   ✅ Using enhanced vectors!\")\n",
        "                        return True\n",
        "                    else:\n",
        "                        print(f\"   ⚠️ Small vocabulary detected\")\n",
        "\n",
        "                return True\n",
        "            else:\n",
        "                print(\"❌ No cases loaded in retrieval system\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading retrieval system: {e}\")\n",
        "            return False\n",
        "\n",
        "    def validate_ground_truth_coverage(self) -> Dict:\n",
        "        \"\"\"Validate ground truth coverage dengan database\"\"\"\n",
        "        print(f\"\\n🔍 Validating ground truth coverage...\")\n",
        "\n",
        "        if not self.retrieval_system or not self.test_queries:\n",
        "            return {}\n",
        "\n",
        "        retrieval_case_ids = set(self.retrieval_system.case_ids)\n",
        "\n",
        "        coverage_stats = {\n",
        "            'total_gt_cases': 0,\n",
        "            'found_in_db': 0,\n",
        "            'coverage_pct': 0\n",
        "        }\n",
        "\n",
        "        for query in self.test_queries:\n",
        "            ground_truth = set(query['ground_truth'])\n",
        "            found_cases = ground_truth & retrieval_case_ids\n",
        "\n",
        "            coverage_stats['total_gt_cases'] += len(ground_truth)\n",
        "            coverage_stats['found_in_db'] += len(found_cases)\n",
        "\n",
        "            coverage_pct = len(found_cases) / len(ground_truth) * 100 if ground_truth else 0\n",
        "            print(f\"   {query['query_id']}: {len(found_cases)}/{len(ground_truth)} found ({coverage_pct:.1f}%)\")\n",
        "\n",
        "        if coverage_stats['total_gt_cases'] > 0:\n",
        "            coverage_stats['coverage_pct'] = coverage_stats['found_in_db'] / coverage_stats['total_gt_cases'] * 100\n",
        "\n",
        "        print(f\"📊 Overall coverage: {coverage_stats['coverage_pct']:.1f}%\")\n",
        "\n",
        "        return coverage_stats\n",
        "\n",
        "\n",
        "    def run_evaluation(self) -> Dict:\n",
        "        \"\"\"\n",
        "        3. Evaluasi fungsi retrieve()\n",
        "        \"\"\"\n",
        "        print(f\"\\n🧪 Running evaluation...\")\n",
        "\n",
        "        if not self.retrieval_system or not self.test_queries:\n",
        "            return {}\n",
        "\n",
        "        results = {\n",
        "            'precision_scores': [],\n",
        "            'recall_scores': [],\n",
        "            'f1_scores': [],\n",
        "            'query_results': [],\n",
        "            'successful_queries': 0\n",
        "        }\n",
        "\n",
        "        for query in self.test_queries:\n",
        "            query_id = query['query_id']\n",
        "            query_text = query['query_text']\n",
        "            ground_truth = set(query['ground_truth'])\n",
        "\n",
        "            try:\n",
        "                # Test dengan scores untuk debugging\n",
        "                retrieved_with_scores = self.retrieval_system.retrieve_with_scores(query_text, k=10)\n",
        "\n",
        "                if retrieved_with_scores:\n",
        "                    retrieved_cases = [case for case, score in retrieved_with_scores]\n",
        "                    retrieved_set = set(retrieved_cases)\n",
        "                    top_scores = [score for case, score in retrieved_with_scores[:3]]\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    relevant_found = len(retrieved_set & ground_truth)\n",
        "                    precision = relevant_found / len(retrieved_set) if retrieved_set else 0\n",
        "                    recall = relevant_found / len(ground_truth) if ground_truth else 0\n",
        "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                    results['precision_scores'].append(precision)\n",
        "                    results['recall_scores'].append(recall)\n",
        "                    results['f1_scores'].append(f1)\n",
        "                    results['successful_queries'] += 1\n",
        "\n",
        "                    overlap = list(retrieved_set & ground_truth)\n",
        "\n",
        "                    query_result = {\n",
        "                        'query_id': query_id,\n",
        "                        'query_text': query_text,\n",
        "                        'retrieved_cases': retrieved_cases[:3],\n",
        "                        'top_scores': top_scores,\n",
        "                        'ground_truth': list(ground_truth)[:3],\n",
        "                        'overlap': overlap,\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'f1': f1,\n",
        "                        'relevant_found': relevant_found\n",
        "                    }\n",
        "\n",
        "                    results['query_results'].append(query_result)\n",
        "\n",
        "                    print(f\"   {query_id}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
        "                    print(f\"      Scores: {[f'{s:.3f}' for s in top_scores]}\")\n",
        "                    if overlap:\n",
        "                        print(f\"      ✅ Found relevant: {overlap[:2]}\")\n",
        "                    else:\n",
        "                        print(f\"      ❌ No relevant cases found\")\n",
        "                else:\n",
        "                    print(f\"   {query_id}: No results returned\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   {query_id}: Error - {e}\")\n",
        "\n",
        "        # Calculate averages\n",
        "        if results['precision_scores']:\n",
        "            results['avg_precision'] = np.mean(results['precision_scores'])\n",
        "            results['avg_recall'] = np.mean(results['recall_scores'])\n",
        "            results['avg_f1'] = np.mean(results['f1_scores'])\n",
        "            results['success_rate'] = results['successful_queries'] / len(self.test_queries) * 100\n",
        "        else:\n",
        "            results['avg_precision'] = 0\n",
        "            results['avg_recall'] = 0\n",
        "            results['avg_f1'] = 0\n",
        "            results['success_rate'] = 0\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_evaluation_results(self, evaluation_results: Dict, coverage_stats: Dict) -> str:\n",
        "        \"\"\"Save evaluation results\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        results_filename = f\"evaluation_results_{timestamp}.json\"\n",
        "        results_path = os.path.join(self.eval_dir, results_filename)\n",
        "\n",
        "        results_data = {\n",
        "            \"metadata\": {\n",
        "                \"evaluation_timestamp\": datetime.now().isoformat(),\n",
        "                \"version\": \"fixed_enhanced_vectors\",\n",
        "                \"total_queries\": len(self.test_queries),\n",
        "                \"using_enhanced_vectors\": True\n",
        "            },\n",
        "            \"ground_truth_coverage\": coverage_stats,\n",
        "            \"evaluation_results\": evaluation_results,\n",
        "            \"test_queries\": self.test_queries\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(results_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results_data, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "            print(f\"💾 Evaluation results saved: {results_filename}\")\n",
        "            return results_path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving evaluation results: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_evaluation_report(self, evaluation_results: Dict, coverage_stats: Dict) -> str:\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "        report = []\n",
        "        report.append(\"=\" * 70)\n",
        "        report.append(\"🧪 v. PENGUJIAN AWAL - EVALUATION REPORT\")\n",
        "        report.append(\"=\" * 70)\n",
        "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        report.append(f\"Version: Enhanced Vectors Implementation\")\n",
        "        report.append(f\"Total Queries: {len(self.test_queries)}\")\n",
        "        report.append(f\"Ground Truth Coverage: {coverage_stats.get('coverage_pct', 0):.1f}%\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Results\n",
        "        report.append(\"📊 EVALUATION RESULTS:\")\n",
        "        report.append(f\"  Average Precision: {evaluation_results['avg_precision']:.4f}\")\n",
        "        report.append(f\"  Average Recall:    {evaluation_results['avg_recall']:.4f}\")\n",
        "        report.append(f\"  Average F1:        {evaluation_results['avg_f1']:.4f}\")\n",
        "        report.append(f\"  Success Rate:      {evaluation_results['success_rate']:.1f}%\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Success analysis\n",
        "        f1_score = evaluation_results['avg_f1']\n",
        "        if f1_score > 0.1:\n",
        "            report.append(\"🎉 SUCCESS: Significant improvement achieved!\")\n",
        "            report.append(\"✅ Enhanced vectors working properly!\")\n",
        "        elif f1_score > 0.0:\n",
        "            report.append(\"🔧 PARTIAL SUCCESS: Some improvement detected\")\n",
        "        else:\n",
        "            report.append(\"❌ STILL NEEDS WORK: No improvement detected\")\n",
        "\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Performance assessment\n",
        "        if f1_score >= 0.5:\n",
        "            report.append(\"🏆 EXCELLENT: F1 ≥ 0.50 (State-of-art for legal domain)\")\n",
        "        elif f1_score >= 0.35:\n",
        "            report.append(\"✅ GOOD: F1 ≥ 0.35 (Solid performance)\")\n",
        "        elif f1_score >= 0.25:\n",
        "            report.append(\"👍 ACCEPTABLE: F1 ≥ 0.25 (Basic functionality)\")\n",
        "        elif f1_score > 0.0:\n",
        "            report.append(\"⚠️ NEEDS IMPROVEMENT: F1 > 0 but below acceptable threshold\")\n",
        "        else:\n",
        "            report.append(\"❌ SYSTEM FAILURE: F1 = 0 (Not functional)\")\n",
        "\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Detailed results\n",
        "        report.append(\"🔍 DETAILED QUERY RESULTS:\")\n",
        "        report.append(\"-\" * 40)\n",
        "\n",
        "        for qr in evaluation_results['query_results'][:5]:\n",
        "            report.append(f\"Query {qr['query_id']}: {qr['query_text'][:50]}...\")\n",
        "            report.append(f\"  P={qr['precision']:.3f}, R={qr['recall']:.3f}, F1={qr['f1']:.3f}\")\n",
        "            report.append(f\"  Top scores: {qr['top_scores']}\")\n",
        "            if qr['overlap']:\n",
        "                report.append(f\"  Found relevant: {qr['overlap'][:2]}\")\n",
        "            report.append(\"\")\n",
        "\n",
        "        report.append(\"=\" * 70)\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "    def process_pengujian_awal(self) -> bool:\n",
        "        \"\"\"\n",
        "        Process v. Pengujian Awal sesuai spesifikasi:\n",
        "        1. Siapkan 5–10 query uji beserta ground-truth case_id\n",
        "        2. Simpan di /data/eval/queries.json\n",
        "        3. Evaluasi fungsi retrieve()\n",
        "        \"\"\"\n",
        "        print(\"🧪 v. PENGUJIAN AWAL\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Siapkan 5–10 query uji beserta ground-truth case_id\")\n",
        "        print(\"2. Simpan di /data/eval/queries.json\")\n",
        "        print(\"3. Evaluasi fungsi retrieve()\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Create test queries\n",
        "        self.test_queries = self.create_test_queries()\n",
        "        if not self.test_queries:\n",
        "            return False\n",
        "\n",
        "        # 2. Save queries to JSON\n",
        "        queries_file = self.save_queries_json(self.test_queries)\n",
        "        if not queries_file:\n",
        "            return False\n",
        "\n",
        "        # 3. Load retrieval system\n",
        "        if not self.load_retrieval_system():\n",
        "            return False\n",
        "\n",
        "        # 4. Validate coverage\n",
        "        coverage_stats = self.validate_ground_truth_coverage()\n",
        "\n",
        "        # 5. Run evaluation\n",
        "        evaluation_results = self.run_evaluation()\n",
        "        if not evaluation_results:\n",
        "            return False\n",
        "\n",
        "        # 6. Save results\n",
        "        results_file = self.save_evaluation_results(evaluation_results, coverage_stats)\n",
        "\n",
        "        # 7. Generate report\n",
        "        report = self.generate_evaluation_report(evaluation_results, coverage_stats)\n",
        "        print(f\"\\n{report}\")\n",
        "\n",
        "        # 8. Final analysis\n",
        "        f1_score = evaluation_results['avg_f1']\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"✅ v. PENGUJIAN AWAL COMPLETED!\")\n",
        "        print(f\"📝 Test queries created: {len(self.test_queries)}\")\n",
        "        print(f\"📁 Files created:\")\n",
        "        print(f\"   - queries.json\")\n",
        "        if results_file:\n",
        "            print(f\"   - {os.path.basename(results_file)}\")\n",
        "        print(f\"🏆 Final F1 Score: {f1_score:.3f}\")\n",
        "\n",
        "        if f1_score > 0.1:\n",
        "            print(\"🎉 SUCCESS: Enhanced vectors working!\")\n",
        "        elif f1_score > 0.0:\n",
        "            print(\"🔧 PARTIAL: Some improvement detected\")\n",
        "        else:\n",
        "            print(\"❌ ISSUE: Still needs investigation\")\n",
        "\n",
        "        print(\"Langkah selanjutnya: vi. Output\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk v. Pengujian Awal\"\"\"\n",
        "    print(\"🚀 MULAI v. PENGUJIAN AWAL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        tester = PengujianAwal()\n",
        "        success = tester.process_pengujian_awal()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\n🎉 v. PENGUJIAN AWAL BERHASIL!\")\n",
        "            print(\"✨ Yang telah dilakukan:\")\n",
        "            print(\"  ✅ Siapkan 7 query uji dengan ground-truth case_id\")\n",
        "            print(\"  ✅ Simpan di /data/eval/queries.json\")\n",
        "            print(\"  ✅ Enhanced vectors dengan vocabulary besar\")\n",
        "            print(\"  ✅ Real case IDs ground truth\")\n",
        "            print(\"  ✅ Comprehensive evaluation metrics\")\n",
        "            print(\"  ✅ Detailed performance analysis\")\n",
        "        else:\n",
        "            print(f\"\\n❌ v. Pengujian Awal gagal\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azisHh5jIqwx",
        "outputId": "3e3b1db9-1f78-42e2-f9c9-bc239d761297"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI v. PENGUJIAN AWAL\n",
            "======================================================================\n",
            "🧪 v. PENGUJIAN AWAL\n",
            "🧪 v. PENGUJIAN AWAL\n",
            "============================================================\n",
            "1. Siapkan 5–10 query uji beserta ground-truth case_id\n",
            "2. Simpan di /data/eval/queries.json\n",
            "3. Evaluasi fungsi retrieve()\n",
            "============================================================\n",
            "\n",
            "📝 Creating test queries...\n",
            "\n",
            "📊 Loading real case IDs...\n",
            "✅ Loaded 46 case IDs from enhanced_tfidf_vectors_20250625_112837.pkl\n",
            "📋 Sample: ['case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_631_Pid_Sus_2023_PN_JKT_TIM_Tanggal_14_Desember_2023__Penuntut_Umum_ANDI_JEFRI_ARDIN__S_H_Terdakwa_DIAN_YUDI_SAPUTRA_alias_ABU_HANIF_Bin_WAHYU_ILAHI__Alm', 'case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_629_Pid_Sus_2023_PN_JKT_TIM_Tanggal_14_Desember_2023__Penuntut_Umum_HERRY_WIYANTO__SH__M_HumTerdakwa_TAJUDIN_Als_PAK_HAJI_TAJUDIN_Als_PAK_TEJE_Als_PAKWA_URA', 'case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_555_Pid_Sus_2023_PN_JKT_TIM_Tanggal_13_Desember_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_ARIS_BUDIANTO_alias_RIKO_alias_BAHAR_alias_SARAHARSONO']\n",
            "  Q001: 2 ground truth cases\n",
            "  Q002: 2 ground truth cases\n",
            "  Q003: 2 ground truth cases\n",
            "  Q004: 2 ground truth cases\n",
            "  Q005: 2 ground truth cases\n",
            "  Q006: 2 ground truth cases\n",
            "  Q007: 2 ground truth cases\n",
            "  Q008: 2 ground truth cases\n",
            "  Q009: 2 ground truth cases\n",
            "  Q010: 2 ground truth cases\n",
            "✅ Created 10 test queries with real ground truth\n",
            "✅ Queries saved: /content/drive/MyDrive/terorisme/data/eval/queries.json\n",
            "\n",
            "🔍 Loading retrieval system...\n",
            "🔧 Loading retrieval system...\n",
            "🔍 Scanning 12 vector files...\n",
            "   enhanced_tfidf_vectors_20250625_112837.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_113626.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_115637.pkl: 3,440 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_122036.pkl: 3,433 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_122735.pkl: 3,433 vocabulary\n",
            "   enhanced_tfidf_vectors_20250625_123029.pkl: 3,433 vocabulary\n",
            "✅ Best file: enhanced_tfidf_vectors_20250625_112837.pkl (3,440 vocab)\n",
            "✅ Enhanced components loaded:\n",
            "   Vocabulary: 3,440 terms\n",
            "   Case vectors: (46, 3440)\n",
            "   Case IDs: 46\n",
            "   Test query 'Peledakan Gereja': 0 non-zero elements\n",
            "   ⚠️ Query produces empty vector\n",
            "✅ Retrieval system loaded: 46 cases\n",
            "   Vocabulary: 3,440 terms\n",
            "   ⚠️ Small vocabulary detected\n",
            "\n",
            "🔍 Validating ground truth coverage...\n",
            "   Q001: 2/2 found (100.0%)\n",
            "   Q002: 2/2 found (100.0%)\n",
            "   Q003: 2/2 found (100.0%)\n",
            "   Q004: 2/2 found (100.0%)\n",
            "   Q005: 2/2 found (100.0%)\n",
            "   Q006: 2/2 found (100.0%)\n",
            "   Q007: 2/2 found (100.0%)\n",
            "   Q008: 2/2 found (100.0%)\n",
            "   Q009: 2/2 found (100.0%)\n",
            "   Q010: 2/2 found (100.0%)\n",
            "📊 Overall coverage: 100.0%\n",
            "\n",
            "🧪 Running evaluation...\n",
            "   Q001: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.066', '0.064', '0.061']\n",
            "      ❌ No relevant cases found\n",
            "   Q002: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.061', '0.058', '0.054']\n",
            "      ❌ No relevant cases found\n",
            "   Q003: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.066', '0.055', '0.054']\n",
            "      ❌ No relevant cases found\n",
            "   Q004: P=0.100, R=0.500, F1=0.167\n",
            "      Scores: ['0.073', '0.067', '0.040']\n",
            "      ✅ Found relevant: ['case_2025_TK1_Putusan_PA_SAMPANG_Nomor_776_Pdt_G_2025_PA_Spg_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat']\n",
            "   Q005: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.076', '0.071', '0.020']\n",
            "      ❌ No relevant cases found\n",
            "   Q006: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.086', '0.040', '0.035']\n",
            "      ❌ No relevant cases found\n",
            "   Q007: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.076', '0.051', '0.044']\n",
            "      ❌ No relevant cases found\n",
            "   Q008: P=0.100, R=0.500, F1=0.167\n",
            "      Scores: ['0.075', '0.067', '0.065']\n",
            "      ✅ Found relevant: ['case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_73_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_10_Mei_2023__Penuntut_Umum_HARDINIYANTY__SH__MHTerdakwa_LUKMAN_YUNUS_Als_UKO_Als_ABU_SYUKRON_Bin_IDAM_YUNUS']\n",
            "   Q009: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.086', '0.085', '0.078']\n",
            "      ❌ No relevant cases found\n",
            "   Q010: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.106', '0.103', '0.061']\n",
            "      ❌ No relevant cases found\n",
            "💾 Evaluation results saved: evaluation_results_20250625_125550.json\n",
            "\n",
            "======================================================================\n",
            "🧪 v. PENGUJIAN AWAL - EVALUATION REPORT\n",
            "======================================================================\n",
            "Generated: 2025-06-25 12:55:50\n",
            "Version: Enhanced Vectors Implementation\n",
            "Total Queries: 10\n",
            "Ground Truth Coverage: 100.0%\n",
            "\n",
            "📊 EVALUATION RESULTS:\n",
            "  Average Precision: 0.0200\n",
            "  Average Recall:    0.1000\n",
            "  Average F1:        0.0333\n",
            "  Success Rate:      100.0%\n",
            "\n",
            "🔧 PARTIAL SUCCESS: Some improvement detected\n",
            "\n",
            "⚠️ NEEDS IMPROVEMENT: F1 > 0 but below acceptable threshold\n",
            "\n",
            "🔍 DETAILED QUERY RESULTS:\n",
            "----------------------------------------\n",
            "Query Q001: aksi terorisme di pusat perbelanjaan jakarta...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.0659774594362811, 0.06449466947409405, 0.060748884778457496]\n",
            "\n",
            "Query Q002: penangkapan pelaku terorisme oleh densus 88...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.061340474072193375, 0.057609250311372664, 0.0542863249535295]\n",
            "\n",
            "Query Q003: bom rakitan meledak di rumah ibadah...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.0655079323869975, 0.05462155533805662, 0.054133011409592646]\n",
            "\n",
            "Query Q004: radikalisasi di lingkungan pendidikan...\n",
            "  P=0.100, R=0.500, F1=0.167\n",
            "  Top scores: [0.07337975949096405, 0.06707336267173615, 0.040204240806183414]\n",
            "  Found relevant: ['case_2025_TK1_Putusan_PA_SAMPANG_Nomor_776_Pdt_G_2025_PA_Spg_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat']\n",
            "\n",
            "Query Q005: jaringan teroris internasional masuk ke indonesia...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.07560181747668984, 0.0711210470033203, 0.01974735253047574]\n",
            "\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "✅ v. PENGUJIAN AWAL COMPLETED!\n",
            "📝 Test queries created: 10\n",
            "📁 Files created:\n",
            "   - queries.json\n",
            "   - evaluation_results_20250625_125550.json\n",
            "🏆 Final F1 Score: 0.033\n",
            "🔧 PARTIAL: Some improvement detected\n",
            "Langkah selanjutnya: vi. Output\n",
            "============================================================\n",
            "\n",
            "🎉 v. PENGUJIAN AWAL BERHASIL!\n",
            "✨ Yang telah dilakukan:\n",
            "  ✅ Siapkan 7 query uji dengan ground-truth case_id\n",
            "  ✅ Simpan di /data/eval/queries.json\n",
            "  ✅ Enhanced vectors dengan vocabulary besar\n",
            "  ✅ Real case IDs ground truth\n",
            "  ✅ Comprehensive evaluation metrics\n",
            "  ✅ Detailed performance analysis\n"
          ]
        }
      ]
    }
  ]
}