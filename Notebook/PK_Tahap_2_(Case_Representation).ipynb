{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/drive', ignore_errors=True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zqQSZ6FIzj9E",
        "outputId": "e6b453df-09b1-4227-e15f-720df041ac3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ekstraksi Metadata**"
      ],
      "metadata": {
        "id": "AaUTf_n-yHJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional"
      ],
      "metadata": {
        "id": "bz7k2pHAxrf6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengaturan logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MetadataExtractor:\n",
        "    \"\"\"Ekstrak metadata terstruktur dari dokumen putusan pengadilan\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.cleaned_dir = \"/data/raw\"  # Input: file teks yang sudah dibersihkan\n",
        "        self.gdrive_cleaned_dir = os.path.join(base_dir, \"CLEANED\")  # Input alternatif\n",
        "        self.output_dir = \"/data/processed\"  # Output directory lokal\n",
        "        self.gdrive_output_dir = os.path.join(base_dir, \"data\", \"processed\")  # Output Google Drive\n",
        "        self.metadata_dir = os.path.join(base_dir, \"METADATA\")  # Backup output\n",
        "        self.logs_dir = \"/logs\"\n",
        "\n",
        "        # Buat direktori\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.gdrive_output_dir, exist_ok=True)  # Buat di Google Drive\n",
        "        os.makedirs(self.metadata_dir, exist_ok=True)\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"🔍 EKSTRAKSI METADATA\")\n",
        "        print(f\"Input: {self.cleaned_dir} atau {self.gdrive_cleaned_dir}\")\n",
        "        print(f\"Output Lokal: {self.output_dir}\")\n",
        "        print(f\"Output GDrive: {self.gdrive_output_dir}\")\n",
        "\n",
        "        # Setup ekstraksi patterns\n",
        "        self.setup_extraction_patterns()\n",
        "\n",
        "        # Mapping bulan Indonesia\n",
        "        self.month_mapping = {\n",
        "            'januari': '01', 'februari': '02', 'maret': '03', 'april': '04',\n",
        "            'mei': '05', 'juni': '06', 'juli': '07', 'agustus': '08',\n",
        "            'september': '09', 'oktober': '10', 'november': '11', 'desember': '12'\n",
        "        }\n",
        "\n",
        "    def setup_extraction_patterns(self):\n",
        "        \"\"\"Definisikan pola regex yang lebih spesifik untuk ekstraksi metadata\"\"\"\n",
        "\n",
        "        # 1. POLA NOMOR PERKARA (lebih spesifik)\n",
        "        self.case_number_patterns = [\n",
        "            r'(?:nomor|register)\\s*(?:perkara)?\\s*:\\s*(\\d+/pid\\.?(?:sus|b)?/?(?:\\d+)?/?pn\\.?\\w+)',  # Pidana\n",
        "            r'(?:nomor|register)\\s*(?:perkara)?\\s*:\\s*(\\d+/pdt\\.?g?/?(?:\\d+)?/?pn\\.?\\w+)',  # Perdata\n",
        "            r'(?:nomor|register)\\s*(?:perkara)?\\s*:\\s*(\\d+/\\w+/\\d{4}/pn\\.?\\w+)',  # Format lengkap\n",
        "            r'perkara\\s+nomor\\s*:\\s*(\\d+/[\\w\\./]+)',\n",
        "            r'dalam\\s+perkara\\s+nomor\\s*:\\s*([^\\n\\r]+?)(?:\\s|$)',\n",
        "        ]\n",
        "\n",
        "        # 2. POLA TANGGAL (lebih kontekstual)\n",
        "        self.date_patterns = [\n",
        "            r'diputuskan?\\s+(?:pada\\s+(?:hari\\s+\\w+\\s+)?)?tanggal\\s*:?\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "            r'dibacakan\\s+(?:pada\\s+(?:hari\\s+\\w+\\s+)?)?tanggal\\s*:?\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "            r'pada\\s+hari\\s+\\w+\\s+tanggal\\s+(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "            r'(?:jakarta|surabaya|bandung|medan|semarang|yogyakarta|makassar|palembang|denpasar|malang),?\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "        ]\n",
        "\n",
        "        # 3. POLA JENIS PERKARA (lebih spesifik dengan konteks)\n",
        "        self.case_type_patterns = [\n",
        "            r'(?:dalam\\s+)?perkara\\s+tindak\\s+pidana\\s+terorisme',\n",
        "            r'tindak\\s+pidana\\s+terorisme',\n",
        "            r'perkara\\s+pidana\\s+(terorisme|khusus|umum)',\n",
        "            r'jenis\\s+perkara\\s*:\\s*terorisme',\n",
        "            r'\\bpidana\\s+terorisme\\b'\n",
        "        ]\n",
        "        # 4. POLA PASAL HUKUM (dengan konteks UU)\n",
        "        self.legal_article_patterns = [\n",
        "            r'pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?(?:\\s+(?:jo\\.?|juncto|dan)\\s+pasal\\s+\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)*)\\s+(?:undang[- ]undang|uu)',\n",
        "            r'melanggar\\s+pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)',\n",
        "            r'berdasarkan\\s+pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)',\n",
        "            r'undang[- ]undang\\s+(?:republik\\s+indonesia\\s+)?nomor\\s+(\\d+)\\s+tahun\\s+(\\d{4})',\n",
        "            r'uu\\s+(?:no\\.?\\s*|nomor\\s+)?(\\d+)/?(\\d{4})',\n",
        "        ]\n",
        "\n",
        "        # 5. POLA PIHAK-PIHAK (dengan delimiter yang jelas)\n",
        "        self.parties_patterns = [\n",
        "            # Terdakwa (format yang umum)\n",
        "            r'terdakwa\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*alias|,\\s*yang\\s+selanjutnya|;|\\n))',\n",
        "            r'nama\\s+lengkap\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "\n",
        "            # Penuntut Umum\n",
        "            r'(?:jaksa\\s+)?penuntut\\s+umum\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*S\\.H|,|\\n))',\n",
        "\n",
        "            # Perkara Perdata\n",
        "            r'penggugat\\s*(?:I|1)?\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "            r'tergugat\\s*(?:I|1)?\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "            r'pemohon\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "            r'termohon\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "\n",
        "            # Hakim\n",
        "            r'hakim\\s+ketua\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*S\\.H|,|\\n))',\n",
        "            r'hakim\\s+anggota\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*S\\.H|,|\\n))',\n",
        "        ]\n",
        "\n",
        "        # 6. POLA PENGADILAN (dengan nama lokasi yang jelas)\n",
        "        self.court_patterns = [\n",
        "            r'pengadilan\\s+negeri\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
        "            r'pengadilan\\s+tinggi\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
        "            r'mahkamah\\s+agung\\s+republik\\s+indonesia',\n",
        "            r'pengadilan\\s+(?:tata\\s+usaha\\s+negara|agama|militer)\\s+([A-Z][a-z]+)',\n",
        "        ]\n",
        "\n",
        "    def extract_case_number(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Ekstrak nomor perkara dengan validasi format\"\"\"\n",
        "        for pattern in self.case_number_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                case_number = match.group(1).strip()\n",
        "                # Validasi format nomor perkara\n",
        "                if re.match(r'\\d+/', case_number) and len(case_number) > 5:\n",
        "                    return case_number\n",
        "        return None\n",
        "\n",
        "    def extract_dates(self, text: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"Ekstrak tanggal-tanggal penting\"\"\"\n",
        "        dates = {'decision_date': None, 'hearing_date': None}\n",
        "\n",
        "        for pattern in self.date_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    day = match.group(1).zfill(2)\n",
        "                    month_name = match.group(2).lower()\n",
        "                    year = match.group(3)\n",
        "\n",
        "                    if month_name in self.month_mapping:\n",
        "                        month = self.month_mapping[month_name]\n",
        "                        formatted_date = f\"{year}-{month}-{day}\"\n",
        "\n",
        "                        context = text[max(0, match.start()-50):match.end()+50].lower()\n",
        "                        if 'diputuskan' in context or 'dibacakan' in context:\n",
        "                            dates['decision_date'] = formatted_date\n",
        "                        else:\n",
        "                            dates['hearing_date'] = formatted_date\n",
        "                except (IndexError, KeyError):\n",
        "                    continue\n",
        "\n",
        "        return dates\n",
        "\n",
        "    def extract_case_type(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Ekstrak jenis perkara\"\"\"\n",
        "        for pattern in self.case_type_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(0).strip()\n",
        "        return None\n",
        "\n",
        "    def extract_legal_articles(self, text: str) -> List[str]:\n",
        "        \"\"\"Ekstrak pasal dan undang-undang dengan validasi\"\"\"\n",
        "        articles = []\n",
        "        for pattern in self.legal_article_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                article = match.group(0).strip()\n",
        "\n",
        "                # Validasi artikel hukum (harus mengandung angka)\n",
        "                if re.search(r'\\d+', article) and len(article) > 5:\n",
        "                    # Bersihkan artikel dari kata yang tidak perlu\n",
        "                    article = re.sub(r'\\s+', ' ', article)\n",
        "                    if article not in articles:\n",
        "                        articles.append(article)\n",
        "\n",
        "        return articles[:10]  # Batasi maksimal 10 pasal untuk menghindari noise\n",
        "\n",
        "    def clean_party_name(self, name: str) -> str:\n",
        "        \"\"\"Membersihkan nama dari gelar dan informasi tambahan\"\"\"\n",
        "        if not name:\n",
        "            return \"\"\n",
        "\n",
        "        # Daftar pattern untuk dibersihkan\n",
        "        clean_patterns = [\n",
        "            r',\\s*S\\.H\\.?.*$',\n",
        "            r',\\s*S\\.E\\.?.*$',\n",
        "            r',\\s*M\\.H\\.?.*$',\n",
        "            r',\\s*alias.*$',\n",
        "            r',\\s*bin\\s+.*$',\n",
        "            r',\\s*binti\\s+.*$',\n",
        "            r'\\s+yang\\s+selanjutnya.*$'\n",
        "        ]\n",
        "\n",
        "        cleaned_name = name.strip()\n",
        "        for pattern in clean_patterns:\n",
        "            cleaned_name = re.sub(pattern, '', cleaned_name, flags=re.IGNORECASE)\n",
        "\n",
        "        return cleaned_name.strip()\n",
        "\n",
        "    def extract_parties(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Ekstrak pihak-pihak yang terlibat dengan validasi nama\"\"\"\n",
        "        parties = {\n",
        "            'defendants': [],      # Terdakwa\n",
        "            'prosecutors': [],     # JPU\n",
        "            'plaintiffs': [],      # Penggugat/Pemohon\n",
        "            'respondents': [],     # Tergugat/Termohon\n",
        "            'judges': []           # Hakim\n",
        "        }\n",
        "\n",
        "        for pattern in self.parties_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                party_name = match.group(1).strip()\n",
        "                party_context = match.group(0).lower()\n",
        "\n",
        "                # Validasi nama (harus dimulai dengan huruf kapital dan minimal 3 karakter)\n",
        "                if not party_name or len(party_name) < 3 or not party_name[0].isupper():\n",
        "                    continue\n",
        "\n",
        "                # Bersihkan nama dari gelar dan informasi tambahan\n",
        "                party_name = self.clean_party_name(party_name)\n",
        "\n",
        "                if len(party_name) < 3:\n",
        "                    continue\n",
        "\n",
        "                # Kategorisasi berdasarkan konteks\n",
        "                if 'terdakwa' in party_context or 'nama lengkap' in party_context:\n",
        "                    parties['defendants'].append(party_name)\n",
        "                elif any(word in party_context for word in ['jaksa', 'penuntut']):\n",
        "                    parties['prosecutors'].append(party_name)\n",
        "                elif 'penggugat' in party_context or 'pemohon' in party_context:\n",
        "                    parties['plaintiffs'].append(party_name)\n",
        "                elif 'tergugat' in party_context or 'termohon' in party_context:\n",
        "                    parties['respondents'].append(party_name)\n",
        "                elif 'hakim' in party_context:\n",
        "                    parties['judges'].append(party_name)\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in parties:\n",
        "            parties[key] = list(dict.fromkeys(parties[key]))\n",
        "\n",
        "        return parties\n",
        "\n",
        "    def extract_court_info(self, text: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"Ekstrak informasi pengadilan\"\"\"\n",
        "        court_info = {'court_name': None, 'court_type': None, 'court_location': None}\n",
        "\n",
        "        for pattern in self.court_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                court_text = match.group(0).strip()\n",
        "                court_info['court_name'] = court_text\n",
        "\n",
        "                if 'negeri' in court_text.lower():\n",
        "                    court_info['court_type'] = 'Pengadilan Negeri'\n",
        "                    if match.groups():\n",
        "                        court_info['court_location'] = match.group(1).strip()\n",
        "                elif 'tinggi' in court_text.lower():\n",
        "                    court_info['court_type'] = 'Pengadilan Tinggi'\n",
        "                    if match.groups():\n",
        "                        court_info['court_location'] = match.group(1).strip()\n",
        "                elif 'mahkamah agung' in court_text.lower():\n",
        "                    court_info['court_type'] = 'Mahkamah Agung'\n",
        "\n",
        "                break\n",
        "\n",
        "        return court_info\n",
        "\n",
        "    def extract_metadata_from_text(self, text: str, filename: str) -> Dict:\n",
        "        \"\"\"Ekstrak semua metadata dari teks\"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return {}\n",
        "\n",
        "        metadata = {\n",
        "            'filename': filename,\n",
        "            'extraction_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        }\n",
        "\n",
        "        # Ekstrak komponen metadata\n",
        "        metadata['case_number'] = self.extract_case_number(text)\n",
        "        dates = self.extract_dates(text)\n",
        "        metadata.update(dates)\n",
        "        metadata['case_type'] = self.extract_case_type(text)\n",
        "        metadata['legal_articles'] = self.extract_legal_articles(text)\n",
        "        metadata['parties'] = self.extract_parties(text)\n",
        "        metadata['court_info'] = self.extract_court_info(text)\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def process_single_file(self, filename: str, source_dir: str) -> Optional[Dict]:\n",
        "        \"\"\"Proses file tunggal dengan validasi hasil\"\"\"\n",
        "        file_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.error(f\"File tidak ditemukan: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            if not text.strip():\n",
        "                logger.warning(f\"File kosong: {filename}\")\n",
        "                return None\n",
        "\n",
        "            metadata = self.extract_metadata_from_text(text, filename)\n",
        "\n",
        "            if metadata:\n",
        "                # Validasi minimal data yang diperlukan\n",
        "                has_case_number = bool(metadata.get('case_number'))\n",
        "                has_parties = any(metadata.get('parties', {}).values())\n",
        "                has_dates = bool(metadata.get('decision_date') or metadata.get('hearing_date'))\n",
        "\n",
        "                status = \"✅\" if (has_case_number or has_parties or has_dates) else \"⚠️\"\n",
        "                print(f\"{status} {filename}\")\n",
        "                return metadata\n",
        "            else:\n",
        "                print(f\"❌ {filename}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error memproses {filename}: {str(e)}\")\n",
        "            print(f\"❌ {filename}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_text_files(self, directory: str) -> List[str]:\n",
        "        \"\"\"Dapatkan daftar file teks\"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            return []\n",
        "        return [f for f in os.listdir(directory)\n",
        "                if f.endswith('.txt') and os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    def save_metadata_to_csv(self, metadata_list: List[Dict]):\n",
        "        \"\"\"Simpan metadata ke CSV terstruktur di kedua lokasi\"\"\"\n",
        "        flattened_data = []\n",
        "\n",
        "        for metadata in metadata_list:\n",
        "            parties = metadata.get('parties', {})\n",
        "            court_info = metadata.get('court_info', {})\n",
        "\n",
        "            flat_record = {\n",
        "                # IDENTITAS DOKUMEN\n",
        "                'nama_file': metadata.get('filename'),\n",
        "                'tanggal_ekstraksi': metadata.get('extraction_timestamp'),\n",
        "\n",
        "                # IDENTITAS PERKARA\n",
        "                'nomor_perkara': metadata.get('case_number'),\n",
        "                'tanggal_putusan': metadata.get('decision_date'),\n",
        "                'tanggal_sidang': metadata.get('hearing_date'),\n",
        "                'jenis_perkara': metadata.get('case_type'),\n",
        "\n",
        "                # INFORMASI PENGADILAN\n",
        "                'nama_pengadilan': court_info.get('court_name'),\n",
        "                'jenis_pengadilan': court_info.get('court_type'),\n",
        "                'lokasi_pengadilan': court_info.get('court_location'),\n",
        "\n",
        "                # PIHAK-PIHAK TERKAIT\n",
        "                'pihak_penggugat': '; '.join(parties.get('plaintiffs', [])),\n",
        "                'pihak_tergugat': '; '.join(parties.get('respondents', [])),\n",
        "                'terdakwa': '; '.join(parties.get('defendants', [])),\n",
        "                'jaksa_penuntut_umum': '; '.join(parties.get('prosecutors', [])),\n",
        "                'hakim': '; '.join(parties.get('judges', [])),\n",
        "\n",
        "                # ASPEK HUKUM\n",
        "                'pasal_yang_dilanggar': '; '.join(metadata.get('legal_articles', [])),\n",
        "                'jumlah_pasal': len(metadata.get('legal_articles', [])),\n",
        "\n",
        "                # HITUNGAN PIHAK\n",
        "                'jumlah_penggugat': len(parties.get('plaintiffs', [])),\n",
        "                'jumlah_tergugat': len(parties.get('respondents', [])),\n",
        "                'jumlah_terdakwa': len(parties.get('defendants', [])),\n",
        "                'jumlah_hakim': len(parties.get('judges', []))\n",
        "            }\n",
        "            flattened_data.append(flat_record)\n",
        "\n",
        "        # Buat DataFrame\n",
        "        df = pd.DataFrame(flattened_data)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        csv_filename = f\"cases.csv\"\n",
        "\n",
        "        # Simpan ke direktori lokal (/data/processed)\n",
        "        csv_path_local = os.path.join(self.output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_local, index=False, encoding='utf-8')\n",
        "        print(f\"📄 CSV Lokal: {csv_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive (/content/drive/MyDrive/terorisme/data/processed)\n",
        "        csv_path_gdrive = os.path.join(self.gdrive_output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_gdrive, index=False, encoding='utf-8')\n",
        "        print(f\"💾 CSV GDrive: {csv_path_gdrive}\")\n",
        "\n",
        "        return csv_path_local, csv_path_gdrive\n",
        "\n",
        "    def save_metadata_to_json(self, metadata_list: List[Dict]):\n",
        "        \"\"\"Simpan metadata ke JSON lengkap di kedua lokasi\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        json_filename = f\"cases.json\"\n",
        "\n",
        "        # Simpan ke direktori lokal\n",
        "        json_path_local = os.path.join(self.output_dir, json_filename)\n",
        "        with open(json_path_local, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"📄 JSON Lokal: {json_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive\n",
        "        json_path_gdrive = os.path.join(self.gdrive_output_dir, json_filename)\n",
        "        with open(json_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"💾 JSON GDrive: {json_path_gdrive}\")\n",
        "\n",
        "        return json_path_local, json_path_gdrive\n",
        "\n",
        "    def process_all_files(self) -> List[Dict]:\n",
        "        \"\"\"Proses semua file untuk ekstraksi metadata\"\"\"\n",
        "        print(\"🔍 i. EKSTRAKSI METADATA\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Mengambil metadata: Nomor Perkara, Tanggal, Jenis Perkara, Pasal, Pihak, dll.\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Cari file dari kedua lokasi\n",
        "        data_raw_files = self.get_text_files(self.cleaned_dir)\n",
        "        gdrive_files = self.get_text_files(self.gdrive_cleaned_dir)\n",
        "\n",
        "        if data_raw_files:\n",
        "            files_to_process = data_raw_files\n",
        "            source_directory = self.cleaned_dir\n",
        "            print(f\"📂 Menggunakan file dari: {self.cleaned_dir}\")\n",
        "        elif gdrive_files:\n",
        "            files_to_process = gdrive_files\n",
        "            source_directory = self.gdrive_cleaned_dir\n",
        "            print(f\"📂 Menggunakan file dari: {self.gdrive_cleaned_dir}\")\n",
        "        else:\n",
        "            print(\"❌ Tidak ada file teks yang ditemukan!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"📁 Ditemukan {len(files_to_process)} file untuk diproses\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Proses setiap file\n",
        "        all_metadata = []\n",
        "        success_count = 0\n",
        "\n",
        "        for i, filename in enumerate(files_to_process, 1):\n",
        "            print(f\"[{i:3d}/{len(files_to_process)}] \", end=\"\")\n",
        "            metadata = self.process_single_file(filename, source_directory)\n",
        "\n",
        "            if metadata:\n",
        "                all_metadata.append(metadata)\n",
        "                success_count += 1\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"✅ BERHASIL: {success_count} file\")\n",
        "        print(f\"❌ GAGAL: {len(files_to_process) - success_count} file\")\n",
        "\n",
        "        if all_metadata:\n",
        "            # Simpan ke CSV dan JSON di kedua lokasi\n",
        "            csv_paths = self.save_metadata_to_csv(all_metadata)\n",
        "            json_paths = self.save_metadata_to_json(all_metadata)\n",
        "\n",
        "            print(f\"📊 Total metadata berhasil diekstrak: {len(all_metadata)} record\")\n",
        "            print(f\"💾 File tersimpan di 2 lokasi: lokal & Google Drive\")\n",
        "\n",
        "        return all_metadata\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk menjalankan ekstraksi metadata\"\"\"\n",
        "    print(\"🚀 MULAI EKSTRAKSI METADATA PUTUSAN PENGADILAN\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        extractor = MetadataExtractor()\n",
        "        metadata_results = extractor.process_all_files()\n",
        "\n",
        "        if metadata_results:\n",
        "            print(\"\\n🎉 EKSTRAKSI METADATA SELESAI!\")\n",
        "            print(f\"Total metadata: {len(metadata_results)} record\")\n",
        "            print(\"File output tersimpan di:\")\n",
        "            print(\"  - Lokal: /data/processed/\")\n",
        "            print(\"  - GDrive: /content/drive/MyDrive/terorisme/data/processed/\")\n",
        "        else:\n",
        "            print(\"\\n❌ Tidak ada metadata yang berhasil diekstrak.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abOnq0E1x0ID",
        "outputId": "32aa8c97-7d08-4674-99ce-2966428f15c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI EKSTRAKSI METADATA PUTUSAN PENGADILAN\n",
            "======================================================================\n",
            "🔍 EKSTRAKSI METADATA\n",
            "Input: /data/raw atau /content/drive/MyDrive/terorisme/CLEANED\n",
            "Output Lokal: /data/processed\n",
            "Output GDrive: /content/drive/MyDrive/terorisme/data/processed\n",
            "🔍 i. EKSTRAKSI METADATA\n",
            "============================================================\n",
            "Mengambil metadata: Nomor Perkara, Tanggal, Jenis Perkara, Pasal, Pihak, dll.\n",
            "============================================================\n",
            "📂 Menggunakan file dari: /content/drive/MyDrive/terorisme/CLEANED\n",
            "📁 Ditemukan 46 file untuk diproses\n",
            "------------------------------------------------------------\n",
            "[  1/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_631_Pid_Sus_2023_PN_JKT_TIM_Tanggal_14_Desember_2023__Penuntut_Umum_ANDI_JEFRI_ARDIN__S_H_Terdakwa_DIAN_YUDI_SAPUTRA_alias_ABU_HANIF_Bin_WAHYU_ILAHI__Alm.txt\n",
            "[  2/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_629_Pid_Sus_2023_PN_JKT_TIM_Tanggal_14_Desember_2023__Penuntut_Umum_HERRY_WIYANTO__SH__M_HumTerdakwa_TAJUDIN_Als_PAK_HAJI_TAJUDIN_Als_PAK_TEJE_Als_PAKWA_URA.txt\n",
            "[  3/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_555_Pid_Sus_2023_PN_JKT_TIM_Tanggal_13_Desember_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_ARIS_BUDIANTO_alias_RIKO_alias_BAHAR_alias_SARAHARSONO.txt\n",
            "[  4/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_532_Pid_Sus_2023_PN_JKT_TIM_Tanggal_8_Nopember_2023__Penuntut_Umum_DAVID_ROGER_J_PAKPAHAN__SHTerdakwa_IDRIS_ABDILLAH_MAHMUD_ALIAS_SETETES_EMBUN_PAGI_BMAHMUD.txt\n",
            "[  5/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_637_Pid_Sus_2023_PN_JKT_TIM_Tanggal_15_Nopember_2023__Penuntut_Umum_AMRI_BAYAKTA__SHTerdakwa_WAHYUDI__alias_JONI_alias_GUNTUR_alias_FAJAR_Bin_ABDUL_PANUT.txt\n",
            "[  6/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_635_Pid_Sus_2023_PN_JKT_TIM_Tanggal_18_Desember_2023__Penuntut_Umum_ARY_PRATAMA__SHTerdakwa_INDRA_SYAHPUTRA_Alias_INDRA_ONO_Alias_ONO_KAY_Alias_ICAN_AN__Alm.txt\n",
            "[  7/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_544_Pid_Sus_2023_PN_JKT_TIM_Tanggal_13_Nopember_2023__Penuntut_Umum_MALINI_SIANTURI__SHTerdakwa_MUHAMMAD_BUDI_SATRIA_Alias_BUDI__alias_KARI__alias_JUNH__Alm.txt\n",
            "[  8/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_533_Pid_Sus_2023_PN_JKT_TIM_Tanggal_8_Nopember_2023__Penuntut_Umum_DAVID_ROGER_J_PAKPAHAN__SHTerdakwa_SUGENG_Alias_SALMAN_ALGHOZALI_Alias_USTAD_SALMANADENAN.txt\n",
            "[  9/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_594_Pid_Sus_2023_PN_JKT_TIM_Tanggal_1_Nopember_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_DJOKO_UTOMO_alias_JACK_alias_BENI_alias_ARI_aliasURIPDI.txt\n",
            "[ 10/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_528_Pid_Sus_2023_PN_JKT_TIM_Tanggal_19_Oktober_2023__Penuntut_Umum_DWI_AGUS_SETYONINGRUM__SH__MHTerdakwa_RAMANDA_PRATAMA_alias_ABU_KENZI_alias_KURAMA_SUARDI.txt\n",
            "[ 11/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_527_Pid_Sus_2023_PN_JKT_TIM_Tanggal_19_Oktober_2023__Penuntut_Umum_BERLIAN_D_NAINGGOLAN__SHTerdakwa_SYAIFULLAH_RIFAI_Alias_SYAIFULLAH_RIFAI_Alias_SAYFRAFII_.txt\n",
            "[ 12/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_545_Pid_Sus_2023_PN_JKT_TIM_Tanggal_11_Oktober_2023__Penuntut_Umum_MALINI_SIANTURI__SHTerdakwa_DIKA_GARNAKA_alias_HAMZAH_alias_ABU_USAMAH_alias_PEMBURA__Alm.txt\n",
            "[ 13/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_211_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_12_Juli_2023__Penuntut_Umum_ANNISA_RK__SHTerdakwa_SYAHRUL_ALS_TOPAN_ALS_BENZ_ALS_ANGGA_BIN_UMARDI_.txt\n",
            "[ 14/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_230_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_2_Agustus_2023__Penuntut_Umum_TEDDY_IRAWAN___SH___MH_Terdakwa_MUHAMAD_RIDWAN_als_DEK_WAN_als_NYAK_WAN_als_AHMAD_RI_ABBAS.txt\n",
            "[ 15/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_209_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_12_Juli_2023__Penuntut_Umum_HOTMAIDA__SHTerdakwa_ARIADI_Alias_KHAIRUL_alias_FERI_alias_JIWO_Bin_ASNAN.txt\n",
            "[ 16/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_210_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_12_Juli_2023__Penuntut_Umum_MUHAMMAD_FAHRUL__SHTerdakwa_SUTANTO_Alias_AWAN_Alias_EKO_Alias_AHMAD_SYAKIR_Bin_SANIMAN.txt\n",
            "[ 17/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_231_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_2_Agustus_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_TEUKU_MAULIZANSYAH_RAMLI_alias_MAULIDAN_alias_PON_I_TAEB.txt\n",
            "[ 18/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_109_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_21_Juni_2023__Penuntut_Umum_ARIF_SUSANTO__SH__MHTerdakwa_MOH__SYAIFULLAH_A__SAHABA_Alias_MAMAT_Alias_MAT_HERDIANSYSAHABA.txt\n",
            "[ 19/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_110_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_21_Juni_2023__Penuntut_Umum_HASBUDDIN_B_PASENG__SH_Terdakwa_MUH__RIZAL_S_Pd_I_Alias_RIZAL_Alias_MAMAT_Alias_MAT_TONGGALA.txt\n",
            "[ 20/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_114_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_24_Mei_2023__Penuntut_Umum_FAISAL_NUR__SH__MHTerdakwa_MUHAMMAD_INDRA_GARUSU_bin_SAINUL_GARUSU.txt\n",
            "[ 21/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_71_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_17_Mei_2023__Penuntut_Umum_TEDDY_IRAWAN___SH___MH_Terdakwa_HERLIANSYAH_als_ANDI_BASO_als_HERLY_BIN_SULTANNI.txt\n",
            "[ 22/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_73_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_10_Mei_2023__Penuntut_Umum_HARDINIYANTY__SH__MHTerdakwa_LUKMAN_YUNUS_Als_UKO_Als_ABU_SYUKRON_Bin_IDAM_YUNUS.txt\n",
            "[ 23/46] ✅ case_2025_TK1_Putusan_PA_SAMPANG_Nomor_776_Pdt_G_2025_PA_Spg_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 24/46] ⚠️ case_2025_TK1_Putusan_MS_CALANG_Nomor_70_Pdt_P_2025_MS_Cag_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt\n",
            "[ 25/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_8_Pid_Sus_2023_PN_Jkt_Brt_Tanggal_13_April_2023__Penuntut_Umum_1_ANDI_JEFRI_ARDIN__SH_MH2_JAHRUDIN__SH3_DENRI_KASWORO__S_H_4_ZULKIFLI__SH__MH5_KHAREZA_SOLEH.txt\n",
            "[ 26/46] ✅ case_2025_TK1_Putusan_PA_SAMPANG_Nomor_830_Pdt_G_2025_PA_Spg_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 27/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_3_Pid_Sus_2023_PN_Jkt_Brt_Tanggal_13_April_2023__Penuntut_Umum_1_POERWOKO_HADI_SASMITO__SH2_Dra__INDRAYATI__H_S__SH__MH3_ADE_SOLEHUDIN__SH__MH4_MARDIAAYANTO.txt\n",
            "[ 28/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_74_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_10_Mei_2023__Penuntut_Umum_JAYA_SIAHAAN___SH_Terdakwa_KHOIRRUDDIN_Alias_JIHAN_Bin_M__SAIFUL_ANWARY.txt\n",
            "[ 29/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1206_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_6_April_2023__Penuntut_Umum_1_HASBUDDIN_B_PASENG__SH2_HEVBEN__SH3_MUCHAMAD_ADYANSYAH__SH__MH4_PONTI_LUKWINANTI_SHARNAMA.txt\n",
            "[ 30/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1109_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_6_April_2023__Penuntut_Umum_1_JUWITA_KAYANA__S_H___M_H_2_TEDDY_IRAWAN_SH3_ERWIN_INDRAPUTRA__SH__MH4_MUHAMAD_RAMLI_WASIS.txt\n",
            "[ 31/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1110_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_6_April_2023__Penuntut_Umum_1_HOTMAIDA_SH2_AMRIL_ABDI__SH3_REZA_OKTAVIAN__S_H___M_H_4_FEBBY_SALAHUDDIN__S__Kom__SSUKAMA.txt\n",
            "[ 32/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1085_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_ARY_PRATAMA__SH2_ARIF_SUSANTO__SH__MH3_AMRI_BAYAKTA__S_H_4_MARDIANA_YOLANDA_I__SILFFENDI.txt\n",
            "[ 33/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1165_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_AGUS_TRI_HARTONO__S_H___M_Hum_2_BERLIAN_D_NAINGGOLAN__SH__MH3_DWI_AGUS_SETYONINGRUGADMAN.txt\n",
            "[ 34/46] ✅ case_2025_TK1_Putusan_PA_POLEWALI_Nomor_325_Pdt_G_2025_PA_Pwl_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 35/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1172_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_ERWIN_INDRAPUTRA__SH__MH2_TEDDY_IRAWAN_SH3_JUWITA_KAYANA__S_H___M_H_4_KHAREZA_MOKH_MIJAR.txt\n",
            "[ 36/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_4_Pid_Sus_2023_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_DAVID_ROGER_JULIUS_PAKPAHAN__SH2_DEASY_MARIANA_MARUF__SH__MH3_FAISAL_NUR__SH__MH4_SORHYUDIN.txt\n",
            "[ 37/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1171_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_HEVBEN__SH2_HASBUDDIN_B_PASENG__SH3_MUCHAMAD_ADYANSYAH__SH__MH4_DWI_INDAH_KARTIKA_EGIMIN.txt\n",
            "[ 38/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1160_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_SUHARTATI__SH__MH2_MALINI_SIANTURI_SH3_Dr__HERRY_WIYANTO__SH_M_Hum4_NURHAYATI_ULFIAN_ALM.txt\n",
            "[ 39/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1105_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Maret_2023__Penuntut_Umum_1_AMRIL_ABDI__SH2_REZA_OKTAVIAN__S_H___M_H_3_HOTMAIDA_SH4_NANDA_KARMILA__SH5_OCTAVIASAKWID.txt\n",
            "[ 40/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_991_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Maret_2023__Penuntut_Umum_1_UMI_HANINDYA_KUSUMA_SH2_IKA_SYAFITRY_SALIM__SH___MH_3_AGUS_JULIANTO_PURNOMO__SH4_AZCHTIAR.txt\n",
            "[ 41/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_992_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Februari_2023__Penuntut_Umum_1_IKA_SYAFITRY_SALIM__SH___MH_2_UMI_HANINDYA_KUSUMA_SH3_AGUS_JULIANTO_PURNOMO__SH4n_MUSA.txt\n",
            "[ 42/46] ⚠️ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1107_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Maret_2023__Penuntut_Umum_1_ARIF_SUSANTO__SH__MH2_ARY_PRATAMA__SH3_AMRI_BAYAKTA__S_H_4_MUHAMAD_RAMLI__SH5_WULA_WARNO.txt\n",
            "[ 43/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_843_Pid_Sus_2022_PN_Jkt_Tim_Tanggal_8_Februari_2023__Penuntut_Umum_RIFQI_ARIALFA_SH_MHTerdakwa_ADI_SUPRIYADI_Als__DAFA_Als__ANAS_Als__ADI_USAMA_Als__BONIMIN.txt\n",
            "[ 44/46] ⚠️ case_2025_TK1_Putusan_PA_TILAMUTA_Nomor_81_Pdt_P_2025_PA_Tlm_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt\n",
            "[ 45/46] ⚠️ case_2025_TK1_Putusan_PA_TILAMUTA_Nomor_73_Pdt_P_2025_PA_Tlm_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt\n",
            "[ 46/46] ⚠️ case_2025_TK1_Putusan_PA_TILAMUTA_Nomor_82_Pdt_P_2025_PA_Tlm_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt\n",
            "------------------------------------------------------------\n",
            "✅ BERHASIL: 46 file\n",
            "❌ GAGAL: 0 file\n",
            "📄 CSV Lokal: /data/processed/cases.csv\n",
            "💾 CSV GDrive: /content/drive/MyDrive/terorisme/data/processed/cases.csv\n",
            "📄 JSON Lokal: /data/processed/cases.json\n",
            "💾 JSON GDrive: /content/drive/MyDrive/terorisme/data/processed/cases.json\n",
            "📊 Total metadata berhasil diekstrak: 46 record\n",
            "💾 File tersimpan di 2 lokasi: lokal & Google Drive\n",
            "\n",
            "🎉 EKSTRAKSI METADATA SELESAI!\n",
            "Total metadata: 46 record\n",
            "File output tersimpan di:\n",
            "  - Lokal: /data/processed/\n",
            "  - GDrive: /content/drive/MyDrive/terorisme/data/processed/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ekstraksi Konten Kunci**"
      ],
      "metadata": {
        "id": "u02dUDJSyW0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import logging"
      ],
      "metadata": {
        "id": "dWmS4U6UyPe9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class KontenKunciExtractor:\n",
        "    \"\"\"Ekstrak konten kunci dari dokumen putusan pengadilan\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.cleaned_dir = \"/data/raw\"\n",
        "        self.gdrive_cleaned_dir = os.path.join(base_dir, \"CLEANED\")\n",
        "        self.output_dir = \"/data/processed\"  # Output lokal\n",
        "        self.gdrive_output_dir = os.path.join(base_dir, \"data\", \"processed\")  # Output Google Drive\n",
        "        self.logs_dir = \"/logs\"\n",
        "\n",
        "        # Buat direktori\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.gdrive_output_dir, exist_ok=True)  # Buat di Google Drive\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"📝 EKSTRAKSI KONTEN KUNCI\")\n",
        "        print(f\"Input: {self.cleaned_dir} atau {self.gdrive_cleaned_dir}\")\n",
        "        print(f\"Output Lokal: {self.output_dir}\")\n",
        "        print(f\"Output GDrive: {self.gdrive_output_dir}\")\n",
        "\n",
        "        # Setup pola ekstraksi konten\n",
        "        self.setup_content_patterns()\n",
        "\n",
        "    def setup_content_patterns(self):\n",
        "        \"\"\"Setup pola regex untuk ekstraksi konten kunci\"\"\"\n",
        "\n",
        "        # 1. POLA RINGKASAN FAKTA\n",
        "        self.fact_summary_patterns = {\n",
        "            'dakwaan': [\n",
        "                r'dakwaan\\s*:?\\s*(.*?)(?=\\n\\n|\\nterdakwa|\\npenuntut|\\nhakim|\\Z)',\n",
        "                r'terdakwa\\s+didakwa\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'melakukan\\s+perbuatan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'barang_bukti': [\n",
        "                r'barang\\s+bukti\\s*:?\\s*(.*?)(?=\\n\\n|\\nsaksi|\\nhakim|\\Z)',\n",
        "                r'bukti.*?yang\\s+diajukan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'alat\\s+bukti\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'fakta_persidangan': [\n",
        "                r'fakta.*?persidangan\\s*:?\\s*(.*?)(?=\\n\\n|\\npertimbangan|\\Z)',\n",
        "                r'berdasarkan\\s+fakta.*?persidangan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'dari\\s+fakta.*?terungkap\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'kronologi': [\n",
        "                r'kronologi\\s*:?\\s*(.*?)(?=\\n\\n|\\npertimbangan|\\Z)',\n",
        "                r'peristiwa\\s+terjadi\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'pada\\s+tanggal.*?terdakwa\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'kerugian': [\n",
        "                r'kerugian.*?negara\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'merugikan.*?keuangan.*?negara\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'nilai\\s+kerugian\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 2. POLA ARGUMEN HUKUM UTAMA\n",
        "        self.legal_argument_patterns = {\n",
        "            'pertimbangan_hakim': [\n",
        "                r'pertimbangan\\s*:?\\s*(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'menimbang\\s*:?\\s*(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'bahwa.*?hakim\\s+berpendapat\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'pasal_yang_terbukti': [\n",
        "                r'terbukti.*?melanggar\\s+pasal\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'perbuatan.*?memenuhi.*?pasal\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'dakwaan.*?terbukti.*?pasal\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'alasan_putusan': [\n",
        "                r'oleh\\s+karena\\s+itu\\s+(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'dengan\\s+demikian\\s+(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'berdasarkan.*?pertimbangan\\s+(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "            ],\n",
        "            'amar_putusan': [\n",
        "                r'mengadili\\s*:?\\s*(.*?)(?=\\Z)',\n",
        "                r'amar\\s*:?\\s*(.*?)(?=\\Z)',\n",
        "                r'memutuskan\\s*:?\\s*(.*?)(?=\\Z)',\n",
        "            ],\n",
        "            'putusan_hukuman': [\n",
        "                r'menjatuhkan\\s+pidana\\s+(.*?)(?=\\n[0-9]|\\Z)',\n",
        "                r'menghukum\\s+terdakwa\\s+(.*?)(?=\\n[0-9]|\\Z)',\n",
        "                r'pidana\\s+penjara\\s+selama\\s+(.*?)(?=\\n|\\Z)',\n",
        "                r'pidana\\s+denda\\s+sebesar\\s+(.*?)(?=\\n|\\Z)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 3. POLA ELEMEN PENTING LAINNYA\n",
        "        self.other_patterns = {\n",
        "            'saksi': [\n",
        "                r'saksi\\s*:?\\s*(.*?)(?=\\nterdakwa|\\npenuntut|\\Z)',\n",
        "                r'keterangan\\s+saksi\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'ahli': [\n",
        "                r'keterangan\\s+ahli\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'ahli\\s+menerangkan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'pengakuan_terdakwa': [\n",
        "                r'terdakwa\\s+mengaku\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'keterangan\\s+terdakwa\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Bersihkan teks yang diekstrak\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove common legal formatting\n",
        "        text = re.sub(r'\\b(?:pasal|ayat|huruf|angka)\\s+\\([^)]+\\)', '', text)\n",
        "        # Remove line numbers\n",
        "        text = re.sub(r'^\\d+\\.?\\s*', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_fact_summary(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Ekstrak ringkasan fakta dari teks\"\"\"\n",
        "        facts = {}\n",
        "\n",
        "        for category, patterns in self.fact_summary_patterns.items():\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    if match.group(1):\n",
        "                        content = self.clean_extracted_text(match.group(1))\n",
        "                        if len(content) > len(extracted_text):\n",
        "                            extracted_text = content\n",
        "\n",
        "            facts[category] = extracted_text[:1000] if extracted_text else \"\"  # Limit to 1000 chars\n",
        "\n",
        "        return facts\n",
        "\n",
        "    def extract_legal_arguments(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Ekstrak argumen hukum utama\"\"\"\n",
        "        arguments = {}\n",
        "\n",
        "        for category, patterns in self.legal_argument_patterns.items():\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    if match.group(1):\n",
        "                        content = self.clean_extracted_text(match.group(1))\n",
        "                        if len(content) > len(extracted_text):\n",
        "                            extracted_text = content\n",
        "\n",
        "            arguments[category] = extracted_text[:1000] if extracted_text else \"\"\n",
        "\n",
        "        return arguments\n",
        "\n",
        "    def extract_other_elements(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Ekstrak elemen penting lainnya\"\"\"\n",
        "        others = {}\n",
        "\n",
        "        for category, patterns in self.other_patterns.items():\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    if match.group(1):\n",
        "                        content = self.clean_extracted_text(match.group(1))\n",
        "                        if len(content) > len(extracted_text):\n",
        "                            extracted_text = content\n",
        "\n",
        "            others[category] = extracted_text[:500] if extracted_text else \"\"\n",
        "\n",
        "        return others\n",
        "\n",
        "    def extract_key_content_from_text(self, text: str, filename: str) -> Dict:\n",
        "        \"\"\"Ekstrak semua konten kunci dari teks\"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return {}\n",
        "\n",
        "        content = {\n",
        "            'filename': filename,\n",
        "            'extraction_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'text_length': len(text),\n",
        "            'word_count': len(text.split())\n",
        "        }\n",
        "\n",
        "        # 1. Ekstrak ringkasan fakta\n",
        "        fact_summary = self.extract_fact_summary(text)\n",
        "        content['fact_summary'] = fact_summary\n",
        "\n",
        "        # 2. Ekstrak argumen hukum\n",
        "        legal_arguments = self.extract_legal_arguments(text)\n",
        "        content['legal_arguments'] = legal_arguments\n",
        "\n",
        "        # 3. Ekstrak elemen lainnya\n",
        "        other_elements = self.extract_other_elements(text)\n",
        "        content['other_elements'] = other_elements\n",
        "\n",
        "        # 4. Hitung kelengkapan konten\n",
        "        content['content_completeness'] = self.calculate_content_completeness(content)\n",
        "\n",
        "        return content\n",
        "\n",
        "    def calculate_content_completeness(self, content: Dict) -> float:\n",
        "        \"\"\"Hitung persentase kelengkapan konten kunci\"\"\"\n",
        "        essential_elements = [\n",
        "            'dakwaan', 'barang_bukti', 'pertimbangan_hakim',\n",
        "            'amar_putusan', 'putusan_hukuman'\n",
        "        ]\n",
        "\n",
        "        score = 0\n",
        "        fact_summary = content.get('fact_summary', {})\n",
        "        legal_arguments = content.get('legal_arguments', {})\n",
        "\n",
        "        for element in essential_elements:\n",
        "            if element in fact_summary and fact_summary[element]:\n",
        "                score += 1\n",
        "            elif element in legal_arguments and legal_arguments[element]:\n",
        "                score += 1\n",
        "\n",
        "        return (score / len(essential_elements)) * 100\n",
        "\n",
        "    def process_single_file(self, filename: str, source_dir: str) -> Optional[Dict]:\n",
        "        \"\"\"Proses file tunggal untuk ekstraksi konten kunci\"\"\"\n",
        "        file_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.error(f\"File tidak ditemukan: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            if not text.strip():\n",
        "                logger.warning(f\"File kosong: {filename}\")\n",
        "                return None\n",
        "\n",
        "            content = self.extract_key_content_from_text(text, filename)\n",
        "\n",
        "            if content:\n",
        "                completeness = content.get('content_completeness', 0)\n",
        "                print(f\"✅ {filename} (Kelengkapan: {completeness:.1f}%)\")\n",
        "                return content\n",
        "            else:\n",
        "                print(f\"❌ {filename}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error memproses {filename}: {str(e)}\")\n",
        "            print(f\"❌ {filename}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_text_files(self, directory: str) -> List[str]:\n",
        "        \"\"\"Dapatkan daftar file teks\"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            return []\n",
        "        return [f for f in os.listdir(directory)\n",
        "                if f.endswith('.txt') and os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    def save_content_to_csv(self, content_list: List[Dict]):\n",
        "        \"\"\"Simpan konten kunci ke CSV di kedua lokasi\"\"\"\n",
        "        flattened_data = []\n",
        "\n",
        "        for content in content_list:\n",
        "            fact_summary = content.get('fact_summary', {})\n",
        "            legal_arguments = content.get('legal_arguments', {})\n",
        "            other_elements = content.get('other_elements', {})\n",
        "\n",
        "            flat_record = {\n",
        "                # IDENTITAS DOKUMEN\n",
        "                'nama_file': content.get('filename'),\n",
        "                'tanggal_ekstraksi': content.get('extraction_timestamp'),\n",
        "                'panjang_teks': content.get('text_length'),\n",
        "                'jumlah_kata': content.get('word_count'),\n",
        "                'kelengkapan_konten_persen': content.get('content_completeness'),\n",
        "\n",
        "                # RINGKASAN FAKTA\n",
        "                'dakwaan': fact_summary.get('dakwaan', ''),\n",
        "                'barang_bukti': fact_summary.get('barang_bukti', ''),\n",
        "                'fakta_persidangan': fact_summary.get('fakta_persidangan', ''),\n",
        "                'kronologi': fact_summary.get('kronologi', ''),\n",
        "                'kerugian': fact_summary.get('kerugian', ''),\n",
        "\n",
        "                # ARGUMEN HUKUM UTAMA\n",
        "                'pertimbangan_hakim': legal_arguments.get('pertimbangan_hakim', ''),\n",
        "                'pasal_yang_terbukti': legal_arguments.get('pasal_yang_terbukti', ''),\n",
        "                'alasan_putusan': legal_arguments.get('alasan_putusan', ''),\n",
        "                'amar_putusan': legal_arguments.get('amar_putusan', ''),\n",
        "                'putusan_hukuman': legal_arguments.get('putusan_hukuman', ''),\n",
        "\n",
        "                # ELEMEN LAINNYA\n",
        "                'keterangan_saksi': other_elements.get('saksi', ''),\n",
        "                'keterangan_ahli': other_elements.get('ahli', ''),\n",
        "                'pengakuan_terdakwa': other_elements.get('pengakuan_terdakwa', '')\n",
        "            }\n",
        "            flattened_data.append(flat_record)\n",
        "\n",
        "        # Buat DataFrame\n",
        "        df = pd.DataFrame(flattened_data)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        csv_filename = f\"konten_kunci_{timestamp}.csv\"\n",
        "\n",
        "        # Simpan ke direktori lokal\n",
        "        csv_path_local = os.path.join(self.output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_local, index=False, encoding='utf-8')\n",
        "        print(f\"📄 CSV Lokal: {csv_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive\n",
        "        csv_path_gdrive = os.path.join(self.gdrive_output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_gdrive, index=False, encoding='utf-8')\n",
        "        print(f\"💾 CSV GDrive: {csv_path_gdrive}\")\n",
        "\n",
        "        return csv_path_local, csv_path_gdrive\n",
        "\n",
        "    def save_content_to_json(self, content_list: List[Dict]):\n",
        "        \"\"\"Simpan konten kunci ke JSON lengkap di kedua lokasi\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        json_filename = f\"konten_kunci_{timestamp}.json\"\n",
        "\n",
        "        # Simpan ke direktori lokal\n",
        "        json_path_local = os.path.join(self.output_dir, json_filename)\n",
        "        with open(json_path_local, 'w', encoding='utf-8') as f:\n",
        "            json.dump(content_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"📄 JSON Lokal: {json_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive\n",
        "        json_path_gdrive = os.path.join(self.gdrive_output_dir, json_filename)\n",
        "        with open(json_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "            json.dump(content_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"💾 JSON GDrive: {json_path_gdrive}\")\n",
        "\n",
        "        return json_path_local, json_path_gdrive\n",
        "\n",
        "    def create_summary_report(self, content_list: List[Dict]):\n",
        "        \"\"\"Buat laporan ringkasan ekstraksi konten\"\"\"\n",
        "        if not content_list:\n",
        "            return\n",
        "\n",
        "        # Statistik dasar\n",
        "        total_files = len(content_list)\n",
        "        avg_completeness = sum(c.get('content_completeness', 0) for c in content_list) / total_files\n",
        "        avg_length = sum(c.get('text_length', 0) for c in content_list) / total_files\n",
        "\n",
        "        # Hitung coverage untuk setiap elemen\n",
        "        coverage = {}\n",
        "        elements = ['dakwaan', 'barang_bukti', 'pertimbangan_hakim', 'amar_putusan', 'putusan_hukuman']\n",
        "\n",
        "        for element in elements:\n",
        "            count = 0\n",
        "            for content in content_list:\n",
        "                fact_summary = content.get('fact_summary', {})\n",
        "                legal_arguments = content.get('legal_arguments', {})\n",
        "\n",
        "                if (fact_summary.get(element) or legal_arguments.get(element)):\n",
        "                    count += 1\n",
        "\n",
        "            coverage[element] = (count / total_files) * 100\n",
        "\n",
        "        # Buat laporan\n",
        "        report = f\"\"\"\n",
        "📊 LAPORAN EKSTRAKSI KONTEN KUNCI\n",
        "====================================\n",
        "Tanggal: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "STATISTIK UMUM:\n",
        "- Total file diproses: {total_files}\n",
        "- Rata-rata kelengkapan: {avg_completeness:.1f}%\n",
        "- Rata-rata panjang teks: {avg_length:,.0f} karakter\n",
        "\n",
        "COVERAGE ELEMEN KUNCI:\n",
        "- Dakwaan: {coverage.get('dakwaan', 0):.1f}%\n",
        "- Barang Bukti: {coverage.get('barang_bukti', 0):.1f}%\n",
        "- Pertimbangan Hakim: {coverage.get('pertimbangan_hakim', 0):.1f}%\n",
        "- Amar Putusan: {coverage.get('amar_putusan', 0):.1f}%\n",
        "- Putusan Hukuman: {coverage.get('putusan_hukuman', 0):.1f}%\n",
        "\n",
        "FILE DENGAN KELENGKAPAN TINGGI (>80%):\n",
        "\"\"\"\n",
        "\n",
        "        high_quality = [c for c in content_list if c.get('content_completeness', 0) > 80]\n",
        "        for content in high_quality[:10]:\n",
        "            filename = content.get('filename', 'Unknown')\n",
        "            completeness = content.get('content_completeness', 0)\n",
        "            report += f\"- {filename}: {completeness:.1f}%\\n\"\n",
        "\n",
        "        if len(high_quality) > 10:\n",
        "            report += f\"... dan {len(high_quality) - 10} file lainnya\\n\"\n",
        "\n",
        "        # Simpan laporan\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        report_filename = f\"laporan_konten_{timestamp}.txt\"\n",
        "        report_path = os.path.join(self.output_dir, report_filename)\n",
        "\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(f\"📋 Laporan disimpan: {report_path}\")\n",
        "\n",
        "    def process_all_files(self) -> List[Dict]:\n",
        "        \"\"\"Proses semua file untuk ekstraksi konten kunci\"\"\"\n",
        "        print(\"📝 ii. EKSTRAKSI KONTEN KUNCI\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Ringkasan fakta (barang bukti, dakwaan)\")\n",
        "        print(\"2. Argumen hukum utama (putusan, pasal)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Cari file dari kedua lokasi\n",
        "        data_raw_files = self.get_text_files(self.cleaned_dir)\n",
        "        gdrive_files = self.get_text_files(self.gdrive_cleaned_dir)\n",
        "\n",
        "        if data_raw_files:\n",
        "            files_to_process = data_raw_files\n",
        "            source_directory = self.cleaned_dir\n",
        "            print(f\"📂 Menggunakan file dari: {self.cleaned_dir}\")\n",
        "        elif gdrive_files:\n",
        "            files_to_process = gdrive_files\n",
        "            source_directory = self.gdrive_cleaned_dir\n",
        "            print(f\"📂 Menggunakan file dari: {self.gdrive_cleaned_dir}\")\n",
        "        else:\n",
        "            print(\"❌ Tidak ada file teks yang ditemukan!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"📁 Ditemukan {len(files_to_process)} file untuk diproses\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Proses setiap file\n",
        "        all_content = []\n",
        "        success_count = 0\n",
        "\n",
        "        for i, filename in enumerate(files_to_process, 1):\n",
        "            print(f\"[{i:3d}/{len(files_to_process)}] \", end=\"\")\n",
        "            content = self.process_single_file(filename, source_directory)\n",
        "\n",
        "            if content:\n",
        "                all_content.append(content)\n",
        "                success_count += 1\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"✅ BERHASIL: {success_count} file\")\n",
        "        print(f\"❌ GAGAL: {len(files_to_process) - success_count} file\")\n",
        "\n",
        "        if all_content:\n",
        "            # Simpan ke CSV dan JSON di kedua lokasi\n",
        "            csv_paths = self.save_content_to_csv(all_content)\n",
        "            json_paths = self.save_content_to_json(all_content)\n",
        "\n",
        "            # Buat laporan ringkasan\n",
        "            self.create_summary_report(all_content)\n",
        "\n",
        "            print(f\"📊 Total konten kunci berhasil diekstrak: {len(all_content)} record\")\n",
        "            print(f\"💾 File tersimpan di 2 lokasi: lokal & Google Drive\")\n",
        "\n",
        "        return all_content\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk menjalankan ekstraksi konten kunci\"\"\"\n",
        "    print(\"🚀 MULAI EKSTRAKSI KONTEN KUNCI\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        extractor = KontenKunciExtractor()\n",
        "        content_results = extractor.process_all_files()\n",
        "\n",
        "        if content_results:\n",
        "            print(\"\\n🎉 EKSTRAKSI KONTEN KUNCI SELESAI!\")\n",
        "            print(f\"Total konten: {len(content_results)} record\")\n",
        "            print(\"File output tersimpan di: /data/processed/\")\n",
        "        else:\n",
        "            print(\"\\n❌ Tidak ada konten kunci yang berhasil diekstrak.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgAStOszyVKu",
        "outputId": "88bf1098-fe39-413e-c55e-137b9212caf4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI EKSTRAKSI KONTEN KUNCI\n",
            "==================================================\n",
            "📝 EKSTRAKSI KONTEN KUNCI\n",
            "Input: /data/raw atau /content/drive/MyDrive/terorisme/CLEANED\n",
            "Output Lokal: /data/processed\n",
            "Output GDrive: /content/drive/MyDrive/terorisme/data/processed\n",
            "📝 ii. EKSTRAKSI KONTEN KUNCI\n",
            "============================================================\n",
            "1. Ringkasan fakta (barang bukti, dakwaan)\n",
            "2. Argumen hukum utama (putusan, pasal)\n",
            "============================================================\n",
            "📂 Menggunakan file dari: /content/drive/MyDrive/terorisme/CLEANED\n",
            "📁 Ditemukan 46 file untuk diproses\n",
            "------------------------------------------------------------\n",
            "[  1/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_631_Pid_Sus_2023_PN_JKT_TIM_Tanggal_14_Desember_2023__Penuntut_Umum_ANDI_JEFRI_ARDIN__S_H_Terdakwa_DIAN_YUDI_SAPUTRA_alias_ABU_HANIF_Bin_WAHYU_ILAHI__Alm.txt (Kelengkapan: 100.0%)\n",
            "[  2/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_629_Pid_Sus_2023_PN_JKT_TIM_Tanggal_14_Desember_2023__Penuntut_Umum_HERRY_WIYANTO__SH__M_HumTerdakwa_TAJUDIN_Als_PAK_HAJI_TAJUDIN_Als_PAK_TEJE_Als_PAKWA_URA.txt (Kelengkapan: 100.0%)\n",
            "[  3/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_555_Pid_Sus_2023_PN_JKT_TIM_Tanggal_13_Desember_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_ARIS_BUDIANTO_alias_RIKO_alias_BAHAR_alias_SARAHARSONO.txt (Kelengkapan: 100.0%)\n",
            "[  4/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_532_Pid_Sus_2023_PN_JKT_TIM_Tanggal_8_Nopember_2023__Penuntut_Umum_DAVID_ROGER_J_PAKPAHAN__SHTerdakwa_IDRIS_ABDILLAH_MAHMUD_ALIAS_SETETES_EMBUN_PAGI_BMAHMUD.txt (Kelengkapan: 100.0%)\n",
            "[  5/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_637_Pid_Sus_2023_PN_JKT_TIM_Tanggal_15_Nopember_2023__Penuntut_Umum_AMRI_BAYAKTA__SHTerdakwa_WAHYUDI__alias_JONI_alias_GUNTUR_alias_FAJAR_Bin_ABDUL_PANUT.txt (Kelengkapan: 100.0%)\n",
            "[  6/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_635_Pid_Sus_2023_PN_JKT_TIM_Tanggal_18_Desember_2023__Penuntut_Umum_ARY_PRATAMA__SHTerdakwa_INDRA_SYAHPUTRA_Alias_INDRA_ONO_Alias_ONO_KAY_Alias_ICAN_AN__Alm.txt (Kelengkapan: 100.0%)\n",
            "[  7/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_544_Pid_Sus_2023_PN_JKT_TIM_Tanggal_13_Nopember_2023__Penuntut_Umum_MALINI_SIANTURI__SHTerdakwa_MUHAMMAD_BUDI_SATRIA_Alias_BUDI__alias_KARI__alias_JUNH__Alm.txt (Kelengkapan: 100.0%)\n",
            "[  8/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_533_Pid_Sus_2023_PN_JKT_TIM_Tanggal_8_Nopember_2023__Penuntut_Umum_DAVID_ROGER_J_PAKPAHAN__SHTerdakwa_SUGENG_Alias_SALMAN_ALGHOZALI_Alias_USTAD_SALMANADENAN.txt (Kelengkapan: 100.0%)\n",
            "[  9/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_594_Pid_Sus_2023_PN_JKT_TIM_Tanggal_1_Nopember_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_DJOKO_UTOMO_alias_JACK_alias_BENI_alias_ARI_aliasURIPDI.txt (Kelengkapan: 100.0%)\n",
            "[ 10/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_528_Pid_Sus_2023_PN_JKT_TIM_Tanggal_19_Oktober_2023__Penuntut_Umum_DWI_AGUS_SETYONINGRUM__SH__MHTerdakwa_RAMANDA_PRATAMA_alias_ABU_KENZI_alias_KURAMA_SUARDI.txt (Kelengkapan: 100.0%)\n",
            "[ 11/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_527_Pid_Sus_2023_PN_JKT_TIM_Tanggal_19_Oktober_2023__Penuntut_Umum_BERLIAN_D_NAINGGOLAN__SHTerdakwa_SYAIFULLAH_RIFAI_Alias_SYAIFULLAH_RIFAI_Alias_SAYFRAFII_.txt (Kelengkapan: 100.0%)\n",
            "[ 12/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_545_Pid_Sus_2023_PN_JKT_TIM_Tanggal_11_Oktober_2023__Penuntut_Umum_MALINI_SIANTURI__SHTerdakwa_DIKA_GARNAKA_alias_HAMZAH_alias_ABU_USAMAH_alias_PEMBURA__Alm.txt (Kelengkapan: 100.0%)\n",
            "[ 13/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_211_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_12_Juli_2023__Penuntut_Umum_ANNISA_RK__SHTerdakwa_SYAHRUL_ALS_TOPAN_ALS_BENZ_ALS_ANGGA_BIN_UMARDI_.txt (Kelengkapan: 100.0%)\n",
            "[ 14/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_230_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_2_Agustus_2023__Penuntut_Umum_TEDDY_IRAWAN___SH___MH_Terdakwa_MUHAMAD_RIDWAN_als_DEK_WAN_als_NYAK_WAN_als_AHMAD_RI_ABBAS.txt (Kelengkapan: 100.0%)\n",
            "[ 15/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_209_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_12_Juli_2023__Penuntut_Umum_HOTMAIDA__SHTerdakwa_ARIADI_Alias_KHAIRUL_alias_FERI_alias_JIWO_Bin_ASNAN.txt (Kelengkapan: 100.0%)\n",
            "[ 16/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_210_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_12_Juli_2023__Penuntut_Umum_MUHAMMAD_FAHRUL__SHTerdakwa_SUTANTO_Alias_AWAN_Alias_EKO_Alias_AHMAD_SYAKIR_Bin_SANIMAN.txt (Kelengkapan: 100.0%)\n",
            "[ 17/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_231_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_2_Agustus_2023__Penuntut_Umum_ERWIN_INDRAPUTRA__SH___MHTerdakwa_TEUKU_MAULIZANSYAH_RAMLI_alias_MAULIDAN_alias_PON_I_TAEB.txt (Kelengkapan: 100.0%)\n",
            "[ 18/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_109_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_21_Juni_2023__Penuntut_Umum_ARIF_SUSANTO__SH__MHTerdakwa_MOH__SYAIFULLAH_A__SAHABA_Alias_MAMAT_Alias_MAT_HERDIANSYSAHABA.txt (Kelengkapan: 100.0%)\n",
            "[ 19/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_110_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_21_Juni_2023__Penuntut_Umum_HASBUDDIN_B_PASENG__SH_Terdakwa_MUH__RIZAL_S_Pd_I_Alias_RIZAL_Alias_MAMAT_Alias_MAT_TONGGALA.txt (Kelengkapan: 100.0%)\n",
            "[ 20/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_114_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_24_Mei_2023__Penuntut_Umum_FAISAL_NUR__SH__MHTerdakwa_MUHAMMAD_INDRA_GARUSU_bin_SAINUL_GARUSU.txt (Kelengkapan: 100.0%)\n",
            "[ 21/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_71_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_17_Mei_2023__Penuntut_Umum_TEDDY_IRAWAN___SH___MH_Terdakwa_HERLIANSYAH_als_ANDI_BASO_als_HERLY_BIN_SULTANNI.txt (Kelengkapan: 100.0%)\n",
            "[ 22/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_73_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_10_Mei_2023__Penuntut_Umum_HARDINIYANTY__SH__MHTerdakwa_LUKMAN_YUNUS_Als_UKO_Als_ABU_SYUKRON_Bin_IDAM_YUNUS.txt (Kelengkapan: 100.0%)\n",
            "[ 23/46] ✅ case_2025_TK1_Putusan_PA_SAMPANG_Nomor_776_Pdt_G_2025_PA_Spg_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 24/46] ✅ case_2025_TK1_Putusan_MS_CALANG_Nomor_70_Pdt_P_2025_MS_Cag_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt (Kelengkapan: 80.0%)\n",
            "[ 25/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_8_Pid_Sus_2023_PN_Jkt_Brt_Tanggal_13_April_2023__Penuntut_Umum_1_ANDI_JEFRI_ARDIN__SH_MH2_JAHRUDIN__SH3_DENRI_KASWORO__S_H_4_ZULKIFLI__SH__MH5_KHAREZA_SOLEH.txt (Kelengkapan: 100.0%)\n",
            "[ 26/46] ✅ case_2025_TK1_Putusan_PA_SAMPANG_Nomor_830_Pdt_G_2025_PA_Spg_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 27/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_3_Pid_Sus_2023_PN_Jkt_Brt_Tanggal_13_April_2023__Penuntut_Umum_1_POERWOKO_HADI_SASMITO__SH2_Dra__INDRAYATI__H_S__SH__MH3_ADE_SOLEHUDIN__SH__MH4_MARDIAAYANTO.txt (Kelengkapan: 100.0%)\n",
            "[ 28/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_74_Pid_Sus_2023_PN_Jkt_Tim_Tanggal_10_Mei_2023__Penuntut_Umum_JAYA_SIAHAAN___SH_Terdakwa_KHOIRRUDDIN_Alias_JIHAN_Bin_M__SAIFUL_ANWARY.txt (Kelengkapan: 100.0%)\n",
            "[ 29/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1206_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_6_April_2023__Penuntut_Umum_1_HASBUDDIN_B_PASENG__SH2_HEVBEN__SH3_MUCHAMAD_ADYANSYAH__SH__MH4_PONTI_LUKWINANTI_SHARNAMA.txt (Kelengkapan: 100.0%)\n",
            "[ 30/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1109_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_6_April_2023__Penuntut_Umum_1_JUWITA_KAYANA__S_H___M_H_2_TEDDY_IRAWAN_SH3_ERWIN_INDRAPUTRA__SH__MH4_MUHAMAD_RAMLI_WASIS.txt (Kelengkapan: 100.0%)\n",
            "[ 31/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1110_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_6_April_2023__Penuntut_Umum_1_HOTMAIDA_SH2_AMRIL_ABDI__SH3_REZA_OKTAVIAN__S_H___M_H_4_FEBBY_SALAHUDDIN__S__Kom__SSUKAMA.txt (Kelengkapan: 100.0%)\n",
            "[ 32/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1085_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_ARY_PRATAMA__SH2_ARIF_SUSANTO__SH__MH3_AMRI_BAYAKTA__S_H_4_MARDIANA_YOLANDA_I__SILFFENDI.txt (Kelengkapan: 100.0%)\n",
            "[ 33/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1165_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_AGUS_TRI_HARTONO__S_H___M_Hum_2_BERLIAN_D_NAINGGOLAN__SH__MH3_DWI_AGUS_SETYONINGRUGADMAN.txt (Kelengkapan: 100.0%)\n",
            "[ 34/46] ✅ case_2025_TK1_Putusan_PA_POLEWALI_Nomor_325_Pdt_G_2025_PA_Pwl_Tanggal_12_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 35/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1172_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_ERWIN_INDRAPUTRA__SH__MH2_TEDDY_IRAWAN_SH3_JUWITA_KAYANA__S_H___M_H_4_KHAREZA_MOKH_MIJAR.txt (Kelengkapan: 100.0%)\n",
            "[ 36/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_4_Pid_Sus_2023_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_DAVID_ROGER_JULIUS_PAKPAHAN__SH2_DEASY_MARIANA_MARUF__SH__MH3_FAISAL_NUR__SH__MH4_SORHYUDIN.txt (Kelengkapan: 100.0%)\n",
            "[ 37/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1171_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_HEVBEN__SH2_HASBUDDIN_B_PASENG__SH3_MUCHAMAD_ADYANSYAH__SH__MH4_DWI_INDAH_KARTIKA_EGIMIN.txt (Kelengkapan: 100.0%)\n",
            "[ 38/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1160_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_30_Maret_2023__Penuntut_Umum_1_SUHARTATI__SH__MH2_MALINI_SIANTURI_SH3_Dr__HERRY_WIYANTO__SH_M_Hum4_NURHAYATI_ULFIAN_ALM.txt (Kelengkapan: 100.0%)\n",
            "[ 39/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1105_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Maret_2023__Penuntut_Umum_1_AMRIL_ABDI__SH2_REZA_OKTAVIAN__S_H___M_H_3_HOTMAIDA_SH4_NANDA_KARMILA__SH5_OCTAVIASAKWID.txt (Kelengkapan: 100.0%)\n",
            "[ 40/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_991_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Maret_2023__Penuntut_Umum_1_UMI_HANINDYA_KUSUMA_SH2_IKA_SYAFITRY_SALIM__SH___MH_3_AGUS_JULIANTO_PURNOMO__SH4_AZCHTIAR.txt (Kelengkapan: 100.0%)\n",
            "[ 41/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_992_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Februari_2023__Penuntut_Umum_1_IKA_SYAFITRY_SALIM__SH___MH_2_UMI_HANINDYA_KUSUMA_SH3_AGUS_JULIANTO_PURNOMO__SH4n_MUSA.txt (Kelengkapan: 100.0%)\n",
            "[ 42/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_BARAT_Nomor_1107_Pid_Sus_2022_PN_Jkt_Brt_Tanggal_16_Maret_2023__Penuntut_Umum_1_ARIF_SUSANTO__SH__MH2_ARY_PRATAMA__SH3_AMRI_BAYAKTA__S_H_4_MUHAMAD_RAMLI__SH5_WULA_WARNO.txt (Kelengkapan: 100.0%)\n",
            "[ 43/46] ✅ case_2023_TK1_Putusan_PN_JAKARTA_TIMUR_Nomor_843_Pid_Sus_2022_PN_Jkt_Tim_Tanggal_8_Februari_2023__Penuntut_Umum_RIFQI_ARIALFA_SH_MHTerdakwa_ADI_SUPRIYADI_Als__DAFA_Als__ANAS_Als__ADI_USAMA_Als__BONIMIN.txt (Kelengkapan: 100.0%)\n",
            "[ 44/46] ✅ case_2025_TK1_Putusan_PA_TILAMUTA_Nomor_81_Pdt_P_2025_PA_Tlm_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt (Kelengkapan: 60.0%)\n",
            "[ 45/46] ✅ case_2025_TK1_Putusan_PA_TILAMUTA_Nomor_73_Pdt_P_2025_PA_Tlm_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt (Kelengkapan: 60.0%)\n",
            "[ 46/46] ✅ case_2025_TK1_Putusan_PA_TILAMUTA_Nomor_82_Pdt_P_2025_PA_Tlm_Tanggal_12_Juni_2025__Pemohon_melawan_Termohon.txt (Kelengkapan: 60.0%)\n",
            "------------------------------------------------------------\n",
            "✅ BERHASIL: 46 file\n",
            "❌ GAGAL: 0 file\n",
            "📄 CSV Lokal: /data/processed/konten_kunci_20250625_121328.csv\n",
            "💾 CSV GDrive: /content/drive/MyDrive/terorisme/data/processed/konten_kunci_20250625_121328.csv\n",
            "📄 JSON Lokal: /data/processed/konten_kunci_20250625_121328.json\n",
            "💾 JSON GDrive: /content/drive/MyDrive/terorisme/data/processed/konten_kunci_20250625_121328.json\n",
            "📋 Laporan disimpan: /data/processed/laporan_konten_20250625_121328.txt\n",
            "📊 Total konten kunci berhasil diekstrak: 46 record\n",
            "💾 File tersimpan di 2 lokasi: lokal & Google Drive\n",
            "\n",
            "🎉 EKSTRAKSI KONTEN KUNCI SELESAI!\n",
            "Total konten: 46 record\n",
            "File output tersimpan di: /data/processed/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "XWpPo-8ozIP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from collections import Counter\n",
        "import logging\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "dxHe5OI8yVIW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Generate features untuk machine learning dari dokumen putusan\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/terorisme\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.cleaned_dir = \"/data/raw\"\n",
        "        self.gdrive_cleaned_dir = os.path.join(base_dir, \"CLEANED\")\n",
        "        self.input_dir = \"/data/processed\"  # Input dari langkah sebelumnya\n",
        "        self.output_dir = \"/data/processed\"\n",
        "        self.gdrive_output_dir = os.path.join(base_dir, \"data\", \"processed\")  # Output Google Drive\n",
        "        self.logs_dir = \"/logs\"\n",
        "\n",
        "        # Buat direktori\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.gdrive_output_dir, exist_ok=True)\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"🔧 FEATURE ENGINEERING\")\n",
        "        print(f\"Input teks: {self.cleaned_dir} atau {self.gdrive_cleaned_dir}\")\n",
        "        print(f\"Input metadata: {self.input_dir}\")\n",
        "        print(f\"Output Lokal: {self.output_dir}\")\n",
        "        print(f\"Output GDrive: {self.gdrive_output_dir}\")\n",
        "\n",
        "        # Initialize vectorizers\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=1000,\n",
        "            stop_words=self.get_indonesian_stopwords(),\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "        self.count_vectorizer = CountVectorizer(\n",
        "            max_features=500,\n",
        "            stop_words=self.get_indonesian_stopwords(),\n",
        "            ngram_range=(1, 1),\n",
        "            min_df=2\n",
        "        )\n",
        "\n",
        "        # Label encoders dan scalers\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def get_indonesian_stopwords(self) -> List[str]:\n",
        "        \"\"\"Daftar stopwords bahasa Indonesia untuk legal documents\"\"\"\n",
        "        return [\n",
        "            'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'dengan', 'untuk', 'dalam', 'oleh',\n",
        "            'adalah', 'akan', 'telah', 'sudah', 'dapat', 'harus', 'tidak', 'belum', 'juga',\n",
        "            'bahwa', 'sebagai', 'atau', 'jika', 'karena', 'sehingga', 'maka', 'agar', 'itu',\n",
        "            'ini', 'tersebut', 'hal', 'ada', 'sebuah', 'suatu', 'semua', 'setiap', 'beberapa',\n",
        "            'pengadilan', 'hakim', 'terdakwa', 'penggugat', 'tergugat', 'putusan', 'perkara',\n",
        "            'pasal', 'undang', 'hukum', 'pidana', 'perdata', 'nomor', 'tanggal', 'tahun',\n",
        "            'republik', 'indonesia', 'negeri', 'jaksa', 'penuntut', 'umum', 'saksi', 'bukti'\n",
        "        ]\n",
        "\n",
        "    def load_text_files(self) -> Dict[str, str]:\n",
        "        \"\"\"Load semua file teks yang sudah dibersihkan\"\"\"\n",
        "        texts = {}\n",
        "\n",
        "        # Cek direktori mana yang tersedia\n",
        "        if os.path.exists(self.cleaned_dir):\n",
        "            source_dir = self.cleaned_dir\n",
        "        elif os.path.exists(self.gdrive_cleaned_dir):\n",
        "            source_dir = self.gdrive_cleaned_dir\n",
        "        else:\n",
        "            logger.error(\"Tidak ada direktori teks yang ditemukan\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"📂 Loading teks dari: {source_dir}\")\n",
        "\n",
        "        for filename in os.listdir(source_dir):\n",
        "            if filename.endswith('.txt'):\n",
        "                filepath = os.path.join(source_dir, filename)\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                        text = f.read()\n",
        "                    texts[filename] = text\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "        print(f\"📁 Loaded {len(texts)} file teks\")\n",
        "        return texts\n",
        "\n",
        "    def load_metadata(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Load metadata dari langkah i\"\"\"\n",
        "        metadata_files = [f for f in os.listdir(self.input_dir) if f.startswith('metadata_') and f.endswith('.csv')]\n",
        "\n",
        "        if not metadata_files:\n",
        "            logger.warning(\"Tidak ada file metadata yang ditemukan\")\n",
        "            return None\n",
        "\n",
        "        # Ambil file metadata terbaru\n",
        "        latest_metadata = max(metadata_files)\n",
        "        metadata_path = os.path.join(self.input_dir, latest_metadata)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(metadata_path, encoding='utf-8')\n",
        "            print(f\"📊 Loaded metadata: {len(df)} records dari {latest_metadata}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading metadata: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_content(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Load konten kunci dari langkah ii\"\"\"\n",
        "        content_files = [f for f in os.listdir(self.input_dir) if f.startswith('konten_kunci_') and f.endswith('.csv')]\n",
        "\n",
        "        if not content_files:\n",
        "            logger.warning(\"Tidak ada file konten kunci yang ditemukan\")\n",
        "            return None\n",
        "\n",
        "        # Ambil file konten terbaru\n",
        "        latest_content = max(content_files)\n",
        "        content_path = os.path.join(self.input_dir, latest_content)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(content_path, encoding='utf-8')\n",
        "            print(f\"📝 Loaded konten kunci: {len(df)} records dari {latest_content}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading konten: {e}\")\n",
        "            return None\n",
        "\n",
        "    def count_syllables(self, word: str) -> int:\n",
        "        \"\"\"Estimasi jumlah suku kata dalam kata (untuk bahasa Indonesia)\"\"\"\n",
        "        vowels = 'aeiouAEIOU'\n",
        "        syllable_count = 0\n",
        "        prev_char_was_vowel = False\n",
        "\n",
        "        for char in word:\n",
        "            if char in vowels:\n",
        "                if not prev_char_was_vowel:\n",
        "                    syllable_count += 1\n",
        "                prev_char_was_vowel = True\n",
        "            else:\n",
        "                prev_char_was_vowel = False\n",
        "\n",
        "        return max(1, syllable_count)  # Minimal 1 suku kata\n",
        "\n",
        "    def calculate_flesch_score(self, words: int, sentences: int, syllables: int) -> float:\n",
        "        \"\"\"Hitung Flesch Reading Ease Score (adaptasi untuk Indonesia)\"\"\"\n",
        "        if sentences == 0 or words == 0:\n",
        "            return 0\n",
        "\n",
        "        avg_sentence_length = words / sentences\n",
        "        avg_syllables_per_word = syllables / words\n",
        "\n",
        "        # Formula Flesch (disesuaikan untuk bahasa Indonesia)\n",
        "        score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n",
        "        return max(0, min(100, score))\n",
        "\n",
        "    def calculate_text_features(self, texts: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"Hitung fitur-fitur teks dasar yang komprehensif\"\"\"\n",
        "        features_data = []\n",
        "\n",
        "        for filename, text in texts.items():\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            # Preprocessing\n",
        "            text_lower = text.lower()\n",
        "            words = text.split()\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            paragraphs = [p for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "            # 1. BASIC LENGTH FEATURES\n",
        "            char_count = len(text)\n",
        "            word_count = len(words)\n",
        "            sentence_count = len(sentences)\n",
        "            paragraph_count = len(paragraphs)\n",
        "\n",
        "            # 2. ADVANCED LEXICAL FEATURES\n",
        "            unique_words = len(set([w.lower() for w in words]))\n",
        "            lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
        "            avg_word_length = np.mean([len(word) for word in words]) if word_count > 0 else 0\n",
        "            avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
        "\n",
        "            # 3. READABILITY METRICS\n",
        "            syllable_count = sum([self.count_syllables(word) for word in words])\n",
        "            flesch_score = self.calculate_flesch_score(word_count, sentence_count, syllable_count)\n",
        "\n",
        "            # 4. LEGAL DOCUMENT SPECIFIC FEATURES\n",
        "            legal_terms_count = len(re.findall(r'\\b(?:pasal|undang|hukum|pidana|perdata|terdakwa|penggugat|hakim|jaksa|dakwaan|vonis|putusan)\\b', text_lower))\n",
        "            law_references = len(re.findall(r'\\b(?:uu|undang.*?undang)\\s+(?:no\\.?|nomor)\\s+\\d+', text_lower))\n",
        "            article_references = len(re.findall(r'\\bpasal\\s+\\d+', text_lower))\n",
        "            court_mentions = len(re.findall(r'\\bpengadilan\\s+(?:negeri|tinggi|agama|militer)', text_lower))\n",
        "\n",
        "            # 5. NUMERIC AND DATE FEATURES\n",
        "            number_mentions = len(re.findall(r'\\b\\d+\\b', text))\n",
        "            date_mentions = len(re.findall(r'\\b\\d{1,2}[\\s/\\-]\\w+[\\s/\\-]\\d{4}\\b', text))\n",
        "            money_mentions = len(re.findall(r'\\b(?:rp\\.?|rupiah)\\s*\\d', text_lower))\n",
        "            percentage_mentions = len(re.findall(r'\\d+\\s*%', text))\n",
        "\n",
        "            # 6. STRUCTURAL DOCUMENT FEATURES\n",
        "            has_header = 1 if re.search(r'\\bputusan\\b.*\\bpengadilan\\b', text_lower) else 0\n",
        "            has_case_number = 1 if re.search(r'\\bnomor\\s*:\\s*\\d+/', text_lower) else 0\n",
        "            has_parties = 1 if re.search(r'\\b(?:terdakwa|penggugat|tergugat)\\s*:', text_lower) else 0\n",
        "            has_consideration = 1 if re.search(r'\\bmenimbang\\b', text_lower) else 0\n",
        "            has_decision = 1 if re.search(r'\\bmengadili\\b', text_lower) else 0\n",
        "            has_conclusion = 1 if re.search(r'\\b(?:demikian|kesimpulan)\\b', text_lower) else 0\n",
        "\n",
        "            # 7. CONTENT TYPE FEATURES\n",
        "            has_dakwaan = 1 if re.search(r'\\bdakwaan\\b', text_lower) else 0\n",
        "            has_tuntutan = 1 if re.search(r'\\btuntutan\\b', text_lower) else 0\n",
        "            has_pembelaan = 1 if re.search(r'\\bpembelaan\\b', text_lower) else 0\n",
        "            has_saksi = 1 if re.search(r'\\bsaksi\\b', text_lower) else 0\n",
        "            has_bukti = 1 if re.search(r'\\bbarang\\s+bukti\\b', text_lower) else 0\n",
        "\n",
        "            # 8. LINGUISTIC COMPLEXITY FEATURES\n",
        "            complex_sentences = len([s for s in sentences if len(s.split()) > 20])\n",
        "            question_count = len(re.findall(r'\\?', text))\n",
        "            exclamation_count = len(re.findall(r'!', text))\n",
        "            quoted_text_count = len(re.findall(r'\"[^\"]*\"', text))\n",
        "\n",
        "            # 9. PARTY INVOLVEMENT FEATURES\n",
        "            defendant_mentions = len(re.findall(r'\\bterdakwa\\b', text_lower))\n",
        "            plaintiff_mentions = len(re.findall(r'\\bpenggugat\\b', text_lower))\n",
        "            judge_mentions = len(re.findall(r'\\bhakim\\b', text_lower))\n",
        "            prosecutor_mentions = len(re.findall(r'\\bjaksa\\b', text_lower))\n",
        "\n",
        "            # 10. LEGAL REASONING FEATURES\n",
        "            because_count = len(re.findall(r'\\b(?:karena|sebab|disebabkan)\\b', text_lower))\n",
        "            therefore_count = len(re.findall(r'\\b(?:oleh karena|dengan demikian|maka)\\b', text_lower))\n",
        "            evidence_count = len(re.findall(r'\\b(?:bukti|terbukti|membuktikan)\\b', text_lower))\n",
        "            violation_count = len(re.findall(r'\\b(?:melanggar|pelanggaran|melakukan)\\b', text_lower))\n",
        "\n",
        "            features_data.append({\n",
        "                'nama_file': filename,\n",
        "\n",
        "                # Basic Features\n",
        "                'char_count': char_count,\n",
        "                'word_count': word_count,\n",
        "                'sentence_count': sentence_count,\n",
        "                'paragraph_count': paragraph_count,\n",
        "\n",
        "                # Lexical Features\n",
        "                'unique_words': unique_words,\n",
        "                'lexical_diversity': lexical_diversity,\n",
        "                'avg_word_length': avg_word_length,\n",
        "                'avg_sentence_length': avg_sentence_length,\n",
        "\n",
        "                # Readability\n",
        "                'syllable_count': syllable_count,\n",
        "                'flesch_score': flesch_score,\n",
        "\n",
        "                # Legal Specific\n",
        "                'legal_terms_count': legal_terms_count,\n",
        "                'law_references': law_references,\n",
        "                'article_references': article_references,\n",
        "                'court_mentions': court_mentions,\n",
        "\n",
        "                # Numeric Features\n",
        "                'number_mentions': number_mentions,\n",
        "                'date_mentions': date_mentions,\n",
        "                'money_mentions': money_mentions,\n",
        "                'percentage_mentions': percentage_mentions,\n",
        "\n",
        "                # Document Structure\n",
        "                'has_header': has_header,\n",
        "                'has_case_number': has_case_number,\n",
        "                'has_parties': has_parties,\n",
        "                'has_consideration': has_consideration,\n",
        "                'has_decision': has_decision,\n",
        "                'has_conclusion': has_conclusion,\n",
        "\n",
        "                # Content Type\n",
        "                'has_dakwaan': has_dakwaan,\n",
        "                'has_tuntutan': has_tuntutan,\n",
        "                'has_pembelaan': has_pembelaan,\n",
        "                'has_saksi': has_saksi,\n",
        "                'has_bukti': has_bukti,\n",
        "\n",
        "                # Linguistic Complexity\n",
        "                'complex_sentences': complex_sentences,\n",
        "                'question_count': question_count,\n",
        "                'exclamation_count': exclamation_count,\n",
        "                'quoted_text_count': quoted_text_count,\n",
        "\n",
        "                # Party Mentions\n",
        "                'defendant_mentions': defendant_mentions,\n",
        "                'plaintiff_mentions': plaintiff_mentions,\n",
        "                'judge_mentions': judge_mentions,\n",
        "                'prosecutor_mentions': prosecutor_mentions,\n",
        "\n",
        "                # Legal Reasoning\n",
        "                'because_count': because_count,\n",
        "                'therefore_count': therefore_count,\n",
        "                'evidence_count': evidence_count,\n",
        "                'violation_count': violation_count\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(features_data)\n",
        "\n",
        "    def create_bag_of_words_features(self, texts: Dict[str, str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Buat bag-of-words dan TF-IDF features dengan preprocessing yang lebih baik\"\"\"\n",
        "        filenames = list(texts.keys())\n",
        "        text_content = [texts[fname] for fname in filenames]\n",
        "\n",
        "        # Filter empty texts\n",
        "        valid_indices = [i for i, text in enumerate(text_content) if text.strip()]\n",
        "        valid_filenames = [filenames[i] for i in valid_indices]\n",
        "        valid_texts = [text_content[i] for i in valid_indices]\n",
        "\n",
        "        if not valid_texts:\n",
        "            return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "        # Preprocessing teks\n",
        "        processed_texts = []\n",
        "        for text in valid_texts:\n",
        "            # Bersihkan teks\n",
        "            text = re.sub(r'\\d+', 'NUMBER', text)  # Replace numbers\n",
        "            text = re.sub(r'[^\\w\\s]', ' ', text)   # Remove punctuation\n",
        "            text = re.sub(r'\\s+', ' ', text)       # Normalize whitespace\n",
        "            processed_texts.append(text.lower())\n",
        "\n",
        "        # 1. COUNT VECTORIZER (Bag of Words)\n",
        "        try:\n",
        "            count_matrix = self.count_vectorizer.fit_transform(processed_texts)\n",
        "            count_feature_names = self.count_vectorizer.get_feature_names_out()\n",
        "\n",
        "            bow_df = pd.DataFrame(\n",
        "                count_matrix.toarray(),\n",
        "                columns=[f'bow_{name}' for name in count_feature_names],\n",
        "                index=valid_filenames\n",
        "            )\n",
        "            bow_df.reset_index(inplace=True)\n",
        "            bow_df.rename(columns={'index': 'nama_file'}, inplace=True)\n",
        "\n",
        "            print(f\"🎯 Bag-of-Words: {bow_df.shape[1]-1} features\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating BoW features: {e}\")\n",
        "            bow_df = pd.DataFrame()\n",
        "\n",
        "        # 2. TF-IDF VECTORIZER\n",
        "        try:\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_texts)\n",
        "            tfidf_feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "            tfidf_df = pd.DataFrame(\n",
        "                tfidf_matrix.toarray(),\n",
        "                columns=[f'tfidf_{name}' for name in tfidf_feature_names],\n",
        "                index=valid_filenames\n",
        "            )\n",
        "            tfidf_df.reset_index(inplace=True)\n",
        "            tfidf_df.rename(columns={'index': 'nama_file'}, inplace=True)\n",
        "\n",
        "            print(f\"📈 TF-IDF: {tfidf_df.shape[1]-1} features\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating TF-IDF features: {e}\")\n",
        "            tfidf_df = pd.DataFrame()\n",
        "\n",
        "        return bow_df, tfidf_df\n",
        "\n",
        "    def create_qa_pairs(self, texts: Dict[str, str], metadata_df: pd.DataFrame = None) -> List[Dict]:\n",
        "        \"\"\"Buat QA pairs sederhana untuk training dengan lebih banyak template\"\"\"\n",
        "        qa_pairs = []\n",
        "\n",
        "        for filename, text in texts.items():\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            # Template QA yang lebih komprehensif\n",
        "            qa_templates = [\n",
        "                # Basic Information Extraction\n",
        "                {\n",
        "                    'question': 'Siapa terdakwa dalam perkara ini?',\n",
        "                    'pattern': r'terdakwa\\s*:\\s*([A-Z][^\\n\\r,;]+?)(?:\\s*(?:,|;|\\n))',\n",
        "                    'type': 'entity_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Siapa hakim dalam perkara ini?',\n",
        "                    'pattern': r'hakim\\s+(?:ketua|anggota)\\s*:\\s*([A-Z][^\\n\\r,;]+?)(?:\\s*(?:,|;|\\n))',\n",
        "                    'type': 'entity_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Siapa jaksa penuntut umum?',\n",
        "                    'pattern': r'(?:jaksa\\s+)?penuntut\\s+umum\\s*:\\s*([A-Z][^\\n\\r,;]+?)(?:\\s*(?:,|;|\\n))',\n",
        "                    'type': 'entity_extraction'\n",
        "                },\n",
        "\n",
        "                # Case Classification\n",
        "                {\n",
        "                    'question': 'Apa jenis perkara ini?',\n",
        "                    'pattern': r'perkara\\s+(pidana\\s+(?:khusus|umum)|perdata|tindak\\s+pidana\\s+\\w+)',\n",
        "                    'type': 'classification'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Apakah ini perkara korupsi?',\n",
        "                    'pattern': r'(?:perkara\\s+)?tindak\\s+pidana\\s+(korupsi)',\n",
        "                    'type': 'classification'\n",
        "                },\n",
        "\n",
        "                # Legal References\n",
        "                {\n",
        "                    'question': 'Pasal apa yang dilanggar?',\n",
        "                    'pattern': r'(?:melanggar\\s+)?pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)',\n",
        "                    'type': 'legal_reference'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Undang-undang apa yang dirujuk?',\n",
        "                    'pattern': r'(?:undang[- ]undang|uu)\\s+(?:republik\\s+indonesia\\s+)?(?:no\\.?\\s*|nomor\\s+)?(\\d+\\s+tahun\\s+\\d{4})',\n",
        "                    'type': 'legal_reference'\n",
        "                },\n",
        "\n",
        "                # Temporal Information\n",
        "                {\n",
        "                    'question': 'Kapan putusan ini dibacakan?',\n",
        "                    'pattern': r'(?:dibacakan|diputuskan)\\s+(?:pada\\s+)?tanggal\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n",
        "                    'type': 'date_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Kapan tindak pidana terjadi?',\n",
        "                    'pattern': r'(?:pada|tanggal)\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})[^,]*(?:terdakwa|melakukan)',\n",
        "                    'type': 'date_extraction'\n",
        "                },\n",
        "\n",
        "                # Decision and Punishment\n",
        "                {\n",
        "                    'question': 'Apa putusan hakim?',\n",
        "                    'pattern': r'mengadili\\s*:\\s*([^.]+\\.)',\n",
        "                    'type': 'decision_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Berapa lama hukuman yang dijatuhkan?',\n",
        "                    'pattern': r'(?:dengan\\s+)?hukuman\\s+(?:penjara|kurungan)\\s+(?:selama\\s+)?(\\d+\\s+(?:tahun|bulan|hari))',\n",
        "                    'type': 'punishment_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Berapa denda yang harus dibayar?',\n",
        "                    'pattern': r'(?:denda|membayar)\\s+(?:sebesar\\s+)?(?:rp\\.?\\s*)?([0-9.,]+(?:\\s*(?:juta|ribu|miliar))?)',\n",
        "                    'type': 'fine_extraction'\n",
        "                },\n",
        "\n",
        "                # Evidence and Facts\n",
        "                {\n",
        "                    'question': 'Apa barang bukti dalam perkara ini?',\n",
        "                    'pattern': r'barang\\s+bukti\\s*(?:berupa|adalah)?\\s*:?\\s*([^.]+)',\n",
        "                    'type': 'evidence_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Berapa kerugian yang ditimbulkan?',\n",
        "                    'pattern': r'kerugian\\s+(?:negara|keuangan)\\s+(?:sebesar\\s+)?(?:rp\\.?\\s*)?([0-9.,]+(?:\\s*(?:juta|ribu|miliar))?)',\n",
        "                    'type': 'damage_extraction'\n",
        "                },\n",
        "\n",
        "                # Legal Reasoning\n",
        "                {\n",
        "                    'question': 'Mengapa terdakwa dianggap bersalah?',\n",
        "                    'pattern': r'(?:terbukti|bersalah)\\s+(?:secara\\s+sah\\s+dan\\s+meyakinkan\\s+)?([^.]+)',\n",
        "                    'type': 'reasoning_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Apa pertimbangan hakim?',\n",
        "                    'pattern': r'menimbang\\s*[,:]\\s*([^;]+)',\n",
        "                    'type': 'consideration_extraction'\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            for template in qa_templates:\n",
        "                matches = re.finditer(template['pattern'], text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    answer = match.group(1).strip()\n",
        "                    if answer and len(answer) > 3 and len(answer) < 300:  # Filter jawaban\n",
        "                        # Bersihkan jawaban\n",
        "                        answer = re.sub(r'\\s+', ' ', answer)\n",
        "                        answer = answer.strip('.,;:')\n",
        "\n",
        "                        qa_pairs.append({\n",
        "                            'filename': filename,\n",
        "                            'question': template['question'],\n",
        "                            'answer': answer,\n",
        "                            'question_type': template['type'],\n",
        "                            'context': text[max(0, match.start()-100):match.end()+100],\n",
        "                            'confidence': min(1.0, len(answer) / 100)  # Simple confidence score\n",
        "                        })\n",
        "\n",
        "        print(f\"❓ Generated {len(qa_pairs)} QA pairs\")\n",
        "        return qa_pairs\n",
        "\n",
        "    def encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Encode categorical features menjadi numerical dengan handling yang lebih baik\"\"\"\n",
        "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "        categorical_columns = [col for col in categorical_columns if col != 'nama_file']\n",
        "\n",
        "        df_encoded = df.copy()\n",
        "\n",
        "        for col in categorical_columns:\n",
        "            if col not in self.label_encoders:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "\n",
        "            # Handle missing values\n",
        "            df_encoded[col] = df_encoded[col].fillna('unknown')\n",
        "\n",
        "            try:\n",
        "                df_encoded[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df_encoded[col])\n",
        "                print(f\"🔢 Encoded {col}: {len(self.label_encoders[col].classes_)} categories\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error encoding {col}: {e}\")\n",
        "\n",
        "        return df_encoded\n",
        "\n",
        "    def combine_all_features(self, text_features: pd.DataFrame, bow_features: pd.DataFrame,\n",
        "                           tfidf_features: pd.DataFrame, metadata_df: pd.DataFrame = None,\n",
        "                           content_df: pd.DataFrame = None) -> pd.DataFrame:\n",
        "        \"\"\"Gabungkan semua features menjadi satu dataset dengan handling yang robust\"\"\"\n",
        "\n",
        "        # Start dengan text features\n",
        "        combined_df = text_features.copy()\n",
        "\n",
        "        # Join dengan metadata jika tersedia\n",
        "        if metadata_df is not None and not metadata_df.empty:\n",
        "            combined_df = pd.merge(combined_df, metadata_df, on='nama_file', how='left')\n",
        "            print(f\"📊 Joined dengan metadata: {combined_df.shape}\")\n",
        "\n",
        "        # Join dengan content jika tersedia\n",
        "        if content_df is not None and not content_df.empty:\n",
        "            # Pilih kolom penting dari content\n",
        "            content_cols = ['nama_file']\n",
        "            if 'kelengkapan_konten_persen' in content_df.columns:\n",
        "                content_cols.append('kelengkapan_konten_persen')\n",
        "            if 'panjang_teks' in content_df.columns:\n",
        "                content_cols.append('panjang_teks')\n",
        "            if 'jumlah_kata' in content_df.columns:\n",
        "                content_cols.append('jumlah_kata')\n",
        "\n",
        "            available_cols = [col for col in content_cols if col in content_df.columns]\n",
        "            if len(available_cols) > 1:  # More than just nama_file\n",
        "                content_subset = content_df[available_cols].copy()\n",
        "                combined_df = pd.merge(combined_df, content_subset, on='nama_file', how='left')\n",
        "                print(f\"📝 Joined dengan konten: {combined_df.shape}\")\n",
        "\n",
        "        # Join dengan BoW features jika tersedia\n",
        "        if not bow_features.empty:\n",
        "            combined_df = pd.merge(combined_df, bow_features, on='nama_file', how='left')\n",
        "            print(f\"🎯 Joined dengan BoW: {combined_df.shape}\")\n",
        "\n",
        "        # Join dengan TF-IDF features jika tersedia\n",
        "        if not tfidf_features.empty:\n",
        "            combined_df = pd.merge(combined_df, tfidf_features, on='nama_file', how='left')\n",
        "            print(f\"📈 Joined dengan TF-IDF: {combined_df.shape}\")\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def create_target_variables(self, combined_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Buat target variables yang komprehensif untuk supervised learning\"\"\"\n",
        "        df_with_targets = combined_df.copy()\n",
        "\n",
        "        # 1. CLASSIFICATION TARGETS\n",
        "        # Jenis perkara classification\n",
        "        if 'jenis_perkara' in df_with_targets.columns:\n",
        "            df_with_targets['is_pidana'] = df_with_targets['jenis_perkara'].apply(\n",
        "                lambda x: 1 if x and 'pidana' in str(x).lower() else 0\n",
        "            )\n",
        "            df_with_targets['is_korupsi'] = df_with_targets['jenis_perkara'].apply(\n",
        "                lambda x: 1 if x and 'korupsi' in str(x).lower() else 0\n",
        "            )\n",
        "            df_with_targets['is_perdata'] = df_with_targets['jenis_perkara'].apply(\n",
        "                lambda x: 1 if x and 'perdata' in str(x).lower() else 0\n",
        "            )\n",
        "\n",
        "        # 2. REGRESSION TARGETS\n",
        "        # Kompleksitas perkara berdasarkan jumlah kata dan fitur lainnya\n",
        "        if 'word_count' in df_with_targets.columns:\n",
        "            # Document complexity score (1-4)\n",
        "            df_with_targets['complexity_score'] = pd.cut(\n",
        "                df_with_targets['word_count'],\n",
        "                bins=[0, 1000, 5000, 10000, float('inf')],\n",
        "                labels=[1, 2, 3, 4]\n",
        "            ).astype(int)\n",
        "\n",
        "            # Document length category\n",
        "            df_with_targets['length_category'] = pd.cut(\n",
        "                df_with_targets['word_count'],\n",
        "                bins=[0, 2000, 7500, 15000, float('inf')],\n",
        "                labels=['short', 'medium', 'long', 'very_long']\n",
        "            )\n",
        "\n",
        "        # Legal complexity based on multiple factors\n",
        "        legal_complexity = 0\n",
        "        if 'article_references' in df_with_targets.columns:\n",
        "            legal_complexity += df_with_targets['article_references'].fillna(0) * 0.3\n",
        "        if 'law_references' in df_with_targets.columns:\n",
        "            legal_complexity += df_with_targets['law_references'].fillna(0) * 0.4\n",
        "        if 'legal_terms_count' in df_with_targets.columns:\n",
        "            legal_complexity += (df_with_targets['legal_terms_count'].fillna(0) / 10) * 0.3\n",
        "\n",
        "        df_with_targets['legal_complexity'] = legal_complexity\n",
        "\n",
        "        # 3. BINARY TARGETS\n",
        "        # Kelengkapan dokumen\n",
        "        if 'has_decision' in df_with_targets.columns and 'has_consideration' in df_with_targets.columns:\n",
        "            df_with_targets['is_complete_doc'] = (\n",
        "                (df_with_targets['has_decision'] == 1) &\n",
        "                (df_with_targets['has_consideration'] == 1) &\n",
        "                (df_with_targets['has_parties'] == 1)\n",
        "            ).astype(int)\n",
        "\n",
        "        # Document quality indicators\n",
        "        if 'has_case_number' in df_with_targets.columns:\n",
        "            df_with_targets['has_proper_format'] = (\n",
        "                (df_with_targets['has_case_number'] == 1) &\n",
        "                (df_with_targets['has_header'] == 1)\n",
        "            ).astype(int)\n",
        "\n",
        "        # High-quality legal document\n",
        "        quality_score = 0\n",
        "        quality_features = ['has_consideration', 'has_decision', 'has_parties', 'has_case_number']\n",
        "        for feature in quality_features:\n",
        "            if feature in df_with_targets.columns:\n",
        "                quality_score += df_with_targets[feature].fillna(0)\n",
        "\n",
        "        df_with_targets['quality_score'] = quality_score\n",
        "        df_with_targets['is_high_quality'] = (quality_score >= 3).astype(int)\n",
        "\n",
        "        # 4. MULTI-CLASS TARGETS\n",
        "        # Document type classification\n",
        "        doc_type = []\n",
        "        for idx, row in df_with_targets.iterrows():\n",
        "            if row.get('has_dakwaan', 0) == 1:\n",
        "                doc_type.append('criminal_case')\n",
        "            elif row.get('is_perdata', 0) == 1:\n",
        "                doc_type.append('civil_case')\n",
        "            elif row.get('has_decision', 0) == 1:\n",
        "                doc_type.append('judgment')\n",
        "            else:\n",
        "                doc_type.append('other')\n",
        "\n",
        "        df_with_targets['document_type'] = doc_type\n",
        "\n",
        "        # Case severity (based on legal terms and complexity)\n",
        "        severity_bins = [0, 5, 15, 30, float('inf')]\n",
        "        severity_labels = ['low', 'medium', 'high', 'critical']\n",
        "\n",
        "        if 'legal_terms_count' in df_with_targets.columns:\n",
        "            df_with_targets['case_severity'] = pd.cut(\n",
        "                df_with_targets['legal_terms_count'].fillna(0),\n",
        "                bins=severity_bins,\n",
        "                labels=severity_labels\n",
        "            )\n",
        "\n",
        "        print(f\"🎯 Created comprehensive target variables\")\n",
        "        return df_with_targets\n",
        "\n",
        "    def create_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Buat derived features dari kombinasi fitur existing\"\"\"\n",
        "        df_derived = df.copy()\n",
        "\n",
        "        # 1. RATIO FEATURES\n",
        "        if 'unique_words' in df_derived.columns and 'word_count' in df_derived.columns:\n",
        "            df_derived['vocabulary_richness'] = df_derived['unique_words'] / (df_derived['word_count'] + 1)\n",
        "\n",
        "        if 'legal_terms_count' in df_derived.columns and 'word_count' in df_derived.columns:\n",
        "            df_derived['legal_term_density'] = df_derived['legal_terms_count'] / (df_derived['word_count'] + 1)\n",
        "\n",
        "        if 'sentence_count' in df_derived.columns and 'paragraph_count' in df_derived.columns:\n",
        "            df_derived['sentences_per_paragraph'] = df_derived['sentence_count'] / (df_derived['paragraph_count'] + 1)\n",
        "\n",
        "        # 2. INTERACTION FEATURES\n",
        "        if 'article_references' in df_derived.columns and 'law_references' in df_derived.columns:\n",
        "            df_derived['total_legal_refs'] = df_derived['article_references'].fillna(0) + df_derived['law_references'].fillna(0)\n",
        "\n",
        "        if 'defendant_mentions' in df_derived.columns and 'prosecutor_mentions' in df_derived.columns:\n",
        "            df_derived['prosecution_intensity'] = df_derived['prosecutor_mentions'].fillna(0) / (df_derived['defendant_mentions'].fillna(0) + 1)\n",
        "\n",
        "        # 3. COMPOSITE SCORES\n",
        "        # Document formality score\n",
        "        formality_features = ['has_header', 'has_case_number', 'has_parties', 'has_consideration', 'has_decision']\n",
        "        formality_score = 0\n",
        "        for feature in formality_features:\n",
        "            if feature in df_derived.columns:\n",
        "                formality_score += df_derived[feature].fillna(0)\n",
        "        df_derived['formality_score'] = formality_score / len(formality_features)\n",
        "\n",
        "        # Content richness score\n",
        "        content_features = ['has_dakwaan', 'has_bukti', 'has_saksi', 'has_tuntutan']\n",
        "        content_score = 0\n",
        "        for feature in content_features:\n",
        "            if feature in df_derived.columns:\n",
        "                content_score += df_derived[feature].fillna(0)\n",
        "        df_derived['content_richness'] = content_score / len(content_features)\n",
        "\n",
        "        print(f\"🔄 Created derived features\")\n",
        "        return df_derived\n",
        "\n",
        "    def perform_feature_selection(self, df: pd.DataFrame, n_components: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Lakukan feature selection dan dimensionality reduction\"\"\"\n",
        "        # Separate numerical and categorical columns\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        numeric_cols = [col for col in numeric_cols if col != 'nama_file']\n",
        "\n",
        "        if len(numeric_cols) < 2:\n",
        "            print(\"⚠️ Tidak cukup fitur numerik untuk PCA\")\n",
        "            return df, pd.DataFrame()\n",
        "\n",
        "        # Prepare data for PCA\n",
        "        numeric_data = df[numeric_cols].fillna(0)\n",
        "\n",
        "        # Standardize features\n",
        "        try:\n",
        "            scaled_data = self.scaler.fit_transform(numeric_data)\n",
        "\n",
        "            # Apply PCA\n",
        "            n_components = min(n_components, len(numeric_cols), len(df) - 1)\n",
        "            pca = PCA(n_components=n_components)\n",
        "            pca_features = pca.fit_transform(scaled_data)\n",
        "\n",
        "            # Create PCA DataFrame\n",
        "            pca_columns = [f'pca_{i+1}' for i in range(n_components)]\n",
        "            pca_df = pd.DataFrame(pca_features, columns=pca_columns, index=df.index)\n",
        "            pca_df['nama_file'] = df['nama_file'].values\n",
        "\n",
        "            # Calculate explained variance\n",
        "            explained_variance = pca.explained_variance_ratio_\n",
        "            cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "            print(f\"📊 PCA completed: {n_components} components explain {cumulative_variance[-1]:.2%} variance\")\n",
        "\n",
        "            return df, pca_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in PCA: {e}\")\n",
        "            return df, pd.DataFrame()\n",
        "\n",
        "    def generate_feature_summary(self, combined_df: pd.DataFrame, qa_pairs: List[Dict], pca_df: pd.DataFrame = None) -> str:\n",
        "        \"\"\"Generate comprehensive feature summary\"\"\"\n",
        "        summary = []\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(\"FEATURE ENGINEERING SUMMARY REPORT\")\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Dataset Overview\n",
        "        summary.append(\"📊 DATASET OVERVIEW\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        summary.append(f\"Total Documents: {combined_df.shape[0]}\")\n",
        "        summary.append(f\"Total Features: {combined_df.shape[1]}\")\n",
        "        summary.append(f\"QA Pairs Generated: {len(qa_pairs)}\")\n",
        "        if pca_df is not None and not pca_df.empty:\n",
        "            summary.append(f\"PCA Components: {pca_df.shape[1] - 1}\")  # -1 for nama_file\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Feature Categories\n",
        "        summary.append(\"🔧 FEATURE CATEGORIES\")\n",
        "        summary.append(\"-\" * 30)\n",
        "\n",
        "        # Count features by category\n",
        "        bow_features = len([col for col in combined_df.columns if col.startswith('bow_')])\n",
        "        tfidf_features = len([col for col in combined_df.columns if col.startswith('tfidf_')])\n",
        "        text_features = len([col for col in combined_df.columns if col in [\n",
        "            'char_count', 'word_count', 'sentence_count', 'paragraph_count',\n",
        "            'unique_words', 'lexical_diversity', 'avg_word_length'\n",
        "        ]])\n",
        "\n",
        "        summary.append(f\"Text Features: {text_features}\")\n",
        "        summary.append(f\"Bag-of-Words Features: {bow_features}\")\n",
        "        summary.append(f\"TF-IDF Features: {tfidf_features}\")\n",
        "        summary.append(f\"Legal Features: {len([col for col in combined_df.columns if 'legal' in col])}\")\n",
        "        summary.append(f\"Target Variables: {len([col for col in combined_df.columns if col.startswith('is_') or col.endswith('_score')])}\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Data Quality\n",
        "        summary.append(\"📈 DATA QUALITY METRICS\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        summary.append(f\"Missing Value Ratio: {combined_df.isnull().sum().sum() / (combined_df.shape[0] * combined_df.shape[1]):.2%}\")\n",
        "        summary.append(f\"Complete Cases: {combined_df.dropna().shape[0]} ({combined_df.dropna().shape[0]/combined_df.shape[0]:.1%})\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Feature Types\n",
        "        summary.append(\"📋 FEATURE TYPES\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        for dtype in combined_df.dtypes.value_counts().items():\n",
        "            summary.append(f\"{dtype[0]}: {dtype[1]} columns\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # QA Statistics\n",
        "        if qa_pairs:\n",
        "            summary.append(\"❓ QA PAIRS STATISTICS\")\n",
        "            summary.append(\"-\" * 30)\n",
        "            qa_types = Counter([qa['question_type'] for qa in qa_pairs])\n",
        "            for qa_type, count in qa_types.most_common():\n",
        "                summary.append(f\"{qa_type}: {count} pairs\")\n",
        "            summary.append(\"\")\n",
        "\n",
        "        # Column List\n",
        "        summary.append(\"📝 ALL COLUMNS\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        for i, col in enumerate(combined_df.columns, 1):\n",
        "            summary.append(f\"{i:3d}. {col}\")\n",
        "\n",
        "        summary.append(\"\")\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(\"END OF REPORT\")\n",
        "        summary.append(\"=\" * 70)\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def save_features_to_files(self, combined_df: pd.DataFrame, qa_pairs: List[Dict], pca_df: pd.DataFrame = None):\n",
        "        \"\"\"Simpan semua features ke file dengan backup di Google Drive\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # 1. SAVE MAIN FEATURES DATASET\n",
        "        features_filename = f\"features_{timestamp}.csv\"\n",
        "        features_path_local = os.path.join(self.output_dir, features_filename)\n",
        "        features_path_gdrive = os.path.join(self.gdrive_output_dir, features_filename)\n",
        "\n",
        "        combined_df.to_csv(features_path_local, index=False, encoding='utf-8')\n",
        "        combined_df.to_csv(features_path_gdrive, index=False, encoding='utf-8')\n",
        "        print(f\"📄 Features CSV Lokal: {features_path_local}\")\n",
        "        print(f\"💾 Features CSV GDrive: {features_path_gdrive}\")\n",
        "\n",
        "        # 2. SAVE PCA FEATURES if available\n",
        "        if pca_df is not None and not pca_df.empty:\n",
        "            pca_filename = f\"features_pca_{timestamp}.csv\"\n",
        "            pca_path_local = os.path.join(self.output_dir, pca_filename)\n",
        "            pca_path_gdrive = os.path.join(self.gdrive_output_dir, pca_filename)\n",
        "\n",
        "            pca_df.to_csv(pca_path_local, index=False, encoding='utf-8')\n",
        "            pca_df.to_csv(pca_path_gdrive, index=False, encoding='utf-8')\n",
        "            print(f\"📊 PCA CSV Lokal: {pca_path_local}\")\n",
        "            print(f\"💾 PCA CSV GDrive: {pca_path_gdrive}\")\n",
        "\n",
        "        # 3. SAVE QA PAIRS\n",
        "        if qa_pairs:\n",
        "            qa_filename = f\"qa_pairs_{timestamp}.json\"\n",
        "            qa_path_local = os.path.join(self.output_dir, qa_filename)\n",
        "            qa_path_gdrive = os.path.join(self.gdrive_output_dir, qa_filename)\n",
        "\n",
        "            with open(qa_path_local, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "            with open(qa_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"❓ QA JSON Lokal: {qa_path_local}\")\n",
        "            print(f\"💾 QA JSON GDrive: {qa_path_gdrive}\")\n",
        "\n",
        "        # 4. SAVE FEATURE SUMMARY\n",
        "        summary_filename = f\"feature_summary_{timestamp}.txt\"\n",
        "        summary_path_local = os.path.join(self.output_dir, summary_filename)\n",
        "        summary_path_gdrive = os.path.join(self.gdrive_output_dir, summary_filename)\n",
        "\n",
        "        summary_content = self.generate_feature_summary(combined_df, qa_pairs, pca_df)\n",
        "\n",
        "        with open(summary_path_local, 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_content)\n",
        "        with open(summary_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_content)\n",
        "\n",
        "        print(f\"📋 Summary Lokal: {summary_path_local}\")\n",
        "        print(f\"💾 Summary GDrive: {summary_path_gdrive}\")\n",
        "\n",
        "        return {\n",
        "            'features_local': features_path_local,\n",
        "            'features_gdrive': features_path_gdrive,\n",
        "            'qa_local': qa_path_local if qa_pairs else None,\n",
        "            'qa_gdrive': qa_path_gdrive if qa_pairs else None,\n",
        "            'summary_local': summary_path_local,\n",
        "            'summary_gdrive': summary_path_gdrive,\n",
        "            'pca_local': pca_path_local if pca_df is not None and not pca_df.empty else None,\n",
        "            'pca_gdrive': pca_path_gdrive if pca_df is not None and not pca_df.empty else None\n",
        "        }\n",
        "\n",
        "    def process_all_features(self) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "        \"\"\"Proses semua tahap feature engineering dengan error handling yang robust\"\"\"\n",
        "        print(\"🔧 iii. FEATURE ENGINEERING - COMPREHENSIVE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Generating features: Text stats, BoW, TF-IDF, QA pairs, PCA\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # 1. LOAD DATA\n",
        "            print(\"\\n📂 Loading data...\")\n",
        "            texts = self.load_text_files()\n",
        "            if not texts:\n",
        "                print(\"❌ Tidak ada file teks yang ditemukan!\")\n",
        "                return pd.DataFrame(), []\n",
        "\n",
        "            metadata_df = self.load_metadata()\n",
        "            content_df = self.load_content()\n",
        "\n",
        "            # 2. CALCULATE TEXT FEATURES\n",
        "            print(\"\\n📊 Calculating comprehensive text features...\")\n",
        "            text_features = self.calculate_text_features(texts)\n",
        "            print(f\"✅ Text features: {text_features.shape}\")\n",
        "\n",
        "            # 3. CREATE BAG-OF-WORDS AND TF-IDF\n",
        "            print(\"\\n🎯 Creating BoW and TF-IDF features...\")\n",
        "            bow_features, tfidf_features = self.create_bag_of_words_features(texts)\n",
        "\n",
        "            # 4. CREATE QA PAIRS\n",
        "            print(\"\\n❓ Creating comprehensive QA pairs...\")\n",
        "            qa_pairs = self.create_qa_pairs(texts, metadata_df)\n",
        "\n",
        "            # 5. COMBINE ALL FEATURES\n",
        "            print(\"\\n🔗 Combining all features...\")\n",
        "            combined_df = self.combine_all_features(\n",
        "                text_features, bow_features, tfidf_features, metadata_df, content_df\n",
        "            )\n",
        "\n",
        "            # 6. CREATE DERIVED FEATURES\n",
        "            print(\"\\n🔄 Creating derived features...\")\n",
        "            combined_df = self.create_derived_features(combined_df)\n",
        "\n",
        "            # 7. ENCODE CATEGORICAL FEATURES\n",
        "            print(\"\\n🔢 Encoding categorical features...\")\n",
        "            combined_df = self.encode_categorical_features(combined_df)\n",
        "\n",
        "            # 8. CREATE TARGET VARIABLES\n",
        "            print(\"\\n🎯 Creating target variables...\")\n",
        "            combined_df = self.create_target_variables(combined_df)\n",
        "\n",
        "            # 9. FEATURE SELECTION AND PCA\n",
        "            print(\"\\n📊 Performing feature selection and PCA...\")\n",
        "            combined_df, pca_df = self.perform_feature_selection(combined_df)\n",
        "\n",
        "            # 10. SAVE ALL FEATURES\n",
        "            print(\"\\n💾 Saving features to files...\")\n",
        "            file_paths = self.save_features_to_files(combined_df, qa_pairs, pca_df)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"✅ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\")\n",
        "            print(f\"📊 Final dataset: {combined_df.shape}\")\n",
        "            print(f\"❓ QA pairs: {len(qa_pairs)}\")\n",
        "            print(f\"💾 Files saved to 2 locations: lokal & Google Drive\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            return combined_df, qa_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in feature engineering: {e}\")\n",
        "            print(f\"💥 ERROR: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return pd.DataFrame(), []\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk menjalankan feature engineering\"\"\"\n",
        "    print(\"🚀 MULAI COMPREHENSIVE FEATURE ENGINEERING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        engineer = FeatureEngineer()\n",
        "        features_df, qa_pairs = engineer.process_all_features()\n",
        "\n",
        "        if not features_df.empty:\n",
        "            print(f\"\\n🎉 FEATURE ENGINEERING BERHASIL!\")\n",
        "            print(f\"Dataset shape: {features_df.shape}\")\n",
        "            print(f\"QA pairs: {len(qa_pairs)}\")\n",
        "            print(\"File output tersimpan di:\")\n",
        "            print(\"  - Lokal: /data/processed/\")\n",
        "            print(\"  - GDrive: /content/drive/MyDrive/terorisme/data/processed/\")\n",
        "        else:\n",
        "            print(\"\\n❌ Tidak ada features yang berhasil dibuat.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n💥 ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIy6L4eyzMiW",
        "outputId": "7c3ac625-b7f7-4a7e-de16-609e6bcb44fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Tidak ada file metadata yang ditemukan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MULAI COMPREHENSIVE FEATURE ENGINEERING\n",
            "======================================================================\n",
            "🔧 FEATURE ENGINEERING\n",
            "Input teks: /data/raw atau /content/drive/MyDrive/terorisme/CLEANED\n",
            "Input metadata: /data/processed\n",
            "Output Lokal: /data/processed\n",
            "Output GDrive: /content/drive/MyDrive/terorisme/data/processed\n",
            "🔧 iii. FEATURE ENGINEERING - COMPREHENSIVE\n",
            "============================================================\n",
            "Generating features: Text stats, BoW, TF-IDF, QA pairs, PCA\n",
            "============================================================\n",
            "\n",
            "📂 Loading data...\n",
            "📂 Loading teks dari: /content/drive/MyDrive/terorisme/CLEANED\n",
            "📁 Loaded 46 file teks\n",
            "📝 Loaded konten kunci: 46 records dari konten_kunci_20250625_111438.csv\n",
            "\n",
            "📊 Calculating comprehensive text features...\n",
            "✅ Text features: (46, 42)\n",
            "\n",
            "🎯 Creating BoW and TF-IDF features...\n",
            "🎯 Bag-of-Words: 500 features\n",
            "📈 TF-IDF: 1000 features\n",
            "\n",
            "❓ Creating comprehensive QA pairs...\n",
            "❓ Generated 2397 QA pairs\n",
            "\n",
            "🔗 Combining all features...\n",
            "📝 Joined dengan konten: (46, 45)\n",
            "🎯 Joined dengan BoW: (46, 545)\n",
            "📈 Joined dengan TF-IDF: (46, 1545)\n",
            "\n",
            "🔄 Creating derived features...\n",
            "🔄 Created derived features\n",
            "\n",
            "🔢 Encoding categorical features...\n",
            "\n",
            "🎯 Creating target variables...\n",
            "🎯 Created comprehensive target variables\n",
            "\n",
            "📊 Performing feature selection and PCA...\n",
            "📊 PCA completed: 45 components explain 100.00% variance\n",
            "\n",
            "💾 Saving features to files...\n",
            "📄 Features CSV Lokal: /data/processed/features_20250625_111644.csv\n",
            "💾 Features CSV GDrive: /content/drive/MyDrive/terorisme/data/processed/features_20250625_111644.csv\n",
            "📊 PCA CSV Lokal: /data/processed/features_pca_20250625_111644.csv\n",
            "💾 PCA CSV GDrive: /content/drive/MyDrive/terorisme/data/processed/features_pca_20250625_111644.csv\n",
            "❓ QA JSON Lokal: /data/processed/qa_pairs_20250625_111644.json\n",
            "💾 QA JSON GDrive: /content/drive/MyDrive/terorisme/data/processed/qa_pairs_20250625_111644.json\n",
            "📋 Summary Lokal: /data/processed/feature_summary_20250625_111644.txt\n",
            "💾 Summary GDrive: /content/drive/MyDrive/terorisme/data/processed/feature_summary_20250625_111644.txt\n",
            "\n",
            "============================================================\n",
            "✅ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\n",
            "📊 Final dataset: (46, 1561)\n",
            "❓ QA pairs: 2397\n",
            "💾 Files saved to 2 locations: lokal & Google Drive\n",
            "============================================================\n",
            "\n",
            "🎉 FEATURE ENGINEERING BERHASIL!\n",
            "Dataset shape: (46, 1561)\n",
            "QA pairs: 2397\n",
            "File output tersimpan di:\n",
            "  - Lokal: /data/processed/\n",
            "  - GDrive: /content/drive/MyDrive/terorisme/data/processed/\n"
          ]
        }
      ]
    }
  ]
}